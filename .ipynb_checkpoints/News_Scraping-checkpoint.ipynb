{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated News Update, or, \"Dwyer's attempt at automating himself out of a job\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Do\n",
    "* Add Governing\n",
    "* Axios\n",
    "* Functionality to add news article to SQL database after the fact\n",
    "* Auto open word doc in gen_docx\n",
    "* Add Phys.org\n",
    "* [Journal of Modern Transportation](https://link.springer.com/journal/40534) -- add offline issues\n",
    "* Add infinite scrolling functionality to css_scraypuh\n",
    "\n",
    "### Can't because of paywalls:\n",
    "* WSJ\n",
    "* Nikkei\n",
    "* Automotive World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T11:29:08.311107Z",
     "start_time": "2018-08-29T11:29:08.300133Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages, define important stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T13:30:37.376863Z",
     "start_time": "2019-01-07T13:30:27.352145Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "\n",
    "import docx\n",
    "from docx.enum.text import WD_COLOR_INDEX\n",
    "from docx.shared import Pt\n",
    "import win32com.client as win32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T13:30:37.453156Z",
     "start_time": "2019-01-07T13:30:37.390327Z"
    }
   },
   "outputs": [],
   "source": [
    "# Keyword lists for each of the different news updates\n",
    "cav_keywords = ['self-driving', 'automated', 'self driving', 'autonomous', 'MaaS', 'ride-sharing', 'ridesharing', 'ride-hailing',\n",
    "                'ridehailing', 'lidar', 'LiDAR', 'rideshare', 'ridehail', 'ride-hail', 'ridesource', 'ride-source', 'ride-sourcing',\n",
    "                'carsharing', 'car-sharing', 'carshare', 'car-share', 'Uber', 'Lyft', 'Chariot', 'connected car', 'Waymo', 'TRI',\n",
    "                'Cruise', 'Zoox', 'Mobileye', 'Softbank', 'peer-to-peer', 'Turo']\n",
    "afv_keywords = ['rare-earth', 'rare earth', 'natural gas', 'electric vehicles', 'electric vehicle', 'electric car', 'EV', 'electrification', 'alternative fuel', 'CNG', 'LNG',\n",
    "                'alt-fuel', 'propane', 'charging stations', 'EVSE', 'electric vehicle charging', 'HEV', 'hybrid', 'hybrid-electric', 'plug-in', 'PHEV', 'electric motor',\n",
    "                'bio-fuel', 'biofuel', 'idle reduction', 'fuel cell', 'electric bus', 'electric buses', 'electric truck', 'electric trucks', 'electric drive',\n",
    "                'battery-electric', 'battery electric', 'electric', 'battery-electric-powered']\n",
    "truck_keywords = ['alternative fuels', 'natural gas', 'compressed natural gas', 'liquefied natural gas', 'CNG', 'LNG', 'propane', 'LPG', 'dimethyl ether', 'DME', 'electric', 'electricity', 'electrified', 'electric drive',\n",
    "                  'battery', 'energy storage', 'hydrogen', 'fuel cell', 'hybrid', 'hybrid electric', 'hybrid hydraulic', ' Phase 2', 'Phase II', 'efficiency', 'fuel efficiency', 'fuel economy', 'aftertreatment',\n",
    "                  'emission control', 'diesel particulate filter', 'DPF', 'selective catalytic reduction', 'SCR', 'aerodynamics', 'sustainability', 'waste heat recovery', 'Rankine', 'organic Rankine', 'SuperTruck',\n",
    "                  'automated manual', 'AMT', 'platooning', 'lithium', 'biofuel', 'fast charging', 'downspeed', 'downsize', 'clean diesel', 'turbocompound', 'rolling resistance', 'skirt', 'boat tail', 'axle', 'low viscosity',\n",
    "                  'catenary', 'autonomy', 'autonomous', 'connected and autonomous', 'connected', 'telematics', 'driver assist', 'CACC', 'active cruise control', 'crash avoidance', 'crashworthiness', 'weigh-in-motion', 'weigh in motion',\n",
    "                  'high productivity', 'truck size and weight', 'V2I', 'V2V', 'vehicle to infrastructure', ' vehicle to vehicle',  'restructuring', 'acquisition', 'driver cost', 'operational efficiency',\n",
    "                  'facilities', 'proving ground', 'partnership', 'regional haul', 'joint venture', 'grant', 'FOA', 'funding opportunity', 'unveil', 'announce', 'offer', 'expansion', 'greenhouse gas', 'GHG', 'emission regulation',\n",
    "                  'emissions regulation', 'idle', 'idling', 'zero emissions', 'strategic plan', 'SmartWay', 'VIUS', 'well to wheels', 'pump to wheels', 'well to pump', 'CARB', 'CEC', 'air resources board', 'energy commission', 'EPA',\n",
    "                  'Environmental Protection Agency', 'smart mobility', 'smart cities']\n",
    "hyperloop_keywords = ['hyperloop', 'high-speed train',\n",
    "                      'high speed train', 'bullet train']\n",
    "\n",
    "# Used for diagnostics/tracking later\n",
    "scrape_specs = {}\n",
    "\n",
    "# Set day of week for each scraper category, to reduce run time (avoid searching for all keyword lists every run)\n",
    "scraper_sched = {'CAV': 0, 'AFV': 2, '21CTP': 4, 'Hyperloop': 0}\n",
    "\n",
    "# Age filter, in days (only want to pull articles that are <= 1 week old)\n",
    "max_age = 7\n",
    "\n",
    "# For file naming and tracking\n",
    "search_date = str(dt.date.today())\n",
    "\n",
    "# Needed for web scraping \"browser\"\n",
    "headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}\n",
    "\n",
    "# For database update; ensures duplicates aren't loaded\n",
    "db_update = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T13:30:37.503023Z",
     "start_time": "2019-01-07T13:30:37.468125Z"
    }
   },
   "outputs": [],
   "source": [
    "# scraped_count = 0\n",
    "# skip_count = 0\n",
    "# too_old = 0\n",
    "# iteration = 0\n",
    "# skip_ind = []\n",
    "# old_ind = []\n",
    "\n",
    "\n",
    "def replace_em(text):\n",
    "    '''Replaces odd characters in text. Used for page titles and summaries'''\n",
    "    bad_chars = ['â€œ', 'â€™', 'â€�', '\\n', 'Â',\n",
    "                 'â€”', '(earlier post)', 'â€?', '\\t', 'â€œ', '(TNS) — ', '(Reuters) - ',\n",
    "                 'DUBAI (Reuters) - ']\n",
    "    for bad_char in bad_chars:\n",
    "        text = text.replace(bad_char, '')\n",
    "    return text\n",
    "\n",
    "\n",
    "def grab_homepage(url):\n",
    "    '''Creates BeautifulSoup object using input url'''\n",
    "#     headers = {'user-agent': 'Mozilla/5.0'}\n",
    "    page_1 = requests.get(url, headers=headers)\n",
    "    return BeautifulSoup(page_1.content, \"html5lib\")\n",
    "\n",
    "\n",
    "def print_results(site, scraped_count, skip_count, too_old, df, duration, scrape_specs):\n",
    "    '''Prints out a quick summary of one website's full scraping and adds summary specs to scrape_specs dictionary'''\n",
    "    print(f'{scraped_count} {site} article(s) scraped')\n",
    "    print(f'{skip_count} {site} article(s) skipped due to error')\n",
    "    print(f'{too_old} {site} article(s) skipped due to age')\n",
    "    print(f'{df.shape[0]} relevant article(s) collected')\n",
    "    scrape_specs[f\"{site}\"] = {'Pages Scraped': scraped_count, 'Relevant Articles': df.shape[0], 'Errors': skip_count,\n",
    "                               'Too old': too_old, 'Time spent': duration}\n",
    "    return scrape_specs\n",
    "\n",
    "\n",
    "def page_scan(title, summary, url, date, source):\n",
    "    '''\n",
    "    Searches a web page title and summary for keywords; returns the dictionary object that is used to create \n",
    "    the final dataframe. Searches the title first; if the keyword is there, it doesn't search the summary.\n",
    "\n",
    "    Only searches for keywords specific to that day of the week's news update.\n",
    "    '''\n",
    "    bool_dict = {'Hyperloop': 0, 'CAV': 0, 'AFV': 0, '21CTP': 0}\n",
    "    title_scrape = title+' '+title.lower()\n",
    "    summary_scrape = summary+' '+summary.lower()\n",
    "\n",
    "    if dt.date.today().weekday() == scraper_sched['CAV']:\n",
    "        if any(keyword in title_scrape for keyword in cav_keywords):\n",
    "            bool_dict['CAV'] = 1\n",
    "        elif any(keyword in summary_scrape for keyword in cav_keywords):\n",
    "            bool_dict['CAV'] = 1\n",
    "\n",
    "    if dt.date.today().weekday() == scraper_sched['Hyperloop']:\n",
    "        if any(keyword in title_scrape for keyword in hyperloop_keywords):\n",
    "            bool_dict['Hyperloop'] = 1\n",
    "        elif any(keyword in summary_scrape for keyword in hyperloop_keywords):\n",
    "            bool_dict['Hyperloop'] = 1\n",
    "\n",
    "    if dt.date.today().weekday() == scraper_sched['AFV']:\n",
    "        if any(keyword in title_scrape for keyword in afv_keywords):\n",
    "            bool_dict['AFV'] = 1\n",
    "        elif any(keyword in summary_scrape for keyword in afv_keywords):\n",
    "            bool_dict['AFV'] = 1\n",
    "\n",
    "    if dt.date.today().weekday() == scraper_sched['21CTP']:\n",
    "        if any(keyword in title + title_scrape for keyword in truck_keywords) & (('truck' in title_scrape) | ('trucks' in title_scrape)):\n",
    "            bool_dict['21CTP'] = 1\n",
    "        elif any(keyword in summary_scrape for keyword in truck_keywords) & (('truck' in summary_scrape) | ('trucks' in summary_scrape)):\n",
    "            bool_dict['21CTP'] = 1\n",
    "\n",
    "    if sum(bool_dict.values()) > 0:\n",
    "        return {'title': title.strip(), 'summary': summary.strip(), 'link': url, 'source': source,\n",
    "                'date': date, 'AFV': bool_dict['AFV'], 'CAV': bool_dict['CAV'], '21CTP': bool_dict['21CTP'],\n",
    "                'Hyperloop': bool_dict['Hyperloop']}\n",
    "    else:\n",
    "        return 'Most definitely nope'\n",
    "\n",
    "# The following two functions are for the Word document output!\n",
    "\n",
    "\n",
    "def add_hyperlink(paragraph, url, text):\n",
    "    '''\n",
    "    :param paragraph: The paragraph we are adding the hyperlink to.\n",
    "    :param url: A string containing the required url\n",
    "    :param text: The text displayed for the url\n",
    "    :return: The hyperlink object\n",
    "    '''\n",
    "    # This gets access to the document.xml.rels file and gets a new relation id value\n",
    "    part = paragraph.part\n",
    "    r_id = part.relate_to(\n",
    "        url, docx.opc.constants.RELATIONSHIP_TYPE.HYPERLINK, is_external=True)\n",
    "\n",
    "    # Create the w:hyperlink tag and add needed values\n",
    "    hyperlink = docx.oxml.shared.OxmlElement('w:hyperlink')\n",
    "    hyperlink.set(docx.oxml.shared.qn('r:id'), r_id, )\n",
    "\n",
    "    # Create a w:r element\n",
    "    new_run = docx.oxml.shared.OxmlElement('w:r')\n",
    "\n",
    "    # Create a new w:rPr element\n",
    "    rPr = docx.oxml.shared.OxmlElement('w:rPr')\n",
    "\n",
    "    # bold the text\n",
    "    u = docx.oxml.shared.OxmlElement('w:b')\n",
    "    rPr.append(u)\n",
    "\n",
    "    # Join all the xml elements together add add the required text to the w:r element\n",
    "    new_run.append(rPr)\n",
    "    new_run.text = text\n",
    "    hyperlink.append(new_run)\n",
    "\n",
    "    paragraph._p.append(hyperlink)\n",
    "\n",
    "    return hyperlink\n",
    "\n",
    "\n",
    "def gen_docx(newstype, dwyer=True, CA_nums='NEED TO INSERT'):\n",
    "    '''\n",
    "    Generates news Word doc using data file from web scrape\n",
    "    :param newstype: Either \"21CTP\", \"CAV\", or \"AFV\"\n",
    "    :param dwyer: If not running on Dwyer's computer, set this to False and put all needed files in the same directory\n",
    "    :param CA_nums: Input string for the CA EVSE numbers (automatically populates the caption for the EVSE bar chart figure)\n",
    "    '''\n",
    "\n",
    "    # select data file (xls) based on the newstype and date. Note that search_date is a global variable defined outside\n",
    "    # of this function. Each news update only happens once a week --> only one xls file per newstype per week --> can't just\n",
    "    # pick any old search_date and make a file.\n",
    "    if dwyer:\n",
    "        # Name of the excel file (standardized)\n",
    "        data_file = f\"{newstype.lower()}_news_updates/{search_date}_{newstype}_news_download.xls\"\n",
    "    else:\n",
    "        data_file = f\"{search_date}_{newstype}_news_download.xls\"\n",
    "\n",
    "    # Read the data in from the selected file\n",
    "    df = pd.read_excel(data_file)\n",
    "    df = df.reset_index(drop=True).T.to_dict()\n",
    "\n",
    "    # Start creating the word doc\n",
    "    newsdoc = docx.Document(docx='python_docx.docx')\n",
    "\n",
    "    # Add up-front stuff - title, headers, and for the AFV update, some other stuff (two captions and some text)\n",
    "    if newstype == 'AFV':\n",
    "        newsdoc.add_heading(\n",
    "            f\"Alternative Fuel Vehicle Weekly News Update – {dt.date.today().strftime('%m/%d/%Y')}\", 0)\n",
    "        newsdoc.add_heading('EVSE Market Analysis', 1)\n",
    "        evse_bar_chart = newsdoc.add_paragraph().add_run('INSERT EVSE BAR CHART HERE')\n",
    "        evse_bar_chart.font.bold = True\n",
    "        evse_bar_chart.font.size = Pt(16)\n",
    "        evse_bar_chart.font.highlight_color = WD_COLOR_INDEX.YELLOW\n",
    "        newsdoc.add_paragraph('Figure: Number of EVSE plugs (note: not stations) by state and charging level.'\n",
    "                              'CA is not included, since it would make the rest of the state numbers illegible.'\n",
    "                              f\"CA holds a disproportionately large share of the total EVSE plugs: {CA_nums} \"\n",
    "                              'of Level 1, Level 2, and DCFC plugs respectively. Data Source: U.S. DOE AFDC Station Locator.',\n",
    "                              style='Caption')\n",
    "        newsdoc.add_paragraph(' ')\n",
    "        newsdoc.add_paragraph('The table below summarizes overall changes in number of EV charging stations by state between '\n",
    "                              f\"{(dt.date.today() - dt.timedelta(7)).strftime('%m/%d/%Y')} and {dt.date.today().strftime('%m/%d/%Y')}:\",\n",
    "                              style='Normal')\n",
    "        newsdoc.add_paragraph('Table 1: Change in number of EV charging stations by state, between '\n",
    "                              f\"{(dt.date.today() - dt.timedelta(7)).strftime('%m/%d/%Y')} and {dt.date.today().strftime('%m/%d/%Y')}\",\n",
    "                              style='Caption')\n",
    "        evse_delta_table = newsdoc.add_paragraph().add_run('INSERT EVSE DELTA TABLE HERE')\n",
    "        evse_delta_table.font.bold = True\n",
    "        evse_delta_table.font.size = Pt(16)\n",
    "        evse_delta_table.font.highlight_color = WD_COLOR_INDEX.YELLOW\n",
    "\n",
    "    if newstype == 'CAV':\n",
    "        newsdoc.add_heading(\n",
    "            f\"Connected and Automated Vehicle Weekly News Update – {dt.date.today().strftime('%m/%d/%Y')}\", 0)\n",
    "        newsdoc.add_paragraph(' ')\n",
    "        newsdoc.add_paragraph('Includes coverage of ride-sharing and other smart mobility technologies. '\n",
    "                              'The majority of this is direct quotations from the respective articles. I '\n",
    "                              'claim none of this text content as my own, having only sifted through the '\n",
    "                              'web to find already-existing pieces relevant to these topics.')\n",
    "\n",
    "    if newstype == '21CTP':\n",
    "        newsdoc.add_heading(\n",
    "            f\"21CTP Trucking Weekly News Update – {dt.date.today().strftime('%m/%d/%Y')}\", 0)\n",
    "\n",
    "    for header in ['Business and Market Analysis', 'Technology, Testing, and Analysis', 'Policy and Government']:\n",
    "        newsdoc.add_heading(header, 1)\n",
    "        newsdoc.add_paragraph('')\n",
    "\n",
    "    # Add all of the actual news items\n",
    "    for row in df:\n",
    "        row = df[row]\n",
    "        newsdoc.add_heading(row['title'], level=2)\n",
    "        p = newsdoc.add_paragraph(row['summary'] + ' ')\n",
    "        p.add_run('(')\n",
    "        # This is where the add_hyperlink function is used\n",
    "        add_hyperlink(p, '{}'.format(row['link']), '{}'.format(row['source']))\n",
    "        p.add_run(')')\n",
    "    if newstype == 'CAV':\n",
    "        newsdoc.add_heading('Relevant Transportation Research', 1)\n",
    "        newsdoc.add_paragraph('This section includes publications, papers, articles, and conferences that investigate and/or'\n",
    "                              'discuss transportation and travel demand impacts of MaaS or other “future travel” considerations.'\n",
    "                              'Portions of the abstract or description (not my words) are included under each title for more information.')\n",
    "    if newstype == 'AFV':\n",
    "        newsdoc.add_heading('Relevant Transportation Research', 1)\n",
    "        newsdoc.add_paragraph('This section includes publications, papers, articles, and conferences that investigate and/or'\n",
    "                              'discuss alternative fuel vehicle impacts on transportation systems. Portions of the abstract '\n",
    "                              'or description (not my words) are included under each title for more information.')\n",
    "    if dwyer:\n",
    "        newsdoc.save(\n",
    "            f\"{newstype.lower()}_news_updates/Energetics {newstype} News Update - {search_date}.docx\")\n",
    "    else:\n",
    "        filename = f\"Energetics {newstype} News Update - {search_date}.docx\"\n",
    "        newsdoc.save(filename)\n",
    "\n",
    "\n",
    "def which_keyword_found(row):\n",
    "    ''' Identifies and stores which keywords triggered the news item pull '''\n",
    "    words_found = []\n",
    "    for keyword in cav_keywords+afv_keywords:\n",
    "        try:\n",
    "            if (row['summary'].find(keyword) > 0) | (row['title'].find(keyword) > 0):\n",
    "                words_found.append(keyword)\n",
    "        except:\n",
    "            continue\n",
    "    return ', '.join(words_found)\n",
    "\n",
    "\n",
    "def keyword_pull(string):\n",
    "    ''' Pulls all relevant capitalized words out of the title, as a quick \"keyword\" list '''\n",
    "    not_keywords = ['A', 'The', 'This', 'I', 'To', 'Who', 'Silicon', 'Valley', 'System', 'Build', 'Payment', 'Business', 'API', 'JV', 'JVs',\n",
    "                    'European', 'American', 'America', 'Europe', 'China', 'But', 'Are', 'They', 'Legal', 'Says', 'AV', 'Revolution', 'Is',\n",
    "                    'TechCrunch', 'For', 'EVs', 'Really', 'Get', 'Money', 'Adds', 'We', 'All', 'Starts', 'Return', 'Apart',\n",
    "                    'Them', 'Cities', 'After', 'Insurance', 'Back', 'Against', 'Would', 'Displace', 'Improves', 'While',\n",
    "                    'That', 'You', 'Find', 'Along', 'From', 'Their', 'Not', 'So', 'Say', 'Experts', 'Drivers', 'Its', 'Into', 'Fully',\n",
    "                    'Ranks', 'Stretch', 'SUV', 'Data', 'Sharing', 'Live', 'When', 'Agencies', 'Still', 'Trying', 'Program', 'Offer', 'Four',\n",
    "                    'Will', 'Backs', 'Just', 'Around', 'Years', 'Its', 'Future', 'Deploying', 'Objects', 'Distance', 'Highlights']\n",
    "    string = string.replace(';', '').replace(',', '').lstrip().split(' ')\n",
    "    keywords = [word for word in string if (\n",
    "        word[0].isupper()) & (word not in not_keywords)]\n",
    "    return ', '.join(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a scraper class that will be used for each website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T13:30:37.531950Z",
     "start_time": "2019-01-07T13:30:37.511002Z"
    }
   },
   "outputs": [],
   "source": [
    "class scraypah:\n",
    "    '''\n",
    "    Scraypah is a web scraper that searches through all of the recent articles on a website and extracts key information\n",
    "    from those that include relevant keywords. It requires a dictionary of parameters specific to each website that needs\n",
    "    to be scraped. See the __init__ docstring for information on the input parameter requirements.\n",
    "    '''\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"This is an object of class scraypah!\"\n",
    "\n",
    "    def __init__(self, params):\n",
    "        '''\n",
    "        Attributes:\n",
    "            params[url] (str): Homepage of the website, where each of the article page links are extracted from\n",
    "            params[source] (str): Name of the website\n",
    "            params[strain_bool] (bool): Is there a soup strainer for this website or not?\n",
    "            params[strain_tag] (str): Tag used for soup strainer\n",
    "            params[strain_attr_name] (str): Attribute name used for soup strainer\n",
    "            params[strain_attr_value] (str): Attribute value used for soup strainer\n",
    "            params[date_loc] (str): Location of the date in the HTML\n",
    "            params[date_format] (str): Allows user to set the date format, if the format on the website does not parse automaticall\n",
    "            params[sum_loc] (str): Location of the summary in the HTML (this is typically the first 3 paragraphs of the article)\n",
    "            params[title_loc] (str): Location of the title in the HTML\n",
    "            params[url_list_query] (str): BeautifulSoup code to extract the list of articles from the website homepage(s) (url)\n",
    "        '''\n",
    "        self.base_url = params['url']\n",
    "        self.source = params['source']\n",
    "        self.strainer = params['strain_bool']\n",
    "        if self.strainer:\n",
    "            self.strain_tag = params['strain_tag']\n",
    "            self.strain_attr_name = params['strain_attr_name']\n",
    "            self.strain_attr_value = params['strain_attr_value']\n",
    "        self.date_loc = params['date_loc']\n",
    "        self.date_format = params['date_format']\n",
    "        self.sum_loc = params['sum_loc']\n",
    "        self.title_loc = params['title_loc']\n",
    "        self.url_list_query = params['url_list_query']\n",
    "\n",
    "    def get_urls(self):\n",
    "        '''Populates self.urls_to_scrape with a list of urls extracted from the website homepage(s)'''\n",
    "        self.urls_to_scrape = []\n",
    "        with requests.Session() as s:\n",
    "\n",
    "            # Checks if the base_url is a single url or a list of urls - some websites publish enough articles\n",
    "            # that we have to pull multiple pages\n",
    "            if isinstance(self.base_url, str):\n",
    "\n",
    "                # Checks if there is a \"soup strainer\" for the website being scraped. See here:\n",
    "                # https://www.crummy.com/software/BeautifulSoup/bs4/doc/#parsing-only-part-of-a-document\n",
    "                if not self.strainer:\n",
    "                    page = requests.get(self.base_url, headers=headers)\n",
    "                    time.sleep(0.5)\n",
    "                    self.base_soup = BeautifulSoup(page.content, \"lxml\")\n",
    "                else:\n",
    "                    only_parse = SoupStrainer(self.strain_tag, attrs={\n",
    "                                              self.strain_attr_name: self.strain_attr_value})\n",
    "                    self.base_soup = BeautifulSoup(requests.get(\n",
    "                        self.base_url, headers=headers).content, \"lxml\", parse_only=only_parse)\n",
    "\n",
    "                time.sleep(1)\n",
    "                self.urls_to_scrape = eval(self.url_list_query)\n",
    "\n",
    "            else:\n",
    "                for url in list(self.base_url):\n",
    "                    if not self.strainer:\n",
    "                        page = requests.get(url, headers=headers)\n",
    "                        time.sleep(0.5)\n",
    "                        self.base_soup = BeautifulSoup(page.content, \"lxml\")\n",
    "                    else:\n",
    "                        only_parse = SoupStrainer(self.strain_tag, attrs={\n",
    "                                                  self.strain_attr_name: self.strain_attr_value})\n",
    "                        self.base_soup = BeautifulSoup(requests.get(\n",
    "                            url, headers=headers).content, \"lxml\", parse_only=only_parse)\n",
    "                    time.sleep(1)\n",
    "                    self.urls_to_scrape += eval(self.url_list_query)\n",
    "        self.urls_to_scrape = list(set(self.urls_to_scrape))\n",
    "\n",
    "    def scrape_em(self):\n",
    "        self.relevant_articles = {}\n",
    "        self.scraped_count = 0\n",
    "        self.skip_count = 0\n",
    "        self.too_old = 0\n",
    "        self.iteration = 0\n",
    "        self.skip_ind = []\n",
    "        self.old_ind = []\n",
    "        for url in self.urls_to_scrape:\n",
    "            time.sleep(0.2)\n",
    "            self.iteration += 1\n",
    "            summary = None\n",
    "            title = None\n",
    "            date = None\n",
    "            try:\n",
    "                with requests.Session() as s:\n",
    "                    page = s.get(url, headers=headers)\n",
    "                    if self.source in ['Semiconductor Engineering', 'Reuters', 'Recode']:\n",
    "                        article = BeautifulSoup(page.content, \"html5lib\")\n",
    "                    else:\n",
    "                        article = BeautifulSoup(page.content, \"lxml\")\n",
    "                    date = pd.to_datetime(eval(self.date_loc).strip().replace(\n",
    "                        '\\\\xa0', '').replace(' -\\nBy:', ''), format=self.date_format).date()\n",
    "                    if (date - dt.date.today()).days >= -max_age:\n",
    "                        if self.source == 'Autoblog':\n",
    "                            try:\n",
    "                                summary = eval(self.sum_loc)\n",
    "                                summary = replace_em(\n",
    "                                    summary[0].text + ' '+summary[1].text + ' '+summary[2].text)\n",
    "                            except:\n",
    "                                summary = ' '.join(article.find('div', attrs={\n",
    "                                                   'class': 'post-body'}).text.replace('\\\\t', '').replace('\\\\n\\\\n', '\\n').split('\\n')[1:4])\n",
    "                        else:\n",
    "                            summary = eval(self.sum_loc)\n",
    "                            try:\n",
    "                                summary = replace_em(\n",
    "                                    summary[0].text + ' '+summary[1].text + ' '+summary[2].text)\n",
    "                            except:\n",
    "                                # Some articles are actually just one paragraph\n",
    "                                summary = replace_em(summary[0])\n",
    "                        title = eval(self.title_loc).replace('â€™', \"'\").replace(\n",
    "                            '\\\\xa0', ' ').replace('\\\\n', '').lstrip().replace('  ', '')\n",
    "                        temp = page_scan(title, summary, url,\n",
    "                                         date, self.source)\n",
    "                        if temp != 'Most definitely nope':\n",
    "                            self.relevant_articles[self.scraped_count] = temp\n",
    "                        self.scraped_count += 1\n",
    "                    else:\n",
    "                        self.too_old += 1\n",
    "                        self.old_ind.append(self.iteration-1)\n",
    "            except Exception as exc:\n",
    "                print(\n",
    "                    f\"{str(exc)}: {url} \\ndate:{date}\\ntitle:{title}\\nsummary:{summary}\")\n",
    "                self.skip_count += 1\n",
    "                self.skip_ind.append(self.iteration-1)\n",
    "                continue\n",
    "        self.relevant_df = pd.DataFrame.from_dict(self.relevant_articles).T\n",
    "        if not self.relevant_df.empty:\n",
    "            self.relevant_df.drop_duplicates('link', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters for each website scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T13:30:37.588794Z",
     "start_time": "2019-01-07T13:30:37.551893Z"
    }
   },
   "outputs": [],
   "source": [
    "# 'article' is the variable that stores the BeautifulSoup soup for a particular article page. e.g. \"energy.gov/some-article.\"\n",
    "# The values of date_loc, sum_loc, and title_loc should be the BeautifulSoup commands for accessing the date location, \n",
    "# summary location and title location, respectively.\n",
    "\n",
    "scraper_dict = {'MIT': {'url': 'http://news.mit.edu/mit-news',\n",
    "                        'source': 'MIT',\n",
    "                        'strain_tag': 'ul',\n",
    "                        'strain_attr_name': 'class',\n",
    "                        'strain_attr_value': 'view-mit-news clearfix',\n",
    "                        'url_list_query': \"['http://news.mit.edu'+item.a['href'] for item in self.base_soup.find('ul', class_='view-mit-news clearfix').find_all('li')]\",\n",
    "                        'date_loc': \"article.find('span', attrs={'itemprop':'datePublished'}).text\",\n",
    "                        'date_format': None,\n",
    "                        'sum_loc': \"article.find('div', attrs={'class': 'field-item even'}).find_all('p')\",\n",
    "                        'title_loc': \"article.find('h1', attrs={'class':'article-heading'}).text\",\n",
    "                        'strain_bool': True},\n",
    "                'SemEng': {'url': 'http://semiengineering.com/category-main-page-iot-security/',\n",
    "                           'source': 'Semiconductor Engineering',\n",
    "                           'strain_tag': 'div',\n",
    "                           'strain_attr_name': 'class',\n",
    "                           'strain_attr_value': 'l_col',\n",
    "                           'url_list_query': \"[item['href'] for item in self.base_soup.find('div', class_='l_col').find_all('a', href=True,title=True)]\",\n",
    "                           'date_loc': \"article.find('div',class_='loop_post_meta').contents[0]\",\n",
    "                           'date_format': None,\n",
    "                           'sum_loc': \"article.find('div', class_='post_cnt post_cnt_first_letter').find_all('p')[1:4]\",\n",
    "                           'title_loc': \"article.find('h1', class_='post_title').text\",\n",
    "                           'strain_bool': True},\n",
    "                'Quartz': {'url': 'https://qz.com/search/self-driving',\n",
    "                           'source': 'Quartz',\n",
    "                           'url_list_query': \"['https://qz.com' + a['href'] for a in self.base_soup.find_all('a', class_='_5ff1a')]\",\n",
    "                           'date_loc': \"article.time.text\",\n",
    "                           'date_format': None,\n",
    "                           'sum_loc': \"article.find_all('p')[:3]\",\n",
    "                           'title_loc': \"article.h1.text\",\n",
    "                           'strain_bool': False},\n",
    "                            # Note: member exclusive articles for Quartz will be skipped.\n",
    "                'Recode': {'url': 'https://www.recode.net/',\n",
    "                           'source': 'Recode',\n",
    "                           'strain_tag': 'a',\n",
    "                           'strain_attr_name': 'data-analytics-link',\n",
    "                           'strain_attr_value': 'article',\n",
    "                           'url_list_query': \"[item['href'] for item in self.base_soup.find_all('a', attrs={'data-analytics-link':'article'})]\",\n",
    "                           'date_loc': \"article.time.text.replace('\\\\n', '')\",\n",
    "                           'date_format': None,\n",
    "                           'sum_loc': \"article.find_all('p')\",\n",
    "                           'title_loc': \"article.h1.text\",\n",
    "                           'strain_bool': True},\n",
    "                'GovTech': {'url': 'http://www.govtech.com/fs/transportation/',\n",
    "                            'source': 'GovTech',\n",
    "                            'url_list_query': \"[item.a['href'] for item in self.base_soup.find_all(class_=['sub-feature-article','feature-article'])]\",\n",
    "                            'date_loc': \"article.find('span', class_='date').text.strip()\",\n",
    "                            'date_format': None,\n",
    "                            'sum_loc': \"[item for item in article.find(class_='col-md-10').find_all('div') if len(str(item)) > 12] \\\n",
    "                                        if len([item for item in article.find(class_='col-md-10').find_all('p')]) < 3 \\\n",
    "                                        else [item for item in article.find(class_='col-md-10').find_all('p')]\",\n",
    "                            'title_loc': \"article.find('h1').text.strip()\",\n",
    "                            'strain_bool': False},\n",
    "                'Reuters': {'url': ['https://www.reuters.com/news/technology',\n",
    "                                    'https://www.reuters.com/news/archive/technologynews?view=page&page=2',\n",
    "                                    'https://www.reuters.com/news/archive/technologynews?view=page&page=3',\n",
    "                                    'https://www.reuters.com/news/archive/technologynews?view=page&page=4',\n",
    "                                    'https://www.reuters.com/news/archive/technologynews?view=page&page=5'],\n",
    "                            'source': 'Reuters',\n",
    "                            'url_list_query': \"['https://www.reuters.com'+item.a['href'] for item in self.base_soup.find_all('div', class_='story-content')]\",\n",
    "                            'date_loc': \"article.find('div', attrs={'class':'ArticleHeader_date'}).text.split('/')[0]\",\n",
    "                            'date_format': None,\n",
    "                            'sum_loc': \"article.find('div', attrs={'class':'StandardArticleBody_body'}).find_all('p')\",\n",
    "                            'title_loc': \"article.h1.text\",\n",
    "                            'strain_bool': False},\n",
    "                'CityLab': {'url': 'https://www.citylab.com/transportation/',\n",
    "                            'source': 'Citylab',\n",
    "                            'strain_tag': ['h2', 'h1'],\n",
    "                            'strain_attr_name': 'class', 'strain_attr_value': ['c-promo__hed', 'c-river-item__hed c-river-item__hed--'],\n",
    "                            'url_list_query': \"[item.a['href'] for item in self.base_soup.find_all(['h1','h2'], class_=['c-promo__hed','c-river-item__hed c-river-item__hed--'])]\",\n",
    "                            'date_loc': \"article.time.text\",\n",
    "                            'date_format': None,\n",
    "                            'sum_loc': \"article.find_all('p')[1:]\",\n",
    "                            'title_loc': \"article.h1.text\",\n",
    "                            'strain_bool': True},\n",
    "                'Autoblog': {'url': ['https://www.autoblog.com/archive/']\n",
    "                                     + ['https://www.autoblog.com/archive/pg-' + str(i) for i in range(2,6)],\n",
    "                             'source': 'Autoblog',\n",
    "                             'strain_tag': 'h6',\n",
    "                             'strain_attr_name': 'class',\n",
    "                             'strain_attr_value': 'record-heading',\n",
    "                             'url_list_query': \"['https://www.autoblog.com' + header.a['href'] for header in self.base_soup.find_all('h6', class_ = 'record-heading')]\",\n",
    "                             'date_loc': \"article.find('div', class_='post-date').text.strip().split(' at')[0]\",\n",
    "                             'date_format': None,\n",
    "                             'sum_loc': \"article.find('div', attrs={'class':'post-body'}).find_all('p')\",\n",
    "                             'title_loc': \"article.h1.text\",\n",
    "                             'strain_bool': True},\n",
    "                'Electrek': {'url': ['https://electrek.co/'] + \n",
    "                                     ['https://electrek.co/page/' + str(i) for i in range(2,6)],\n",
    "                             'source': 'Electrek',\n",
    "                             'strain_tag': 'h1',\n",
    "                             'strain_attr_name': 'class', 'strain_attr_value': 'post-title',\n",
    "                             'url_list_query': \"[item.a['href'] for item in self.base_soup.find_all('h1', class_='post-title')]\",\n",
    "                             'date_loc': \"article.find('p', class_='time-twitter').text\",\n",
    "                             'date_format': None,\n",
    "                             'sum_loc': \"article.find('div', class_='post-body').find_all('p')[1:]\",\n",
    "                             'title_loc': \"article.find('h1', class_='post-title').text\",\n",
    "                             'strain_bool': True},\n",
    "                'The Verge': {'url': 'https://www.theverge.com/transportation',\n",
    "                              'source': 'The Verge',\n",
    "                              'strain_tag': 'h2',\n",
    "                              'strain_attr_name': 'class', 'strain_attr_value': 'c-entry-box--compact__title',\n",
    "                              'url_list_query': \"[item.a['href'] for item in self.base_soup.find_all('h2', class_='c-entry-box--compact__title')]\",\n",
    "                              'date_loc': \"article.time.text\",\n",
    "                              'date_format': None,\n",
    "                              'sum_loc': \"article.find_all('p')\",\n",
    "                              'title_loc': \"article.h1.text\",\n",
    "                              'strain_bool': True},\n",
    "                'Crunchbase': {'url': 'https://news.crunchbase.com/',\n",
    "                               'source': 'Crunchbase',\n",
    "                               'url_list_query': \"[item.a['href'] for item in self.base_soup.find_all('h2',class_=['entry-title h3','entry-title h5'])]\",\n",
    "                               'date_loc': \"article.find('div', class_='meta-item herald-date').text\",\n",
    "                               'date_format': None,\n",
    "                               'sum_loc': \"article.find('div', class_='entry-content herald-entry-content').find_all('p')[1:]\",\n",
    "                               'title_loc': \"article.find('h1', class_='entry-title h1').text\",\n",
    "                               'strain_bool': False},\n",
    "                'Truck News': {'url': ['https://www.trucknews.com/news',\n",
    "                                       'https://www.trucknews.com/news/page/2/'],\n",
    "                               'source': 'Truck News',\n",
    "                               'url_list_query': \"[item.a['href'] for item in self.base_soup.find('ul', class_='media-list').find_all('h4')]\",\n",
    "                               'date_loc': \"article.find('div', class_ = 'well').find('p').text.split('by')[0].strip()\",\n",
    "                               'date_format': None,\n",
    "                               'sum_loc': \"[p.text for p in article.find('div', class_ = 'the-content').find_all('p')]\",\n",
    "                               'title_loc': \"article.find('h2').text.strip()\",\n",
    "                               'strain_bool': False},\n",
    "                'Trucks.com': {'url': ['https://www.trucks.com/category/news/tech/autonomous-vehicles/',\n",
    "                                       'https://www.trucks.com/category/editors-picks/'],\n",
    "                               'source': 'Trucks.com',\n",
    "                               'url_list_query': \"[item.find(['h2','div'], attrs={'class':['title','h4']}).a['href'] for item in self.base_soup.find_all('div', attrs={'class':['content-block','cb-meta container-page-trucks']})]\",\n",
    "                               'date_loc': \"article.find('div',class_='date-author').text.strip().split(' by')[0]\",\n",
    "                               'date_format': None,\n",
    "                               'sum_loc': \"article.find('section', attrs={'itemprop':'articleBody'}).find_all('p', attrs={'class':None})\",\n",
    "                               'title_loc': \"article.h1.text\",\n",
    "                               'strain_bool': False},\n",
    "                'TechCrunch': {'url': ['https://techcrunch.com/', \n",
    "                                       'https://techcrunch.com/page/2/', \n",
    "                                       'https://techcrunch.com/page/3/', \n",
    "                                       'https://techcrunch.com/page/4/'], \n",
    "                               'source': 'TechCrunch',\n",
    "                               'strain_tag': 'a',\n",
    "                               'strain_attr_name': 'class',\n",
    "                               'strain_attr_value': 'post-block__title__link',\n",
    "                               'url_list_query': \"[item['href'] for item in self.base_soup.find_all('a', class_='post-block__title__link')]\",\n",
    "                               'date_loc': \"url[23:33]\",\n",
    "                               'date_format': None,\n",
    "                               'sum_loc': \"article.find('div', attrs={'class':'article-content'}).find_all('p')\",\n",
    "                               'title_loc': \"article.find('h1', attrs={'class':'article__title'}).text\",\n",
    "                               'strain_bool': True},\n",
    "                'Charged EVs': {'url': ['https://chargedevs.com/category/newswire/', 'https://chargedevs.com/category/newswire/page/2/'],\n",
    "                                'source': 'Charged EVs',\n",
    "                                'strain_tag': 'h3',\n",
    "                                'strain_attr_name': 'class',\n",
    "                                'strain_attr_value': 'h2',\n",
    "                                'url_list_query': '[item.a[\"href\"] for item in self.base_soup.find_all(\"h3\", class_=\"h2\")]',\n",
    "                                'date_loc': \"article.find('time').text\",\n",
    "                                'date_format': None,\n",
    "                                'sum_loc': \"article.find('section',class_='entry-content clearfix').find_all('p')\",\n",
    "                                'title_loc': \"article.find('h2', class_='page-title').text\",\n",
    "                                'strain_bool': True},\n",
    "                'ARS Technica': {'url': 'https://arstechnica.com/cars/',\n",
    "                                 'source': 'ARS Technica',\n",
    "                                 'strain_tag': 'a',\n",
    "                                 'strain_attr_name': 'class',\n",
    "                                 'strain_attr_value': 'overlay',\n",
    "                                 'url_list_query': \"[item['href'] for item in self.base_soup.find_all('a', attrs={'class': 'overlay'})]\",\n",
    "                                 'date_loc': \"article.find('time', attrs={'class':'date'}).text\",\n",
    "                                 'date_format': None,\n",
    "                                 'sum_loc': \"article.find('div', attrs={'itemprop':'articleBody'}).find_all('p', attrs={'class':None})\",\n",
    "                                 'title_loc': \"article.h1.text\",\n",
    "                                 'strain_bool': True},\n",
    "                'Venture Beat': {'url': 'https://venturebeat.com/category/transportation/',\n",
    "                                 'source': 'Venture Beat',\n",
    "                                 'url_list_query': \"[item.a['href'] for item in self.base_soup.select('h2.article-title')]+[item.a['href'] for item in self.base_soup.select('article')]\",\n",
    "                                 'date_loc': \"article.find('meta', attrs={'property':'article:published_time'})['content']\",\n",
    "                                 'date_format': None,\n",
    "                                 'sum_loc': \"[p.text for p in article.find('div', class_ = 'article-content').find_all('p')]\",\n",
    "                                 'title_loc': \"article.find('h1').text\",\n",
    "                                 'strain_bool': False},\n",
    "                'IEEE Spectrum': {'url': 'https://spectrum.ieee.org/transportation', 'source': 'IEEE Spectrum',\n",
    "                                  'strain_tag': 'article',\n",
    "                                  'strain_attr_name': 'class',\n",
    "                                  'url_list_query': \"['https://spectrum.ieee.org'+item.a['href'] for item in self.base_soup.find_all('article')]\",\n",
    "                                  'strain_attr_value': 'item sml_article transportation',\n",
    "                                  'date_loc': \"article.label.text\",\n",
    "                                  'date_format': '%d %b %Y | %H:%M GMT',\n",
    "                                  'sum_loc': \"article.find_all('p', limit=5)\",\n",
    "                                  'title_loc': \"article.h1.text\",\n",
    "                                  'strain_bool': True},\n",
    "                'Transport Topics': {'url': ['https://www.ttnews.com/government',\n",
    "                                             'https://www.ttnews.com/government?page=1',\n",
    "                                             'https://www.ttnews.com/government?page=2',\n",
    "                                             'https://www.ttnews.com/government?page=3',\n",
    "                                             'https://www.ttnews.com/business',\n",
    "                                             'https://www.ttnews.com/business?page=1',\n",
    "                                             'https://www.ttnews.com/business?page=2',\n",
    "                                             'https://www.ttnews.com/technology',\n",
    "                                             'https://www.ttnews.com/technology?page=1',\n",
    "                                             'https://www.ttnews.com/technology?page=2',\n",
    "                                             'https://www.ttnews.com/equipment',\n",
    "                                             'https://www.ttnews.com/equipment?page=1',\n",
    "                                             'https://www.ttnews.com/equipment?page=2'],\n",
    "                                     'source': 'Transport Topics',\n",
    "                                     'url_list_query': \"['https://www.ttnews.com'+item.a['href'] for item in self.base_soup.find_all('h2', class_='content-access-1067')]\",\n",
    "                                     'date_loc': \"article.find('span',class_='date-display-single')['content']\",\n",
    "                                     'date_format': None,\n",
    "                                     'sum_loc': \"[p for p in article.find_all('p') if p.text and len(p.text)>10]\",\n",
    "                                     'title_loc': \"article.find('h1').text\",\n",
    "                                     'strain_bool': False},\n",
    "                'GreenCarCongress': {'url': ['http://www.greencarcongress.com/', 'http://www.greencarcongress.com/page/2/'],\n",
    "                                     'source': 'GreenCarCongress',\n",
    "                                     'strain_tag': 'article',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'post entry',\n",
    "                                     'url_list_query': \"[item.a['href'] for item in self.base_soup.find_all('article', attrs={'class': 'post entry'})]\",\n",
    "                                     'date_loc': \"article.find('span', attrs={'class':'entry-date'}).a.text\",\n",
    "                                     'date_format': None,\n",
    "                                     'sum_loc': \"article.find_all('p', limit=5)\",\n",
    "                                     'title_loc': \"article.h2.a.text\",\n",
    "                                     'strain_bool': True},\n",
    "                'Green Car Reports': {'url': 'https://www.greencarreports.com/news',\n",
    "                                     'source': 'Green Car Reports',\n",
    "                                     'strain_tag': 'div',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'right-side',\n",
    "                                     'url_list_query': \"['https://www.greencarreports.com' + item.h2.a['href'] for item in self.base_soup.find_all('div', attrs={'class': 'right-side'})]\",\n",
    "                                     'date_loc': \"article.find('div', class_='by-line-comments-views-date').span.text\",\n",
    "                                     'date_format': None,\n",
    "                                     'sum_loc': \"article.find('div', class_='article_content').find_all('p', limit = 5)\",\n",
    "                                     'title_loc': \"article.h1.text\",\n",
    "                                     'strain_bool': True},\n",
    "                'The Fuse': {'url': 'http://energyfuse.org/category/autonomous-vehicles/',\n",
    "                                     'source': 'The Fuse',\n",
    "                                     'strain_tag': 'div',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'category-content-block active',\n",
    "                                     'url_list_query': \"[a['href'] for a in self.base_soup.find_all('a', class_='full-block-link')]\",\n",
    "                                     'date_loc': \"article.h2.text.split('| ')[-1]\",\n",
    "                                     'date_format': None,\n",
    "                                     'sum_loc': \"article.find('div', class_='content-wrapper').find('p')\",\n",
    "                                     'title_loc': \"article.h1.text\",\n",
    "                                     'strain_bool': True},\n",
    "                'Business Wire': {'url': 'https://www.businesswire.com/portal/site/home/news/',\n",
    "                                     'source': 'Business Wire',\n",
    "                                     'strain_tag': 'a',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'bwTitleLink',\n",
    "                                     'url_list_query': \"['https://www.businesswire.com' + a['href'] for a in self.base_soup.find_all('a', class_='bwTitleLink') if '/en/' in a['href']]\",\n",
    "                                     'date_loc': \"' '.join(article.find('time').text.split()[:3])\",\n",
    "                                     'date_format': None,\n",
    "                                     'sum_loc': \"['<p>' + p.text + '</p>' for p in article.find('div', class_='bw-release-story').find_all('p')]\",\n",
    "                                     'title_loc': \"' '.join(article.h1.text.strip().split())\",\n",
    "                                     'strain_bool': True},\n",
    "                'U.S. Department of Energy': {'url': 'https://www.energy.gov/listings/energy-news',\n",
    "                                     'source': 'U.S. Department of Energy',\n",
    "                                     'strain_tag': 'a',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'title-link',\n",
    "                                     'url_list_query': \"['https://www.energy.gov' + a['href'] for a in self.base_soup.find_all('a', class_='title-link')]\",\n",
    "                                     'date_loc': \"article.find('div', class_='node-hero-date').text\",\n",
    "                                     'date_format': None,\n",
    "                                     'sum_loc': \"article.find('div', class_='field-items').find_all('p', limit = 3)\",\n",
    "                                     'title_loc': \"article.h1.text\",\n",
    "                                     'strain_bool': True},\n",
    "               \n",
    "                'jmtonline': {'url': 'https://link.springer.com/journal/40534/onlineFirst/page/1',\n",
    "                                     'source': 'Journal of Modern Transportation',\n",
    "                                     'strain_tag': 'div',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'toc-item',\n",
    "                                     'url_list_query': \"['https://link.springer.com' + title.a['href'] for title in self.base_soup.find_all('div', class_='toc-item') if title.p.text == 'OriginalPaper']\",\n",
    "                                     'date_loc': \"article.time.text\",\n",
    "                                     'date_format': None,\n",
    "                                     'sum_loc': \"article.find('section', class_='Abstract').p\",\n",
    "                                     'title_loc': \"article.h1.text\",\n",
    "                                     'strain_bool': True},\n",
    "                \n",
    "                'Jalopnik': {'url':  'https://jalopnik.com/c/news',\n",
    "                                     'source': 'Jalopnik',\n",
    "                                     'strain_tag': 'article',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'postlist__item--compact',\n",
    "                                     'url_list_query': \"[article.a['href'] for article in self.base_soup.find_all('article', class_='postlist__item--compact')]\",\n",
    "                                     'date_loc': \"article.time['datetime'].split('T')[0]\",\n",
    "                                     'date_format': None,\n",
    "                                     # The below method of searching for the summary filters out some odd divs in the middle of articles.\n",
    "                                     'sum_loc': \"[par for par in list(article.find('div', class_='post-content entry-content js_entry-content ').children) if str(par)[:3] =='<p>']\",\n",
    "                                     'title_loc': \"article.h1.text\",\n",
    "                                     'strain_bool': True},\n",
    "                \n",
    "                'Governing': {'url':  'http://www.governing.com/news/headlines',\n",
    "                                     'source': 'Governing',\n",
    "                                     'strain_tag': 'article',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'postlist__item--compact',\n",
    "                                     'url_list_query': \"[article.a['href'] for article in self.base_soup.find_all('article', class_='postlist__item--compact')]\",\n",
    "                                     'date_loc': \"article.time['datetime'].split('T')[0]\",\n",
    "                                     'date_format': None,\n",
    "                                     # The below method of searching for the summary filters out some odd divs in the middle of articles.\n",
    "                                     'sum_loc': \"[par for par in list(article.find('div', class_='post-content entry-content js_entry-content ').children) if str(par)[:3] =='<p>']\",\n",
    "                                     'title_loc': \"article.h1.text\",\n",
    "                                     'strain_bool': True},\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the scrapers\n",
    "Note: there will be a couple errors, especially with Autoblog. The scraper for that site still picks up a couple irrelevant items that it can't handle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing a single scraper - only needed when adding new sites (don't want to run all of them over and over...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T13:37:17.149891Z",
     "start_time": "2019-01-07T13:37:04.771546Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Governing article(s) scraped\n",
      "0 Governing article(s) skipped due to error\n",
      "0 Governing article(s) skipped due to age\n",
      "0 relevant article(s) collected\n"
     ]
    }
   ],
   "source": [
    "scrape_specs = {}\n",
    "scraypahs = {}\n",
    "temp_start_time = time.time()\n",
    "\n",
    "name = 'Governing'\n",
    "scraypahs[name] = scraypah(scraper_dict[name])\n",
    "scraypahs[name].get_urls()\n",
    "scraypahs[name].scrape_em()\n",
    "scrape_specs = print_results(scraypahs[name].source, scraypahs[name].scraped_count, scraypahs[name].skip_count,\n",
    "                             scraypahs[name].too_old, scraypahs[name].relevant_df, round(\n",
    "                                 time.time()-temp_start_time, 2),\n",
    "                             scrape_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T13:36:05.340083Z",
     "start_time": "2019-01-07T13:30:37.595775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MIT\n",
      "7 MIT article(s) scraped\n",
      "0 MIT article(s) skipped due to error\n",
      "25 MIT article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "SemEng\n",
      "'NoneType' object is not callable: https://semiengineering.com/inferencing-at-the-edge/ \n",
      "date:2019-01-10\n",
      "title:None\n",
      "summary:[<p><span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe allowfullscreen=\"true\" class=\"youtube-player\" height=\"360\" src=\"https://www.youtube.com/embed/1BTxwew--5U?version=3&amp;rel=1&amp;fs=1&amp;autohide=2&amp;showsearch=0&amp;showinfo=1&amp;iv_load_policy=1&amp;wmode=transparent\" style=\"border:0;\" type=\"text/html\" width=\"640\"></iframe></span></p>]\n",
      "4 Semiconductor Engineering article(s) scraped\n",
      "1 Semiconductor Engineering article(s) skipped due to error\n",
      "13 Semiconductor Engineering article(s) skipped due to age\n",
      "2 relevant article(s) collected\n",
      "\n",
      "Quartz\n",
      "'NoneType' object has no attribute 'text': https://qz.com/1495144/chinas-self-driving-efforts-need-to-accelerate/ \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "0 Quartz article(s) scraped\n",
      "1 Quartz article(s) skipped due to error\n",
      "9 Quartz article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "Recode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EBarnard\\AppData\\Local\\Continuum\\anaconda3-1\\lib\\site-packages\\dateutil\\parser\\_parser.py:1204: UnknownTimezoneWarning: tzname EST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  category=UnknownTimezoneWarning)\n",
      "C:\\Users\\EBarnard\\AppData\\Local\\Continuum\\anaconda3-1\\lib\\site-packages\\dateutil\\parser\\_parser.py:1204: UnknownTimezoneWarning: tzname EDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  category=UnknownTimezoneWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 Recode article(s) scraped\n",
      "0 Recode article(s) skipped due to error\n",
      "16 Recode article(s) skipped due to age\n",
      "2 relevant article(s) collected\n",
      "\n",
      "GovTech\n",
      "7 GovTech article(s) scraped\n",
      "0 GovTech article(s) skipped due to error\n",
      "40 GovTech article(s) skipped due to age\n",
      "2 relevant article(s) collected\n",
      "\n",
      "Reuters\n",
      "61 Reuters article(s) scraped\n",
      "0 Reuters article(s) skipped due to error\n",
      "0 Reuters article(s) skipped due to age\n",
      "3 relevant article(s) collected\n",
      "\n",
      "CityLab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EBarnard\\AppData\\Local\\Continuum\\anaconda3-1\\lib\\site-packages\\dateutil\\parser\\_parser.py:1204: UnknownTimezoneWarning: tzname ET identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  category=UnknownTimezoneWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 Citylab article(s) scraped\n",
      "0 Citylab article(s) skipped due to error\n",
      "7 Citylab article(s) skipped due to age\n",
      "3 relevant article(s) collected\n",
      "\n",
      "Autoblog\n",
      "HTTPSConnectionPool(host='www.autoblog.comhttps', port=443): Max retries exceeded with url: //www.autoblog.com/photos/top-10-classic-cars-on-the-rise/ (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000000000B7D18D0>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond')): https://www.autoblog.comhttps://www.autoblog.com/photos/top-10-classic-cars-on-the-rise/ \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'text': https://www.autoblog.com/2019/01/10/2019-genesis-g70-review-comparison/ \n",
      "date:2019-01-10\n",
      "title:None\n",
      "summary:None\n",
      "HTTPSConnectionPool(host='www.autoblog.comhttps', port=443): Max retries exceeded with url: //www.autoblog.com/photos/best-new-car-deals/ (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000000000C4BABE0>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond')): https://www.autoblog.comhttps://www.autoblog.com/photos/best-new-car-deals/ \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "HTTPSConnectionPool(host='www.autoblog.comhttps', port=443): Max retries exceeded with url: //www.autoblog.com/photos/best-selling-cars/ (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000000000BF8DD30>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond')): https://www.autoblog.comhttps://www.autoblog.com/photos/best-selling-cars/ \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'text': https://www.autoblog.com/2019/01/09/2019-jaguar-i-pace-review-quick-spin/ \n",
      "date:2019-01-09\n",
      "title:None\n",
      "summary:None\n",
      "100 Autoblog article(s) scraped\n",
      "5 Autoblog article(s) skipped due to error\n",
      "0 Autoblog article(s) skipped due to age\n",
      "5 relevant article(s) collected\n",
      "\n",
      "Electrek\n",
      "44 Electrek article(s) scraped\n",
      "0 Electrek article(s) skipped due to error\n",
      "0 Electrek article(s) skipped due to age\n",
      "6 relevant article(s) collected\n",
      "\n",
      "The Verge\n",
      "'NoneType' object has no attribute 'text': https://www.theverge.com/2018/12/28/18152942/2018-tech-recap-facebook-google-microsoft-twitter-amazon \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "25 The Verge article(s) scraped\n",
      "1 The Verge article(s) skipped due to error\n",
      "13 The Verge article(s) skipped due to age\n",
      "9 relevant article(s) collected\n",
      "\n",
      "Crunchbase\n",
      "('Unknown string format:', '5 hours ago'): https://news.crunchbase.com/news/drone-in-aisle-one-pensa-systems-raises-5m-series-a-to-make-retail-more-efficient/ \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "('Unknown string format:', '4 hours ago'): https://news.crunchbase.com/news/how-the-government-shutdown-could-delay-unicorn-ipos/ \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "11 Crunchbase article(s) scraped\n",
      "2 Crunchbase article(s) skipped due to error\n",
      "5 Crunchbase article(s) skipped due to age\n",
      "2 relevant article(s) collected\n",
      "\n",
      "Truck News\n",
      "21 Truck News article(s) scraped\n",
      "0 Truck News article(s) skipped due to error\n",
      "9 Truck News article(s) skipped due to age\n",
      "1 relevant article(s) collected\n",
      "\n",
      "Trucks.com\n",
      "5 Trucks.com article(s) scraped\n",
      "0 Trucks.com article(s) skipped due to error\n",
      "9 Trucks.com article(s) skipped due to age\n",
      "1 relevant article(s) collected\n",
      "\n",
      "TechCrunch\n",
      "('Unknown string format:', 'video-arti'): https://techcrunch.com/video-article/google-built-an-entire-theme-park-ride-in-the-ces-parking-lot-because-they-can/ \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "78 TechCrunch article(s) scraped\n",
      "1 TechCrunch article(s) skipped due to error\n",
      "0 TechCrunch article(s) skipped due to age\n",
      "7 relevant article(s) collected\n",
      "\n",
      "Charged EVs\n",
      "'NoneType' object is not callable: https://chargedevs.com/newswire/zf-friedrichshafen-to-invest-e800-million-in-hybrid-transmission-technology/ \n",
      "date:2019-01-11\n",
      "title:None\n",
      "summary:[<p>ZF Friedrichshafen, a German-based automotive driveline and chassis manufacturer, is set to invest â‚¬800 million in the electrification of its transmission technology. Over the next four years, ZF will put the investment into its primary transmission plant located in SaarbrÃ¼cken, Germany.</p>, <p>â€œThe share of hybrid drives in production will increase tenfold over the next few years –</p>]\n",
      "12 Charged EVs article(s) scraped\n",
      "1 Charged EVs article(s) skipped due to error\n",
      "7 Charged EVs article(s) skipped due to age\n",
      "3 relevant article(s) collected\n",
      "\n",
      "ARS Technica\n",
      "7 ARS Technica article(s) scraped\n",
      "0 ARS Technica article(s) skipped due to error\n",
      "23 ARS Technica article(s) skipped due to age\n",
      "3 relevant article(s) collected\n",
      "\n",
      "Venture Beat\n",
      "20 Venture Beat article(s) scraped\n",
      "0 Venture Beat article(s) skipped due to error\n",
      "20 Venture Beat article(s) skipped due to age\n",
      "13 relevant article(s) collected\n",
      "\n",
      "IEEE Spectrum\n",
      "1 IEEE Spectrum article(s) scraped\n",
      "0 IEEE Spectrum article(s) skipped due to error\n",
      "18 IEEE Spectrum article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "Transport Topics\n",
      "67 Transport Topics article(s) scraped\n",
      "0 Transport Topics article(s) skipped due to error\n",
      "30 Transport Topics article(s) skipped due to age\n",
      "9 relevant article(s) collected\n",
      "\n",
      "GreenCarCongress\n",
      "51 GreenCarCongress article(s) scraped\n",
      "0 GreenCarCongress article(s) skipped due to error\n",
      "29 GreenCarCongress article(s) skipped due to age\n",
      "16 relevant article(s) collected\n",
      "\n",
      "Green Car Reports\n",
      "20 Green Car Reports article(s) scraped\n",
      "0 Green Car Reports article(s) skipped due to error\n",
      "0 Green Car Reports article(s) skipped due to age\n",
      "1 relevant article(s) collected\n",
      "\n",
      "The Fuse\n",
      "0: http://energyfuse.org/this-week-in-avs-self-driving-grocery-deliveries-pave-coalition-formed-for-av-education-and-more/ \n",
      "date:2019-01-11\n",
      "title:None\n",
      "summary:<p><strong><span style=\"text-decoration: underline;\">Cruise and DoorDash are going to start using self-driving cars to deliver groceries and takeout</span></strong></p>\n",
      "0 The Fuse article(s) scraped\n",
      "1 The Fuse article(s) skipped due to error\n",
      "5 The Fuse article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "Business Wire\n",
      "24 Business Wire article(s) scraped\n",
      "0 Business Wire article(s) skipped due to error\n",
      "0 Business Wire article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "U.S. Department of Energy\n",
      "4 U.S. Department of Energy article(s) scraped\n",
      "0 U.S. Department of Energy article(s) skipped due to error\n",
      "21 U.S. Department of Energy article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "jmtonline\n",
      "0 Journal of Modern Transportation article(s) scraped\n",
      "0 Journal of Modern Transportation article(s) skipped due to error\n",
      "7 Journal of Modern Transportation article(s) skipped due to age\n",
      "0 relevant article(s) collected\n",
      "\n",
      "Jalopnik\n",
      "19 Jalopnik article(s) scraped\n",
      "0 Jalopnik article(s) skipped due to error\n",
      "1 Jalopnik article(s) skipped due to age\n",
      "1 relevant article(s) collected\n"
     ]
    }
   ],
   "source": [
    "scrape_specs = {}\n",
    "scraypahs = {}\n",
    "start_time = time.time()\n",
    "\n",
    "for site in list(scraper_dict.keys()):\n",
    "    temp_start_time = time.time()\n",
    "    print('\\n'+site)\n",
    "    scraypahs[site] = scraypah(scraper_dict[site])\n",
    "    scraypahs[site].get_urls()\n",
    "    scraypahs[site].scrape_em()\n",
    "    scrape_specs = print_results(scraypahs[site].source, scraypahs[site].scraped_count, scraypahs[site].skip_count,\n",
    "                                 scraypahs[site].too_old, scraypahs[site].relevant_df, round(\n",
    "                                     time.time()-temp_start_time, 2),\n",
    "                                 scrape_specs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T14:34:17.790747Z",
     "start_time": "2018-11-09T14:34:17.279486Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Errors</th>\n",
       "      <th>Pages Scraped</th>\n",
       "      <th>Relevant Articles</th>\n",
       "      <th>Time spent</th>\n",
       "      <th>Too old</th>\n",
       "      <th>Time per relevant article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MIT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.11</td>\n",
       "      <td>25.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Semiconductor Engineering</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.44</td>\n",
       "      <td>13.0</td>\n",
       "      <td>17.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quartz</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.73</td>\n",
       "      <td>9.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Recode</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.21</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.105000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GovTech</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>28.70</td>\n",
       "      <td>40.0</td>\n",
       "      <td>14.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Reuters</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>40.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Citylab</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.54</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.513333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Autoblog</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>133.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.692000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Electrek</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>29.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Verge</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>24.63</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.736667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Crunchbase</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.55</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.775000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Truck News</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.08</td>\n",
       "      <td>9.0</td>\n",
       "      <td>29.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Trucks.com</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.50</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>1.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>35.84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Charged EVs</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>43.05</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ARS Technica</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>25.98</td>\n",
       "      <td>23.0</td>\n",
       "      <td>8.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Venture Beat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>20.24</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.556923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>IEEE Spectrum</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.33</td>\n",
       "      <td>18.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Transport Topics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>67.11</td>\n",
       "      <td>30.0</td>\n",
       "      <td>7.456667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>GreenCarCongress</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>155.90</td>\n",
       "      <td>29.0</td>\n",
       "      <td>9.743750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Green Car Reports</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>The Fuse</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.24</td>\n",
       "      <td>5.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Business Wire</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>U.S. Department of Energy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.05</td>\n",
       "      <td>21.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Journal of Modern Transportation</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.46</td>\n",
       "      <td>7.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Jalopnik</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.540000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               index  Errors  Pages Scraped  \\\n",
       "0                                MIT     0.0            7.0   \n",
       "1          Semiconductor Engineering     1.0            4.0   \n",
       "2                             Quartz     1.0            0.0   \n",
       "3                             Recode     0.0           19.0   \n",
       "4                            GovTech     0.0            7.0   \n",
       "5                            Reuters     0.0           61.0   \n",
       "6                            Citylab     0.0           14.0   \n",
       "7                           Autoblog     5.0          100.0   \n",
       "8                           Electrek     0.0           44.0   \n",
       "9                          The Verge     1.0           25.0   \n",
       "10                        Crunchbase     2.0           11.0   \n",
       "11                        Truck News     0.0           21.0   \n",
       "12                        Trucks.com     0.0            5.0   \n",
       "13                        TechCrunch     1.0           78.0   \n",
       "14                       Charged EVs     1.0           12.0   \n",
       "15                      ARS Technica     0.0            7.0   \n",
       "16                      Venture Beat     0.0           20.0   \n",
       "17                     IEEE Spectrum     0.0            1.0   \n",
       "18                  Transport Topics     0.0           67.0   \n",
       "19                  GreenCarCongress     0.0           51.0   \n",
       "20                 Green Car Reports     0.0           20.0   \n",
       "21                          The Fuse     1.0            0.0   \n",
       "22                     Business Wire     0.0           24.0   \n",
       "23         U.S. Department of Energy     0.0            4.0   \n",
       "24  Journal of Modern Transportation     0.0            0.0   \n",
       "25                          Jalopnik     0.0           19.0   \n",
       "\n",
       "    Relevant Articles  Time spent  Too old  Time per relevant article  \n",
       "0                 0.0       40.11     25.0                        inf  \n",
       "1                 2.0       35.44     13.0                  17.720000  \n",
       "2                 0.0        6.73      9.0                        inf  \n",
       "3                 2.0       26.21     16.0                  13.105000  \n",
       "4                 2.0       28.70     40.0                  14.350000  \n",
       "5                 3.0       40.44      0.0                  13.480000  \n",
       "6                 3.0       13.54      7.0                   4.513333  \n",
       "7                 5.0      133.46      0.0                  26.692000  \n",
       "8                 6.0       29.22      0.0                   4.870000  \n",
       "9                 9.0       24.63     13.0                   2.736667  \n",
       "10                2.0       11.55      5.0                   5.775000  \n",
       "11                1.0       29.08      9.0                  29.080000  \n",
       "12                1.0       19.50      9.0                  19.500000  \n",
       "13                7.0       35.84      0.0                   5.120000  \n",
       "14                3.0       43.05      7.0                  14.350000  \n",
       "15                3.0       25.98     23.0                   8.660000  \n",
       "16               13.0       20.24     20.0                   1.556923  \n",
       "17                0.0       31.33     18.0                        inf  \n",
       "18                9.0       67.11     30.0                   7.456667  \n",
       "19               16.0      155.90     29.0                   9.743750  \n",
       "20                1.0       10.73      0.0                  10.730000  \n",
       "21                0.0        5.24      5.0                        inf  \n",
       "22                0.0       11.94      0.0                        inf  \n",
       "23                0.0       37.05     21.0                        inf  \n",
       "24                0.0        7.46      7.0                        inf  \n",
       "25                1.0       16.54      1.0                  16.540000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smart Mobility articles found: 89\n",
      "Alternative Fuel Vehicle articles found: 0\n",
      "21CTP articles found: 0\n",
      "Hyperloop articles found: 1\n"
     ]
    }
   ],
   "source": [
    "# Meta-data from the scrape session\n",
    "scrape_specs_df = pd.DataFrame.from_dict(scrape_specs).T.reset_index()\n",
    "scrape_specs_df['Time per relevant article'] = scrape_specs_df['Time spent'] / \\\n",
    "    scrape_specs_df['Relevant Articles']\n",
    "display(scrape_specs_df)\n",
    "\n",
    "# List all of the relevant news from each of the scrapers (each scraypah item has an attribute \"relevant_df\", which is a pandas\n",
    "# dataframe with all of the selected news items from that website)\n",
    "all_news_dfs = []\n",
    "for key, value in scraypahs.items():\n",
    "    all_news_dfs.append(value.relevant_df)\n",
    "\n",
    "# Stack all of the articles into a single dataframe and do some cleaning (drop duplicate articles)\n",
    "all_df = pd.concat(all_news_dfs)\n",
    "all_df = all_df[['title', 'date', 'AFV', 'CAV', '21CTP', 'Hyperloop',\n",
    "                 'summary', 'source', 'link']].sort_values('date', ascending=False)\n",
    "all_df.drop_duplicates(subset='title', inplace=True)\n",
    "all_df = all_df.replace('\\$', '$', regex=True)\n",
    "\n",
    "print('Smart Mobility articles found: {}'.format(\n",
    "    all_df['CAV'].sum().astype(int)))\n",
    "print('Alternative Fuel Vehicle articles found: {}'.format(\n",
    "    all_df['AFV'].sum().astype(int)))\n",
    "print('21CTP articles found: {}'.format(all_df['21CTP'].sum().astype(int)))\n",
    "print('Hyperloop articles found: {}'.format(\n",
    "    all_df['Hyperloop'].sum().astype(int)))\n",
    "\n",
    "# Populate meta-data columns (helpful for searching all news items in the future if we want)\n",
    "all_df['reason_for_tag'] = all_df.apply(which_keyword_found, axis=1)\n",
    "all_df['keywords'] = all_df['title'].str.strip().apply(keyword_pull)\n",
    "\n",
    "# Format for excel writing\n",
    "AFV_news = all_df[all_df['AFV'] == 1].sort_values(\n",
    "    'date', ascending=False).drop(['AFV', 'CAV', '21CTP', 'Hyperloop'], axis=1)\n",
    "CAV_news = all_df[all_df['CAV'] == 1].sort_values(\n",
    "    'date', ascending=False).drop(['AFV', 'CAV', '21CTP', 'Hyperloop'], axis=1)\n",
    "truck_news = all_df[all_df['21CTP'] == 1].sort_values(\n",
    "    'date', ascending=False).drop(['AFV', 'CAV', '21CTP', 'Hyperloop'], axis=1)\n",
    "hyperloop_news = all_df[all_df['Hyperloop'] == 1].sort_values(\n",
    "    'date', ascending=False).drop(['AFV', 'CAV', '21CTP', 'Hyperloop'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write dataframe to a spreadsheet\n",
    "CAVs on Monday, AFVs on Wednesday, 21CTP on Friday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T16:55:54.374736Z",
     "start_time": "2018-10-17T16:55:54.098473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monday!\n",
      "Some hyperloop stuff!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<win32com.gen_py.None.Workbook>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press enter to close Excel:  \n"
     ]
    }
   ],
   "source": [
    "if (dt.date.today().weekday() == scraper_sched['CAV']):\n",
    "    print('Monday!')\n",
    "    filename = f'cav_news_updates/{search_date}_cav_news_download.xls'\n",
    "    CAV_news.to_excel(filename)\n",
    "    if hyperloop_news.shape[0] > 0:\n",
    "        filename2 = f'hyperloop_news_updates/{search_date}_hyperloop_news_download.xls'\n",
    "        hyperloop_news.to_excel(filename2)\n",
    "        print('Some hyperloop stuff!')\n",
    "elif (dt.date.today().weekday() == scraper_sched['AFV']):\n",
    "    print('Wednesday!')\n",
    "    filename = f'afv_news_updates/{search_date}_afv_news_download.xls'\n",
    "    AFV_news.to_excel(filename)\n",
    "elif (dt.date.today().weekday() == scraper_sched['21CTP']):\n",
    "    print('Friday!')\n",
    "    filename = f'21CTP_news_updates/{search_date}_21CTP_news_download.xls'\n",
    "    truck_news.to_excel(filename)\n",
    "\n",
    "# # Open excel file to edit or add any additional news items\n",
    "cwd = os.getcwd()\n",
    "xls_file = cwd+'/'+filename\n",
    "\n",
    "excel = win32.gencache.EnsureDispatch('Excel.Application')\n",
    "excel.Visible = True\n",
    "\n",
    "# # open the file\n",
    "excel.Workbooks.Open(xls_file)\n",
    "\n",
    "# # wait before closing\n",
    "_ = input(\"Press enter to close Excel: \")\n",
    "excel.Application.Quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Word file from the news update spreadsheets\n",
    "Automatically does CAV on Mondays, AFV on Wednesdays, and 21CTP on Fridays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:25:20.873985Z",
     "start_time": "2018-10-17T17:25:20.757295Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAV\n"
     ]
    }
   ],
   "source": [
    "if dt.date.today().weekday() == scraper_sched['AFV']:\n",
    "    print('AFV')\n",
    "    gen_docx('AFV')\n",
    "elif dt.date.today().weekday() == scraper_sched['CAV']:\n",
    "    print('CAV')\n",
    "    gen_docx('CAV')\n",
    "elif dt.date.today().weekday() == scraper_sched['21CTP']:\n",
    "    print('21CTP')\n",
    "    gen_docx('21CTP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update news item tracking and news scraper meta-data databases\n",
    "Only run when **final** news item spreadsheet is saved in your working directory (i.e., after you have manually added other articles to the already-saved spreadsheet from the cell above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T11:30:00.039138Z",
     "start_time": "2018-09-12T11:30:00.031855Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is in case you go to upload this week's news items to the database, and realize you forgot to do last week's. Just replace\n",
    "# all instances of \"search_date\" in the next cell with \"last_week\"and run it. Make sure you switch them all back to \"search_date\"..\n",
    "last_week = str((pd.to_datetime(search_date) - dt.timedelta(days=7)).date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T21:47:24.974039Z",
     "start_time": "2018-11-19T21:47:24.165717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAV\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('news_updates.db')\n",
    "if (dt.date.today().weekday() == scraper_sched['CAV']) & (~db_update):\n",
    "    print('CAV')\n",
    "    pd.read_excel('cav_news_updates/{}_cav_news_download.xls'.format(search_date)\n",
    "                  ).to_sql('CAV', conn, if_exists='append', index=False)\n",
    "    db_update = True\n",
    "elif (dt.date.today().weekday() == scraper_sched['AFV']) & (~db_update):\n",
    "    print('AFV')\n",
    "    pd.read_excel('afv_news_updates/{}_afv_news_download.xls'.format(search_date)\n",
    "                  ).to_sql('AFV', conn, if_exists='append', index=False)\n",
    "    db_update = True\n",
    "conn.close()\n",
    "\n",
    "# This saves the meta-data from all of the scraper runs every Wednesday (print out \"scrape_specs_df\" to see what the meta-data includes)\n",
    "if dt.date.today().weekday() == 2:\n",
    "    conn = sqlite3.connect('news_updates_meta.db')\n",
    "    scrape_specs_df.drop(['Time spent', 'Time per relevant article'], axis=1).to_sql(\n",
    "        'news_updates_meta', conn, if_exists='append', index=False)\n",
    "    conn.close()\n",
    "    print('Uploaded metadata! So many datas!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Academic articles scraper (**NOTE** I only run this for CAVs, so only on Mondays)\n",
    "Dumps all recently-published articles (in the past week) and their abstracts into a word file. Only does a few journals right now. Check your working directory for a file called *{date} papers.docx* after you run the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T15:07:50.976312Z",
     "start_time": "2018-12-19T15:07:50.960358Z"
    }
   },
   "outputs": [],
   "source": [
    "def css_scraypuh(selenium_dict_value):\n",
    "    '''\n",
    "    bad_egg: Missing a key component (usually abstract), so skip printout/tracking\n",
    "    still_more: Date is still within past week, continue scraping!\n",
    "    '''\n",
    "    max_age = 7 # take this out\n",
    "    scraped_count = 0\n",
    "    papers = {}\n",
    "    \n",
    "    # Important -- otherwise variables could be referenced before assignment.\n",
    "    title = ''\n",
    "    date = ''\n",
    "    summary = ''\n",
    "    \n",
    "    if type(selenium_dict_value['url']) != list:\n",
    "        selenium_dict_value['url'] = [selenium_dict_value['url']]\n",
    "    for url in selenium_dict_value['url']:\n",
    "        load_count = 0\n",
    "        still_more = True\n",
    "    \n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html5lib\")\n",
    "    \n",
    "        # Click on CSS element to load more articles if available.\n",
    "        if selenium_dict_value['load_more_css'] != None:\n",
    "            for loadNo in range(selenium_dict_value['max_loads']):\n",
    "                driver.find_element_by_css_selector(selenium_dict_value['load_more_css']).click()\n",
    "            soup = BeautifulSoup(driver.page_source, \"html5lib\")\n",
    "        papers_to_scrape = eval(selenium_dict_value['url_list_query'])\n",
    "        \n",
    "        for paper in papers_to_scrape:\n",
    "            bad_egg = False\n",
    "            if not still_more or scraped_count > selenium_dict_value['max_scrapes']:\n",
    "                break\n",
    "            # Open article URL using selenium.\n",
    "            driver.get(paper)\n",
    "            soup = BeautifulSoup(driver.page_source, \"html5lib\")\n",
    "            # Get article publication date, title, and summary.\n",
    "            try:\n",
    "                # Click on CSS element to reveal the location of the publication date if needed.\n",
    "                if selenium_dict_value['date_css'] != None:\n",
    "                    driver.find_element_by_css_selector(selenium_dict_value['date_css']).click()\n",
    "                    soup = BeautifulSoup(driver.page_source, \"html5lib\")\n",
    "                # Scrape the publication date.\n",
    "                date = eval(selenium_dict_value['date_loc'])\n",
    "                date = dt.datetime.strptime(date, selenium_dict_value['date_format']).date()\n",
    "                # If the publication date shows the article is more recent than than max_age (in days) OR if articles are not \n",
    "                # ordered by date (as in the case of Detroit News) scrape the title and summary.\n",
    "                too_old = ((date - dt.date.today()).days <= -max_age)\n",
    "                if not too_old:\n",
    "                    # Click on CSS element to reveal the location of the title if needed.\n",
    "                    if selenium_dict_value['title_css'] != None:\n",
    "                        driver.find_element_by_css_selector(selenium_dict_value['title_css']).click()\n",
    "                        soup = BeautifulSoup(driver.page_source, \"html5lib\")\n",
    "                    # Scrape and clean the title.\n",
    "                    title = eval(selenium_dict_value['title_loc'])\n",
    "                    title = replace_em(title)\n",
    "                    # Click on CSS element to reveal the location of the summary if needed.\n",
    "                    if selenium_dict_value['sum_css'] != None:\n",
    "                        driver.find_element_by_css_selector(selenium_dict_value['sum_css']).click()\n",
    "                        soup = BeautifulSoup(driver.page_source, \"html5lib\")\n",
    "                    # Scrape and clean the summary.\n",
    "                    summary = eval(selenium_dict_value['sum_loc'])\n",
    "                    summary = ' '.join([replace_em(p.text) for p in summary]) # Assumes that the summary is a list of paragraphs.\n",
    "                    scraped_count += 1 \n",
    "                    papers[scraped_count] = {\n",
    "                            'title': title, 'summary': summary, 'link': paper, \n",
    "                            'source': selenium_dict_value['source'], 'date': date}\n",
    "                    \n",
    "                # If article is too old, set still_more = False (if articles are ordered by date).  \n",
    "                # This will break out of the for loop so that paypuh_scraypuh() stops scraping papers from this source.\n",
    "                else:\n",
    "                    if selenium_dict_value['ordered_bool']:\n",
    "                        still_more = False\n",
    "            \n",
    "            except Exception as e:\n",
    "                bad_egg = True\n",
    "                #print('bad egg in {}: {}'.format(selenium_dict_value['source'], paper))\n",
    "                # Print out exception info.\n",
    "                #print('Exception info: ', sys.exc_info(), '\\n')\n",
    "                pass\n",
    "            \n",
    "    # Print the number of papers scraped from each source.\n",
    "    print('{} new papers in {}'.format(scraped_count, selenium_dict_value['source']))\n",
    "    return pd.DataFrame(papers).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T15:08:24.521491Z",
     "start_time": "2018-12-19T15:07:50.999251Z"
    }
   },
   "outputs": [],
   "source": [
    "# sum_loc must provide directions for finding a list of paragraphs (not just paragraph text).\n",
    "selenium_dict = {\n",
    "                    'Detroit News': {'url': 'https://www.detroitnews.com/autos/',\n",
    "                                     'source': 'Detroit News',\n",
    "                                     'url_list_query': \"['https://www.detroitnews.com' + a['href'] for a in soup.find('div', class_='headline-page active').find_all('a')]\",\n",
    "                                     'load_more_css': \"a.button-add-content\",\n",
    "                                     'max_scrapes': 30,\n",
    "                                     'max_loads': 2, # Make sure that loads is not too many -- otherwise load button may become inactive.  Test this.\n",
    "                                     'ordered_bool': False,\n",
    "                                     'date_css': None,\n",
    "                                     'date_loc': \"soup.find('span', class_='asset-metabar-time asset-metabar-item nobyline').text.split('ET ')[1].split(' |')[0]\",\n",
    "                                     'date_format': '%b. %d, %Y',\n",
    "                                     'sum_css': None,\n",
    "                                     'sum_loc': \"[soup.find('p', class_='speakable-p-1 p-text'), soup.find('p', class_='speakable-p-2 p-text')]\",\n",
    "                                     'title_loc': \"soup.find('h1', class_='asset-headline speakable-headline').text\",\n",
    "                                     'title_css': None},\n",
    "                                 \n",
    "                        'Transportation Research': {'url': ['https://www.journals.elsevier.com/transportation-research-part-a-policy-and-practice/recent-articles',\n",
    "                                                            'https://www.journals.elsevier.com/transportation-research-part-b-methodological/recent-articles',\n",
    "                                                            'https://www.journals.elsevier.com/transportation-research-part-c-emerging-technologies/recent-articles',\n",
    "                                                            'https://www.journals.elsevier.com/transportation-research-part-d-transport-and-environment/recent-articles',\n",
    "                                                            'https://www.journals.elsevier.com/transportation-research-part-e-logistics-and-transportation-review/recent-articles',\n",
    "                                                            'https://www.journals.elsevier.com/transportation-research-part-f-traffic-psychology-and-behaviour/recent-articles' \n",
    "                                                           ],\n",
    "                                     'source': 'Transportation Research',\n",
    "                                     'url_list_query': \"[div.a['href'] for div in soup.find_all('div', class_='pod-listing-header')]\",\n",
    "                                     'load_more_css': None,\n",
    "                                     'max_scrapes': 100,\n",
    "                                     'max_loads': None,\n",
    "                                     'ordered_bool': True,\n",
    "                                     'date_css': \"button.show-hide-details\",\n",
    "                                     'date_loc': \"soup.find('div', 'wrapper').p.text.split('online ')[1][:-1]\",\n",
    "                                     'date_format': '%d %B %Y',\n",
    "                                     'sum_css': None,\n",
    "                                     'sum_loc': \"soup.find('div', class_='Abstracts').find_all('p', limit = 3)\",\n",
    "                                     'title_loc': \"soup.h1.text\",\n",
    "                                     'title_css': None},\n",
    "    \n",
    "                            'Journal of Urban Economics': {'url': 'https://www.journals.elsevier.com/journal-of-urban-economics/recent-articles',\n",
    "                                     'source': 'Journal of Urban Economics',\n",
    "                                     'url_list_query': \"[div.a['href'] for div in soup.find_all('div', class_='pod-listing-header')]\",\n",
    "                                     'load_more_css': None,\n",
    "                                     'max_scrapes': 100,\n",
    "                                     'max_loads': None,\n",
    "                                     'ordered_bool': True,\n",
    "                                     'date_css': \"button.show-hide-details\",\n",
    "                                     'date_loc': \"soup.find('div', 'wrapper').p.text.split('online ')[1][:-1]\",\n",
    "                                     'date_format': '%d %B %Y',\n",
    "                                     'sum_css': None,\n",
    "                                     'sum_loc': \"soup.find('div', class_='Abstracts').find_all('p', limit = 3)\",\n",
    "                                     'title_loc': \"soup.h1.text\",\n",
    "                                     'title_css': None},\n",
    "    \n",
    "                            'Transport Policy': {'url': 'https://www.journals.elsevier.com/transport-policy/recent-articles',\n",
    "                                     'source': 'Transport Policy',\n",
    "                                     'url_list_query': \"[div.a['href'] for div in soup.find_all('div', class_='pod-listing-header')]\",\n",
    "                                     'load_more_css': None,\n",
    "                                     'max_scrapes': 100,\n",
    "                                     'max_loads': None,\n",
    "                                     'ordered_bool': True,\n",
    "                                     'date_css': \"button.show-hide-details\",\n",
    "                                     'date_loc': \"soup.find('div', 'wrapper').p.text.split('online ')[1][:-1]\",\n",
    "                                     'date_format': '%d %B %Y',\n",
    "                                     'sum_css': None,\n",
    "                                     'sum_loc': \"soup.find('div', class_='Abstracts').find_all('p', limit = 3)\",\n",
    "                                     'title_loc': \"soup.h1.text\",\n",
    "                                     'title_css': None}\n",
    "                                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.journals.elsevier.com/transportation-research-part-a-policy-and-practice/recent-articles\n",
      "https://www.journals.elsevier.com/transportation-research-part-b-methodological/recent-articles\n",
      "https://www.journals.elsevier.com/transportation-research-part-c-emerging-technologies/recent-articles\n",
      "https://www.journals.elsevier.com/transportation-research-part-d-transport-and-environment/recent-articles\n",
      "https://www.journals.elsevier.com/transportation-research-part-e-logistics-and-transportation-review/recent-articles\n",
      "https://www.journals.elsevier.com/transportation-research-part-f-traffic-psychology-and-behaviour/recent-articles\n",
      "24 new papers in Transportation Research\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome()\n",
    "df = css_scraypuh(selenium_dict['Transportation Research'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>source</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>A novel traffic management strategy in automa...</td>\n",
       "      <td>Driving aggressiveness management policy to en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-16</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>Areas of interventions for shared modes (Scho...</td>\n",
       "      <td>Identifying areas of interventions for improve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-15</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>This paper examines the transportation-growth ...</td>\n",
       "      <td>The Transportation-growth nexus in USA: Fresh ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-12</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>Activity-based models appeared as an answer to...</td>\n",
       "      <td>Mobile phone records to feed activity-based tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-01-12</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>With the recent advances in battery technology...</td>\n",
       "      <td>Optimizing the deployment of electric vehicle ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-01-16</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>Reliable shortest problem with link travel ti...</td>\n",
       "      <td>An algorithm for reliable shortest path proble...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-01-16</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>Reference models are dedicated to non-ordered...</td>\n",
       "      <td>A new family of qualitative choice models: An ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-01-15</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>A queueing theory-based model for matching ta...</td>\n",
       "      <td>A Markov decision process approach to vacant t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-01-14</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>We propose a link-based cyclic tradable credi...</td>\n",
       "      <td>Promoting social equity with cyclic tradable c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2019-01-12</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>This paper addresses one of the most challengi...</td>\n",
       "      <td>Fair cost allocation for ridesharing services ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>Mode and destination type choice were found t...</td>\n",
       "      <td>Modeling household-level hurricane evacuation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2019-01-16</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>Image processing algorithm developed for accu...</td>\n",
       "      <td>Tracking vehicle trajectories and fuel rates i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2019-01-16</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>A MIP model is built to improve the OD access...</td>\n",
       "      <td>Timetable synchronization of last trains for u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2019-01-15</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>CAVs are capable of learning the optimal cont...</td>\n",
       "      <td>Deep reinforcement learning enabled self-learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2019-01-15</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>A mixed integer linear model is developed to ...</td>\n",
       "      <td>Coordinated optimization of tram trajectories ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2019-01-15</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>Vehicle platoon is a set of vehicles driving ...</td>\n",
       "      <td>Behaviorally stable vehicle platooning for ene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>The relationship between commuting behaviour ...</td>\n",
       "      <td>Commuting behavior and congestion satisfaction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>Health impact assessment of long-range transp...</td>\n",
       "      <td>Modeling health equity in active transportatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>We monitored PM levels pre and post odd-even ...</td>\n",
       "      <td>The effect of odd-even driving scheme on PM2.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2019-01-16</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>Interest in foldable ocean containers to addr...</td>\n",
       "      <td>The impact of foldable ocean containers on bac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2019-01-16</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>This study investigates the purchase intentio...</td>\n",
       "      <td>Effect of environmental awareness on purchase ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2019-01-16</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>Driving is a combination of top-down and bott...</td>\n",
       "      <td>Quantifying the difference between intention a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>Distraction caused by a music player and phon...</td>\n",
       "      <td>Driver behaviour at the onset of yellow signal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>Drivers’ toll lane choices and their travel t...</td>\n",
       "      <td>The effect of information on drivers’ toll lan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date                                               link  \\\n",
       "1   2019-01-17  https://www.sciencedirect.com/science/article/...   \n",
       "2   2019-01-16  https://www.sciencedirect.com/science/article/...   \n",
       "3   2019-01-15  https://www.sciencedirect.com/science/article/...   \n",
       "4   2019-01-12  https://www.sciencedirect.com/science/article/...   \n",
       "5   2019-01-12  https://www.sciencedirect.com/science/article/...   \n",
       "6   2019-01-16  https://www.sciencedirect.com/science/article/...   \n",
       "7   2019-01-16  https://www.sciencedirect.com/science/article/...   \n",
       "8   2019-01-15  https://www.sciencedirect.com/science/article/...   \n",
       "9   2019-01-14  https://www.sciencedirect.com/science/article/...   \n",
       "10  2019-01-12  https://www.sciencedirect.com/science/article/...   \n",
       "11  2019-01-17  https://www.sciencedirect.com/science/article/...   \n",
       "12  2019-01-16  https://www.sciencedirect.com/science/article/...   \n",
       "13  2019-01-16  https://www.sciencedirect.com/science/article/...   \n",
       "14  2019-01-15  https://www.sciencedirect.com/science/article/...   \n",
       "15  2019-01-15  https://www.sciencedirect.com/science/article/...   \n",
       "16  2019-01-15  https://www.sciencedirect.com/science/article/...   \n",
       "17  2019-01-17  https://www.sciencedirect.com/science/article/...   \n",
       "18  2019-01-17  https://www.sciencedirect.com/science/article/...   \n",
       "19  2019-01-17  https://www.sciencedirect.com/science/article/...   \n",
       "20  2019-01-16  https://www.sciencedirect.com/science/article/...   \n",
       "21  2019-01-16  https://www.sciencedirect.com/science/article/...   \n",
       "22  2019-01-16  https://www.sciencedirect.com/science/article/...   \n",
       "23  2019-01-17  https://www.sciencedirect.com/science/article/...   \n",
       "24  2019-01-17  https://www.sciencedirect.com/science/article/...   \n",
       "\n",
       "                     source  \\\n",
       "1   Transportation Research   \n",
       "2   Transportation Research   \n",
       "3   Transportation Research   \n",
       "4   Transportation Research   \n",
       "5   Transportation Research   \n",
       "6   Transportation Research   \n",
       "7   Transportation Research   \n",
       "8   Transportation Research   \n",
       "9   Transportation Research   \n",
       "10  Transportation Research   \n",
       "11  Transportation Research   \n",
       "12  Transportation Research   \n",
       "13  Transportation Research   \n",
       "14  Transportation Research   \n",
       "15  Transportation Research   \n",
       "16  Transportation Research   \n",
       "17  Transportation Research   \n",
       "18  Transportation Research   \n",
       "19  Transportation Research   \n",
       "20  Transportation Research   \n",
       "21  Transportation Research   \n",
       "22  Transportation Research   \n",
       "23  Transportation Research   \n",
       "24  Transportation Research   \n",
       "\n",
       "                                              summary  \\\n",
       "1    A novel traffic management strategy in automa...   \n",
       "2    Areas of interventions for shared modes (Scho...   \n",
       "3   This paper examines the transportation-growth ...   \n",
       "4   Activity-based models appeared as an answer to...   \n",
       "5   With the recent advances in battery technology...   \n",
       "6    Reliable shortest problem with link travel ti...   \n",
       "7    Reference models are dedicated to non-ordered...   \n",
       "8    A queueing theory-based model for matching ta...   \n",
       "9    We propose a link-based cyclic tradable credi...   \n",
       "10  This paper addresses one of the most challengi...   \n",
       "11   Mode and destination type choice were found t...   \n",
       "12   Image processing algorithm developed for accu...   \n",
       "13   A MIP model is built to improve the OD access...   \n",
       "14   CAVs are capable of learning the optimal cont...   \n",
       "15   A mixed integer linear model is developed to ...   \n",
       "16   Vehicle platoon is a set of vehicles driving ...   \n",
       "17   The relationship between commuting behaviour ...   \n",
       "18   Health impact assessment of long-range transp...   \n",
       "19   We monitored PM levels pre and post odd-even ...   \n",
       "20   Interest in foldable ocean containers to addr...   \n",
       "21   This study investigates the purchase intentio...   \n",
       "22   Driving is a combination of top-down and bott...   \n",
       "23   Distraction caused by a music player and phon...   \n",
       "24   Drivers’ toll lane choices and their travel t...   \n",
       "\n",
       "                                                title  \n",
       "1   Driving aggressiveness management policy to en...  \n",
       "2   Identifying areas of interventions for improve...  \n",
       "3   The Transportation-growth nexus in USA: Fresh ...  \n",
       "4   Mobile phone records to feed activity-based tr...  \n",
       "5   Optimizing the deployment of electric vehicle ...  \n",
       "6   An algorithm for reliable shortest path proble...  \n",
       "7   A new family of qualitative choice models: An ...  \n",
       "8   A Markov decision process approach to vacant t...  \n",
       "9   Promoting social equity with cyclic tradable c...  \n",
       "10  Fair cost allocation for ridesharing services ...  \n",
       "11  Modeling household-level hurricane evacuation ...  \n",
       "12  Tracking vehicle trajectories and fuel rates i...  \n",
       "13  Timetable synchronization of last trains for u...  \n",
       "14  Deep reinforcement learning enabled self-learn...  \n",
       "15  Coordinated optimization of tram trajectories ...  \n",
       "16  Behaviorally stable vehicle platooning for ene...  \n",
       "17  Commuting behavior and congestion satisfaction...  \n",
       "18  Modeling health equity in active transportatio...  \n",
       "19  The effect of odd-even driving scheme on PM2.5...  \n",
       "20  The impact of foldable ocean containers on bac...  \n",
       "21  Effect of environmental awareness on purchase ...  \n",
       "22  Quantifying the difference between intention a...  \n",
       "23  Driver behaviour at the onset of yellow signal...  \n",
       "24  The effect of information on drivers’ toll lan...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.detroitnews.com/autos/\n",
      "5 new papers in Detroit News\n",
      "https://www.journals.elsevier.com/transportation-research-part-a-policy-and-practice/recent-articles\n",
      "https://www.journals.elsevier.com/transportation-research-part-b-methodological/recent-articles\n",
      "https://www.journals.elsevier.com/transportation-research-part-c-emerging-technologies/recent-articles\n",
      "https://www.journals.elsevier.com/transportation-research-part-d-transport-and-environment/recent-articles\n",
      "https://www.journals.elsevier.com/transportation-research-part-e-logistics-and-transportation-review/recent-articles\n",
      "https://www.journals.elsevier.com/transportation-research-part-f-traffic-psychology-and-behaviour/recent-articles\n",
      "24 new papers in Transportation Research\n",
      "https://www.journals.elsevier.com/journal-of-urban-economics/recent-articles\n",
      "0 new papers in Journal of Urban Economics\n",
      "https://www.journals.elsevier.com/transport-policy/recent-articles\n",
      "0 new papers in Transport Policy\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome()\n",
    "week_o_papers = []\n",
    "for site in selenium_dict:\n",
    "    week_o_papers.append(css_scraypuh(selenium_dict[site]))\n",
    "week_o_papers = pd.concat(week_o_papers)\n",
    "#week_o_papers.to_excel('{} papers.xls'.format(search_date))\n",
    "week_o_papers.dropna(how='all', axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>source</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-18</td>\n",
       "      <td>https://www.detroitnews.com/story/business/aut...</td>\n",
       "      <td>Detroit News</td>\n",
       "      <td>Tokyo – Mitsubishi Motors held a board meeting...</td>\n",
       "      <td>Mitsubishi board examines new charges against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-18</td>\n",
       "      <td>https://www.detroitnews.com/story/business/aut...</td>\n",
       "      <td>Detroit News</td>\n",
       "      <td>Tesla, recognizing as imperative its ability t...</td>\n",
       "      <td>Tesla to cut staff 7%, says road ahead very di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>https://www.detroitnews.com/story/business/aut...</td>\n",
       "      <td>Detroit News</td>\n",
       "      <td>Detroit — The ongoing standoff between Preside...</td>\n",
       "      <td>Bill Ford: Washington standoff is bad for busi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>https://www.detroitnews.com/story/business/aut...</td>\n",
       "      <td>Detroit News</td>\n",
       "      <td>Four Audi AG officials have been indicted for ...</td>\n",
       "      <td>Four Audi officials indicted for diesel cheating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>https://www.detroitnews.com/story/news/local/m...</td>\n",
       "      <td>Detroit News</td>\n",
       "      <td>Detroit — Lt. Gov. Garlin Gilchrist II on Thur...</td>\n",
       "      <td>Gilchrist makes Detroit auto show appearance t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>A novel traffic management strategy in automa...</td>\n",
       "      <td>Driving aggressiveness management policy to en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-16</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>Areas of interventions for shared modes (Scho...</td>\n",
       "      <td>Identifying areas of interventions for improve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-15</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>This paper examines the transportation-growth ...</td>\n",
       "      <td>The Transportation-growth nexus in USA: Fresh ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-12</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>Activity-based models appeared as an answer to...</td>\n",
       "      <td>Mobile phone records to feed activity-based tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-01-12</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>With the recent advances in battery technology...</td>\n",
       "      <td>Optimizing the deployment of electric vehicle ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-01-16</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>Reliable shortest problem with link travel ti...</td>\n",
       "      <td>An algorithm for reliable shortest path proble...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-01-16</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>Reference models are dedicated to non-ordered...</td>\n",
       "      <td>A new family of qualitative choice models: An ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-01-15</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>A queueing theory-based model for matching ta...</td>\n",
       "      <td>A Markov decision process approach to vacant t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-01-14</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>We propose a link-based cyclic tradable credi...</td>\n",
       "      <td>Promoting social equity with cyclic tradable c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2019-01-12</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>This paper addresses one of the most challengi...</td>\n",
       "      <td>Fair cost allocation for ridesharing services ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>Mode and destination type choice were found t...</td>\n",
       "      <td>Modeling household-level hurricane evacuation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2019-01-16</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>Image processing algorithm developed for accu...</td>\n",
       "      <td>Tracking vehicle trajectories and fuel rates i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2019-01-16</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>A MIP model is built to improve the OD access...</td>\n",
       "      <td>Timetable synchronization of last trains for u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2019-01-15</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>CAVs are capable of learning the optimal cont...</td>\n",
       "      <td>Deep reinforcement learning enabled self-learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2019-01-15</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>A mixed integer linear model is developed to ...</td>\n",
       "      <td>Coordinated optimization of tram trajectories ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2019-01-15</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>Vehicle platoon is a set of vehicles driving ...</td>\n",
       "      <td>Behaviorally stable vehicle platooning for ene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>The relationship between commuting behaviour ...</td>\n",
       "      <td>Commuting behavior and congestion satisfaction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>Health impact assessment of long-range transp...</td>\n",
       "      <td>Modeling health equity in active transportatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>We monitored PM levels pre and post odd-even ...</td>\n",
       "      <td>The effect of odd-even driving scheme on PM2.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2019-01-16</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>Interest in foldable ocean containers to addr...</td>\n",
       "      <td>The impact of foldable ocean containers on bac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2019-01-16</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>This study investigates the purchase intentio...</td>\n",
       "      <td>Effect of environmental awareness on purchase ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2019-01-16</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>Driving is a combination of top-down and bott...</td>\n",
       "      <td>Quantifying the difference between intention a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>Distraction caused by a music player and phon...</td>\n",
       "      <td>Driver behaviour at the onset of yellow signal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Transportation Research</td>\n",
       "      <td>Drivers’ toll lane choices and their travel t...</td>\n",
       "      <td>The effect of information on drivers’ toll lan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date                                               link  \\\n",
       "1   2019-01-18  https://www.detroitnews.com/story/business/aut...   \n",
       "2   2019-01-18  https://www.detroitnews.com/story/business/aut...   \n",
       "3   2019-01-17  https://www.detroitnews.com/story/business/aut...   \n",
       "4   2019-01-17  https://www.detroitnews.com/story/business/aut...   \n",
       "5   2019-01-17  https://www.detroitnews.com/story/news/local/m...   \n",
       "1   2019-01-17  https://www.sciencedirect.com/science/article/...   \n",
       "2   2019-01-16  https://www.sciencedirect.com/science/article/...   \n",
       "3   2019-01-15  https://www.sciencedirect.com/science/article/...   \n",
       "4   2019-01-12  https://www.sciencedirect.com/science/article/...   \n",
       "5   2019-01-12  https://www.sciencedirect.com/science/article/...   \n",
       "6   2019-01-16  https://www.sciencedirect.com/science/article/...   \n",
       "7   2019-01-16  https://www.sciencedirect.com/science/article/...   \n",
       "8   2019-01-15  https://www.sciencedirect.com/science/article/...   \n",
       "9   2019-01-14  https://www.sciencedirect.com/science/article/...   \n",
       "10  2019-01-12  https://www.sciencedirect.com/science/article/...   \n",
       "11  2019-01-17  https://www.sciencedirect.com/science/article/...   \n",
       "12  2019-01-16  https://www.sciencedirect.com/science/article/...   \n",
       "13  2019-01-16  https://www.sciencedirect.com/science/article/...   \n",
       "14  2019-01-15  https://www.sciencedirect.com/science/article/...   \n",
       "15  2019-01-15  https://www.sciencedirect.com/science/article/...   \n",
       "16  2019-01-15  https://www.sciencedirect.com/science/article/...   \n",
       "17  2019-01-17  https://www.sciencedirect.com/science/article/...   \n",
       "18  2019-01-17  https://www.sciencedirect.com/science/article/...   \n",
       "19  2019-01-17  https://www.sciencedirect.com/science/article/...   \n",
       "20  2019-01-16  https://www.sciencedirect.com/science/article/...   \n",
       "21  2019-01-16  https://www.sciencedirect.com/science/article/...   \n",
       "22  2019-01-16  https://www.sciencedirect.com/science/article/...   \n",
       "23  2019-01-17  https://www.sciencedirect.com/science/article/...   \n",
       "24  2019-01-17  https://www.sciencedirect.com/science/article/...   \n",
       "\n",
       "                     source  \\\n",
       "1              Detroit News   \n",
       "2              Detroit News   \n",
       "3              Detroit News   \n",
       "4              Detroit News   \n",
       "5              Detroit News   \n",
       "1   Transportation Research   \n",
       "2   Transportation Research   \n",
       "3   Transportation Research   \n",
       "4   Transportation Research   \n",
       "5   Transportation Research   \n",
       "6   Transportation Research   \n",
       "7   Transportation Research   \n",
       "8   Transportation Research   \n",
       "9   Transportation Research   \n",
       "10  Transportation Research   \n",
       "11  Transportation Research   \n",
       "12  Transportation Research   \n",
       "13  Transportation Research   \n",
       "14  Transportation Research   \n",
       "15  Transportation Research   \n",
       "16  Transportation Research   \n",
       "17  Transportation Research   \n",
       "18  Transportation Research   \n",
       "19  Transportation Research   \n",
       "20  Transportation Research   \n",
       "21  Transportation Research   \n",
       "22  Transportation Research   \n",
       "23  Transportation Research   \n",
       "24  Transportation Research   \n",
       "\n",
       "                                              summary  \\\n",
       "1   Tokyo – Mitsubishi Motors held a board meeting...   \n",
       "2   Tesla, recognizing as imperative its ability t...   \n",
       "3   Detroit — The ongoing standoff between Preside...   \n",
       "4   Four Audi AG officials have been indicted for ...   \n",
       "5   Detroit — Lt. Gov. Garlin Gilchrist II on Thur...   \n",
       "1    A novel traffic management strategy in automa...   \n",
       "2    Areas of interventions for shared modes (Scho...   \n",
       "3   This paper examines the transportation-growth ...   \n",
       "4   Activity-based models appeared as an answer to...   \n",
       "5   With the recent advances in battery technology...   \n",
       "6    Reliable shortest problem with link travel ti...   \n",
       "7    Reference models are dedicated to non-ordered...   \n",
       "8    A queueing theory-based model for matching ta...   \n",
       "9    We propose a link-based cyclic tradable credi...   \n",
       "10  This paper addresses one of the most challengi...   \n",
       "11   Mode and destination type choice were found t...   \n",
       "12   Image processing algorithm developed for accu...   \n",
       "13   A MIP model is built to improve the OD access...   \n",
       "14   CAVs are capable of learning the optimal cont...   \n",
       "15   A mixed integer linear model is developed to ...   \n",
       "16   Vehicle platoon is a set of vehicles driving ...   \n",
       "17   The relationship between commuting behaviour ...   \n",
       "18   Health impact assessment of long-range transp...   \n",
       "19   We monitored PM levels pre and post odd-even ...   \n",
       "20   Interest in foldable ocean containers to addr...   \n",
       "21   This study investigates the purchase intentio...   \n",
       "22   Driving is a combination of top-down and bott...   \n",
       "23   Distraction caused by a music player and phon...   \n",
       "24   Drivers’ toll lane choices and their travel t...   \n",
       "\n",
       "                                                title  \n",
       "1   Mitsubishi board examines new charges against ...  \n",
       "2   Tesla to cut staff 7%, says road ahead very di...  \n",
       "3   Bill Ford: Washington standoff is bad for busi...  \n",
       "4    Four Audi officials indicted for diesel cheating  \n",
       "5   Gilchrist makes Detroit auto show appearance t...  \n",
       "1   Driving aggressiveness management policy to en...  \n",
       "2   Identifying areas of interventions for improve...  \n",
       "3   The Transportation-growth nexus in USA: Fresh ...  \n",
       "4   Mobile phone records to feed activity-based tr...  \n",
       "5   Optimizing the deployment of electric vehicle ...  \n",
       "6   An algorithm for reliable shortest path proble...  \n",
       "7   A new family of qualitative choice models: An ...  \n",
       "8   A Markov decision process approach to vacant t...  \n",
       "9   Promoting social equity with cyclic tradable c...  \n",
       "10  Fair cost allocation for ridesharing services ...  \n",
       "11  Modeling household-level hurricane evacuation ...  \n",
       "12  Tracking vehicle trajectories and fuel rates i...  \n",
       "13  Timetable synchronization of last trains for u...  \n",
       "14  Deep reinforcement learning enabled self-learn...  \n",
       "15  Coordinated optimization of tram trajectories ...  \n",
       "16  Behaviorally stable vehicle platooning for ene...  \n",
       "17  Commuting behavior and congestion satisfaction...  \n",
       "18  Modeling health equity in active transportatio...  \n",
       "19  The effect of odd-even driving scheme on PM2.5...  \n",
       "20  The impact of foldable ocean containers on bac...  \n",
       "21  Effect of environmental awareness on purchase ...  \n",
       "22  Quantifying the difference between intention a...  \n",
       "23  Driver behaviour at the onset of yellow signal...  \n",
       "24  The effect of information on drivers’ toll lan...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "week_o_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T15:08:25.507884Z",
     "start_time": "2018-12-19T15:08:25.392357Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<docx.text.paragraph.Paragraph at 0xe3c7b38>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20aba8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Element {http://schemas.openxmlformats.org/wordprocessingml/2006/main}hyperlink at 0xdbf8608>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20ad68>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.paragraph.Paragraph at 0xb1a4208>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20abe0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Element {http://schemas.openxmlformats.org/wordprocessingml/2006/main}hyperlink at 0xdbf8cc8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20ad68>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.paragraph.Paragraph at 0xe3c7b38>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20a7f0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Element {http://schemas.openxmlformats.org/wordprocessingml/2006/main}hyperlink at 0xc620108>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xc48a2e8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.paragraph.Paragraph at 0xb1a4208>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20a358>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Element {http://schemas.openxmlformats.org/wordprocessingml/2006/main}hyperlink at 0xc620ec8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xc48a2e8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.paragraph.Paragraph at 0xe3c7b38>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20a710>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Element {http://schemas.openxmlformats.org/wordprocessingml/2006/main}hyperlink at 0xc620088>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20a908>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.paragraph.Paragraph at 0xb1a4208>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20a828>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Element {http://schemas.openxmlformats.org/wordprocessingml/2006/main}hyperlink at 0xc620348>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20a908>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.paragraph.Paragraph at 0xc48a2e8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20a208>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Element {http://schemas.openxmlformats.org/wordprocessingml/2006/main}hyperlink at 0xc6208c8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20a828>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.paragraph.Paragraph at 0xb1a4208>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20a208>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Element {http://schemas.openxmlformats.org/wordprocessingml/2006/main}hyperlink at 0xc6200c8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20afd0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.paragraph.Paragraph at 0xc48a2e8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20ac50>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Element {http://schemas.openxmlformats.org/wordprocessingml/2006/main}hyperlink at 0xc620488>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20afd0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.paragraph.Paragraph at 0xb1a4208>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20ac50>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Element {http://schemas.openxmlformats.org/wordprocessingml/2006/main}hyperlink at 0xc620488>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20afd0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.paragraph.Paragraph at 0xc48a2e8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20a2e8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Element {http://schemas.openxmlformats.org/wordprocessingml/2006/main}hyperlink at 0xc6209c8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20ae80>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.paragraph.Paragraph at 0xb1a4208>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20a908>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Element {http://schemas.openxmlformats.org/wordprocessingml/2006/main}hyperlink at 0xc620688>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20ae80>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.paragraph.Paragraph at 0xc48a2e8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20a828>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Element {http://schemas.openxmlformats.org/wordprocessingml/2006/main}hyperlink at 0xc620d08>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20ae80>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.paragraph.Paragraph at 0xb1a4208>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20a390>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Element {http://schemas.openxmlformats.org/wordprocessingml/2006/main}hyperlink at 0xc620288>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20ae80>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.paragraph.Paragraph at 0xc48a2e8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20a908>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Element {http://schemas.openxmlformats.org/wordprocessingml/2006/main}hyperlink at 0xc620288>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20ae80>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.paragraph.Paragraph at 0xb1a4208>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20af98>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Element {http://schemas.openxmlformats.org/wordprocessingml/2006/main}hyperlink at 0xc620288>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20ae80>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.paragraph.Paragraph at 0xc48a2e8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20a940>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Element {http://schemas.openxmlformats.org/wordprocessingml/2006/main}hyperlink at 0xc620288>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20ae80>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.paragraph.Paragraph at 0xb1a4208>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20a828>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Element {http://schemas.openxmlformats.org/wordprocessingml/2006/main}hyperlink at 0xc620288>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20ae80>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.paragraph.Paragraph at 0xc48a2e8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20a390>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Element {http://schemas.openxmlformats.org/wordprocessingml/2006/main}hyperlink at 0xb0b2048>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20ae80>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.paragraph.Paragraph at 0xb1a4208>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20a908>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Element {http://schemas.openxmlformats.org/wordprocessingml/2006/main}hyperlink at 0xb0b2048>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20ae80>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.paragraph.Paragraph at 0xc48a2e8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20af98>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Element {http://schemas.openxmlformats.org/wordprocessingml/2006/main}hyperlink at 0xb0b28c8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20ae80>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.paragraph.Paragraph at 0xb1a4208>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20a940>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Element {http://schemas.openxmlformats.org/wordprocessingml/2006/main}hyperlink at 0xc620d08>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<docx.text.run.Run at 0xb20ae80>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsdoc = docx.Document(docx='python_docx.docx')\n",
    "\n",
    "for row in week_o_papers.reset_index(drop=True).T:\n",
    "    row = week_o_papers.iloc[row, :]\n",
    "    newsdoc.add_heading(row['title'], level=2)\n",
    "    p = newsdoc.add_paragraph(row['summary'] + ' ')\n",
    "    p.add_run('(')\n",
    "    add_hyperlink(p, '{}'.format(row['link']), '{}'.format(row['source']))\n",
    "    p.add_run(')')\n",
    "newsdoc.save('{} papers.docx'.format(search_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T15:08:25.392357Z",
     "start_time": "2018-12-19T15:08:24.874540Z"
    }
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('news_papers.db')\n",
    "week_o_papers.to_sql('news_papers', conn, if_exists='append', index=False)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For scraper development (no need to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T19:53:09.102598Z",
     "start_time": "2018-09-10T19:53:09.098609Z"
    }
   },
   "outputs": [],
   "source": [
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T13:47:47.978384Z",
     "start_time": "2018-09-12T13:47:47.546331Z"
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://venturebeat.com/2018/09/19/renault-unveils-autonomous-delivery-concept-ez-pro-with-customizable-robo-pods/'\n",
    "page = requests.get(url, headers = headers)\n",
    "soup = BeautifulSoup(page.content, 'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[item for item in soup.find('div', class_ = ['the-content','article-content']).find_all('p')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T13:50:25.090095Z",
     "start_time": "2018-09-12T13:50:25.083116Z"
    }
   },
   "outputs": [],
   "source": [
    "[p for p in soup.find_all('p') if p.text and len(p.text)>10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'strain_tag':'a', \n",
    "'strain_attr_name':'class', \n",
    "'strain_attr_value':'post-block__title__link',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T14:29:28.993469Z",
     "start_time": "2018-09-12T14:29:28.988481Z"
    }
   },
   "outputs": [],
   "source": [
    "test_dict = {'url':'https://venturebeat.com/category/transportation/', \n",
    "                                 'source': 'Venture Beat', \n",
    "                                 'url_list_query':\"[item.a['href'] for item in self.base_soup.select('h2.article-title')]+[item.a['href'] for item in self.base_soup.select('article')]\",\n",
    "                                 'date_loc': \"article.find('meta', attrs={'property':'article:published_time'})['content']\", \n",
    "                                 'date_format':None,\n",
    "                                 'sum_loc': \"[p for p in article.find('div', class_ = ['the-content','article-content']).find_all('p')]\",\n",
    "                                 'title_loc':\"article.find('h1').text\", \n",
    "                                 'strain_bool':False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T14:29:30.856779Z",
     "start_time": "2018-09-12T14:29:29.190320Z"
    }
   },
   "outputs": [],
   "source": [
    "scraper = scraypah(test_dict)\n",
    "scraper.get_urls()\n",
    "scraper.urls_to_scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T14:02:34.361309Z",
     "start_time": "2018-09-12T14:02:34.354316Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_a_scraypah(attr_dict):\n",
    "    start = time.time()\n",
    "    scraper = scraypah(attr_dict)\n",
    "    print(time.time()-start)\n",
    "    scraper.get_urls()\n",
    "    print(time.time()-start)\n",
    "    scraper.scrape_em()\n",
    "    print(time.time()-start)\n",
    "    return scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T13:57:54.173153Z",
     "start_time": "2018-09-12T13:57:54.167161Z"
    }
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_colwidth', 100):\n",
    "    print(ttopics.relevant_df['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttopics.relevant_df['summary'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T14:04:25.884353Z",
     "start_time": "2018-09-12T14:02:40.006755Z"
    }
   },
   "outputs": [],
   "source": [
    "ttopics = test_a_scraypah(test_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unused scrapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Engadget': {'url': ['https://www.engadget.com/tags/transportation/', 'https://www.engadget.com/tag/transportation/page/2/'],\n",
    "                             'source': 'Engadget',\n",
    "                             'strain_tag': 'a',\n",
    "                             'strain_attr_name': 'class',\n",
    "                             'strain_attr_value': 'o-hit__link',\n",
    "                             'url_list_query': \"['https://www.engadget.com'+item['href'] for item in self.base_soup.find_all('a', attrs={'class':'o-hit__link'})]\",\n",
    "                             'date_loc': \"article.find('meta', attrs={'name':'published_at'})['content']\",\n",
    "                             'date_format': None,\n",
    "                             'sum_loc': \"article.find('div', attrs={'class':'container@m-'}).find_all('p')\",\n",
    "                             'title_loc': \"article.title.text\",\n",
    "                             'strain_bool': True},\n",
    "'NGV Global': {'url': 'http://www.ngvglobal.com/',\n",
    "                               'source': 'NGV Global',\n",
    "                               'strain_tag': 'h2',\n",
    "                               'strain_attr_name': 'class',\n",
    "                               'strain_attr_value': 'entry-title',\n",
    "                               'url_list_query': \"[item.a['href'] for item in self.base_soup.find_all('h2', attrs={'class':'entry-title'})]\",\n",
    "                               'date_loc': \"article.find('time')['title']\",\n",
    "                               'date_format': None,\n",
    "                               'sum_loc': \"article.find('div', attrs={'class':'pf-content'}).find_all('p')\",\n",
    "                               'title_loc': \"article.find('h1', attrs={'class':'entry-title'}).text\",\n",
    "                               'strain_bool': True},"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change Log\n",
    "* 8/29/2018: Added Citylab, Electrek, cleaned code\n",
    "* 8/7/2018: Added Transport Reviews to academic paper scraper\n",
    "* 7/30/2018: Fixed GovTech scraper\n",
    "* 6/29/2018: Changed the whole scraper over to utilize a new class called *scraypah*. \n",
    "* 5/12/2018: Added Semiconductor Engineering scraper and academic articles scraper (~3 hours)\n",
    "* 4/13/2018: Integrated word document production through python\n",
    "* 3/19/2018: Added OEM/Gov section that quickly checks 17 sites for updates - only prints a notification that it needs to be checked if there are new updates from the past week\n",
    "* 2/27/2018: Wrote a function *page_scan* to more efficiently create the relevant web page dictionary \"profiles\"\n",
    "* 2/27/2018: Added 21CTP trucking news keywords to search for. Integrated functionality into existing web scraper.\n",
    "* 2/14/2018: Added NGV Global scraper for AFV stuff\n",
    "* 2/14/2018: Added fuel cells, hybrid, hybrid-electric, 'electric buses', 'electric truck', 'electric trucks', 'electric drive' to the search terms for AFVs...\n",
    "* 1/31/2018: Added *print_results* function to streamline printed results for each scraper. Added counter to track #articles that were too old. Added meta-data tracking capability (dumps into SQL database every week)\n",
    "* 1/31/2018: Split EV market analysis and web scraper into two different Notebooks\n",
    "* 1/26/2018: Added Lexology scraper\n",
    "* 1/19/2018: Fixed GreenCarCongress scraper (site redesign)\n",
    "* 1/4/2018: Added Engadget scraper\n",
    "* 1/4/2018: Added \"replace_em\" function to streamline removal of meaningless substrings from body text summaries\n",
    "* 12/29/2017: Added Reuters, MITNews, and ARSTechnica scrapers. Did some streamlining in the EV Sales analysis\n",
    "* 12/20/2017: Wrote up quick-guide to all the post-Python processing needed for the final News Update doc.\n",
    "* 12/20/2017: Changed to .xls format. Had to import a different package to do so, but makes mail merge work better\n",
    "* 12/13/2017: Fixed Trucks.com scraper - was pulling out the wrong date for each article (pulled a date from the sidebar...)\n",
    "* 12/8/2017: Edited Trucks.com search so that it doesn't pick up paragraph tags that are actually image captions (added condition that \"class = None\")\n",
    "* 12/8/2017: Added a bunch of comments, specifically in the first code segment (\"IEEE Spectrum\") for explanatory purposes\n",
    "* 1/8/2019: Added Green Car Reports, DOE, Business Wire, and The Fuse.\n",
    "* 1/10/2019: Uploaded ipynb to Energetics' GitHub (EICode).  Log entries can now be found via GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "498px",
    "left": "1480.3px",
    "right": "20px",
    "top": "119.976px",
    "width": "658px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
