{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated News Update, or, \"Dwyer's attempt at automating himself out of a job\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Do\n",
    "* Fix Detroit News\n",
    "* Functionality to add news article to SQL database after the fact\n",
    "* Add Phys.org\n",
    "* Add infinite scrolling functionality to css_scraypuh\n",
    "\n",
    "### Can't because of paywalls:\n",
    "* WSJ\n",
    "* Nikkei\n",
    "* Automotive World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T11:29:08.311107Z",
     "start_time": "2018-08-29T11:29:08.300133Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages, define important stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T13:30:37.376863Z",
     "start_time": "2019-01-07T13:30:27.352145Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import requests\n",
    "\n",
    "import docx\n",
    "from docx.enum.text import WD_COLOR_INDEX\n",
    "from docx.shared import Pt\n",
    "from docx.shared import Inches\n",
    "\n",
    "import random\n",
    "\n",
    "#!pip install nltk\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables and dictionary of scrapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T13:30:37.453156Z",
     "start_time": "2019-01-07T13:30:37.390327Z"
    }
   },
   "outputs": [],
   "source": [
    "# Search date and search date string for file naming and tracking\n",
    "search_date = dt.date.today()\n",
    "search_date_str = str(search_date)\n",
    "\n",
    "# Essential keys to include in scraper_info subdictionaries:\n",
    "# 'day': Integer that represents the day of the week that corresponds with a particular scraper. e.g., 0==Monday.\n",
    "# 'keywords': Keyword lists for each of the different news updates\n",
    "# 'bad_words': If any of these words are in the article title, drop article. Words are not case sensitive.\n",
    "# 'ideal_no_articles': Ideal number of articles to be scraped for each scraper.  If excess articles are scraped, the articles will be sampled (articles from best sources are sampled first).\n",
    "#                      Journal articles do not count towards this total.\n",
    "# 'max age': Age filter, in days (If max_age==7, it means we only want to pull articles that are <= 1 week old)\n",
    "# 'news_download_filename': subpath (starting in directory that contains News_Scraping.ipynb) to which news_downloads should be downloaded plus filename for news_download\n",
    "# 'make_deletions': boolean that specifies whether deletions (i.e. of duplicate articles, irrelevant articles, etc.) should be made.  True if deletions desired, False if not.\n",
    "# 'gen_docx': boolean that specifies whether docx should be created from news_download file.  True if docx desired, False if not.\n",
    "# 'restrict_sites: feature coming soon.\n",
    "\n",
    "# Optional keys to include in scraper_info subdictionaries:\n",
    "# The following keys are dependent on 'make_deletions'.  They MUST be included if 'make_deletions' is True.\n",
    "#    'deletions_filename': subpath (starting in directory that contains News_Scraping.ipynb) to which news_deletions should be downloaded plus filename for news_deletions\n",
    "# The following keys are dependent on 'gendocx'.  They MUST be included if 'gendocx' is True.\n",
    "#    'docx_title': Title of docx file (string)\n",
    "#    'docx_intro': Intro paragraph for docx file (string or None)\n",
    "#    'docx_headers': Dictionary of codes that indicate news_download categories and their corresponding docx headers\n",
    "#    'auto_id_research': boolean that specifies whether or not research category should be auto-id'd in the news_download file.  If True, articles from journals will be \n",
    "#                        automatically categorized as '4.' If False, articles from journals will not be categorized.  Mark as False if you don't want a separate research\n",
    "#                        section in the docx.\n",
    "# 'research_intro': String. May be included in the scraper_info dict or not.  Include if a paragraph of introduction to the research section of the docx file is desired.  Otherwise leave out.\n",
    "\n",
    "\n",
    "scraper_info = { '21CTP':{'day': 4,\n",
    "                          'keywords': ['alternative fuel', 'natural gas', 'compressed natural gas', 'liquefied natural gas', 'CNG', 'LNG', 'propane', 'LPG', 'dimethyl ether', \n",
    "                                       'DME', 'electric', 'electricity', 'electrified', 'electric drive', 'battery', 'energy storage', 'hydrogen', 'fuel cell', 'hybrid', \n",
    "                                       'hybrid electric', 'hybrid hydraulic', ' Phase 2', 'Phase II', 'efficiency', 'fuel efficiency', 'fuel economy', 'aftertreatment',\n",
    "                                       'emission control', 'diesel particulate filter', 'DPF', 'selective catalytic reduction', 'SCR', 'aerodynamics', 'sustainability', \n",
    "                                       'waste heat recovery', 'Rankine', 'organic Rankine', 'SuperTruck','automated manual', 'AMT', 'platooning', 'lithium', 'biofuel', 'fast charging', \n",
    "                                       'downspeed', 'downsize', 'clean diesel', 'turbocompound', 'rolling resistance', 'skirt', 'boat tail', 'axle', 'low viscosity',\n",
    "                                       'catenary', 'autonomy', 'autonomous', 'connected and autonomous', 'connected', 'telematics', 'driver assist', 'CACC', 'active cruise control', \n",
    "                                       'crash avoidance', 'crashworthiness', 'weigh-in-motion', 'weigh in motion','high productivity', 'truck size and weight', 'V2I', 'V2V', \n",
    "                                       'vehicle to infrastructure', ' vehicle to vehicle',  'restructuring', 'acquisition', 'driver cost', 'operational efficiency',\n",
    "                                       'facilities', 'proving ground', 'partnership', 'regional haul', 'joint venture', 'grant', 'FOA', 'funding opportunity', 'unveil', \n",
    "                                       'announce', 'offer', 'expansion', 'greenhouse gas', 'GHG', 'emission regulation',\n",
    "                                       'emissions regulation', 'idle', 'idling', 'zero emissions', 'strategic plan', 'SmartWay', 'VIUS', 'well to wheels', 'pump to wheels', \n",
    "                                       'well to pump', 'CARB', 'CEC', 'air resources board', 'energy commission', 'EPA','Environmental Protection Agency', 'smart mobility', \n",
    "                                       'smart cities'],\n",
    "                          'bad_words':[],\n",
    "                          'ideal_no_articles': 35,\n",
    "                          'max_age':7,\n",
    "                          'news_download_filename': f'21CTP_news_updates/{search_date_str}_21CTP_news_download.xls',\n",
    "                          'deletions_filename': f'21CTP_news_updates/{search_date_str}_21CTP_news_deletions.xls',\n",
    "                          'make_deletions': True,\n",
    "                          'gen_docx': True,\n",
    "                          'docx_title': \"21CTP Weekly News Update\",\n",
    "                          'docx_intro':None,\n",
    "                          'docx_headers':{1: 'Business and Market Analysis',\n",
    "                                          2: 'Technology, Testing, and Analysis', \n",
    "                                          3: 'Policy and Government', \n",
    "                                          4: 'Relevant Transportation Research'},\n",
    "                          'auto_id_research':True,\n",
    "                          'research_intro':'This section includes publications, papers, articles, and conferences that investigate and/or '\n",
    "                                           'are related to trucking. Portions of the abstract '\n",
    "                                           \"or description (not Energetics' words) are included under each title for more information.\",\n",
    "                          'restrict_sites':False,\n",
    "                          'news_df': None\n",
    "                       },\n",
    "                 'CAV':{'day': 0,\n",
    "                        'keywords': ['self-driving', 'automated', 'self driving', 'autonomous', 'MaaS', 'ride-sharing', 'ridesharing',\n",
    "                                     'ridehailing', 'lidar', 'LiDAR', 'rideshare', 'ridehail', 'ride-hail', 'ridesource', 'ride-source', 'ride-sourcing',\n",
    "                                     'carsharing', 'car-sharing', 'carshare', 'car-share', 'Uber', 'Lyft', 'Chariot', 'connected car', 'Waymo', 'TRI',\n",
    "                                     'Cruise', 'Zoox', 'Mobileye', 'Softbank', 'peer-to-peer', 'Turo'],\n",
    "                        'bad_words':[],\n",
    "                        'ideal_no_articles': 45,\n",
    "                        'max_age':7,\n",
    "                        'deletions_filename' : f'cav_news_updates/{search_date_str}_CAV_news_deletions.xls',\n",
    "                        'news_download_filename': f'cav_news_updates/{search_date_str}_cav_news_download.xls',\n",
    "                        'make_deletions': True,\n",
    "                        'gen_docx': True,\n",
    "                        'docx_title':\"Smart Mobility Weekly News Update\",\n",
    "                        'docx_intro':'Includes coverage of ride-sharing and other smart mobility technologies. '\n",
    "                                     'The majority of this is direct quotations from the respective articles. Energetics '\n",
    "                                     'claims none of this text content as its own, having only sifted through the '\n",
    "                                     'web to find already-existing pieces relevant to these topics.',\n",
    "                        'docx_headers':{1: 'Business and Market Analysis',\n",
    "                                        2: 'Technology, Testing, and Analysis', \n",
    "                                        3: 'Policy and Government', \n",
    "                                        4: 'Relevant Transportation Research'},\n",
    "                        'auto_id_research':True,\n",
    "                        'research_intro':'This section includes publications, papers, articles, and conferences that investigate and/or '\n",
    "                                         'discuss transportation and travel demand impacts of MaaS or other “future travel” considerations. '\n",
    "                                         \"Portions of the abstract or description (not Energetics' words) are included under each title for more\"\n",
    "                                         'information.',\n",
    "                        'restrict_sites':False,\n",
    "                          'news_df': None\n",
    "                       },\n",
    "                 'AFV':{'day':2,\n",
    "                        'keywords': ['rare-earth', 'rare earth', 'natural gas', 'electric vehicle', 'electric car', 'EV', 'electrification', 'alternative fuel', 'CNG', 'LNG',\n",
    "                                       'alt-fuel', 'propane', 'charging station', 'EVSE', 'electric vehicle charging', 'HEV', 'hybrid', 'hybrid-electric', 'plug-in', 'PHEV', \n",
    "                                       'electric motor', 'bio-fuel', 'biofuel', 'idle reduction', 'fuel cell', 'electric bus', 'electric truck', 'electric drive',\n",
    "                                       'battery-electric', 'battery electric', 'battery-electric-powered', 'regenerative braking'],\n",
    "                        'bad_words': [\"Today's Car News\", 'image', 'picture', 'podcast', 'video', 'photo', 'tweet', 'scooter', \n",
    "                                      'motorcycle', 'shipping', 'This Week in Reverse'],\n",
    "                        'ideal_no_articles':50,\n",
    "                        'max_age':7,\n",
    "                        'news_download_filename' : f'afv_news_updates/{search_date_str}_afv_news_download.xls',\n",
    "                        'deletions_filename' : f'afv_news_updates/{search_date_str}_afv_news_deletions.xls',\n",
    "                        'make_deletions': True,\n",
    "                        'gen_docx': True,\n",
    "                        'docx_title':\"Alternative Fuel Vehicle Weekly News Update\",\n",
    "                        'docx_intro':None,\n",
    "                        'docx_headers':{1: 'Business and Market Analysis',\n",
    "                                        2: 'Technology, Testing, and Analysis', \n",
    "                                        3: 'Policy and Government', \n",
    "                                        4: 'Relevant Transportation Research'},\n",
    "                        'auto_id_research':True,\n",
    "                        'research_intro':'This section includes publications, papers, articles, and conferences that investigate and/or '\n",
    "                                         'discuss alternative fuel vehicle impacts on transportation systems. Portions of the abstract '\n",
    "                                         \"or description (not Energetics' words) are included under each title for more information.\",\n",
    "                        'restrict_sites':False,\n",
    "                          'news_df': None\n",
    "                         },\n",
    "                 'Hyperloop':{'day':0,\n",
    "                              'keywords': ['hyperloop', 'high-speed train', 'high speed train', 'bullet train', 'ET3', 'Non-Traditional and Emerging Transportation Technology Council'],\n",
    "                              'bad_words':[],\n",
    "                              'ideal_no_articles': 35,\n",
    "                              'max_age':7,\n",
    "                              'news_download_filename': f'hyperloop_news_updates/{search_date_str}_hyperloop_news_download.xls',\n",
    "                              'deletions_filename':f'hyperloop_news_updates/{search_date_str}_hyperloop_news_deletions.xls',\n",
    "                              'make_deletions': False,\n",
    "                              'gen_docx': False,\n",
    "                              'auto_id_research':True,\n",
    "                              'restrict_sites':False,\n",
    "                          'news_df': None\n",
    "                             },\n",
    "                 'eVTOL':{'day':2,\n",
    "                          'keywords': ['UAV', 'VTOL', 'Pipistrel', 'Airbus', 'Uber Air', 'UberAir', 'Uber Elevate', 'Bell Helicopter', 'Kitty Hawk', 'Kittyhawk', 'Lilium', 'Karem', \n",
    "                                       'Volocopter', 'Aurora Flight Sciences', 'Embraer', 'vertical take-off', 'flying taxi','air taxi', 'air car', 'Sikorsky', 'PAV', 'flying car', \n",
    "                                       'unmanned aerial vehicle', 'passenger air vehicle', 'UAM', 'urban air mobility', 'DreamMaker', 'Butterfly', 'Vahana'],\n",
    "                          'bad_words':[],\n",
    "                          'ideal_no_articles': 35,\n",
    "                          'max_age':7,\n",
    "                          'news_download_filename':f'eVTOL_news_updates/{search_date_str}_eVTOL_news_download.xls',\n",
    "                          'deletions_filename':f'eVTOL_news_updates/{search_date_str}_eVTOL_news_deletions.xls',\n",
    "                          'make_deletions': False,\n",
    "                          'gen_docx': False,\n",
    "                          'auto_id_research':True,\n",
    "                          'restrict_sites':False,\n",
    "                          'news_df': None\n",
    "                         },\n",
    "                'INL':{'day':4,\n",
    "                       'keywords':['carbon fiber', 'carbon-fiber', 'polymer', 'composite', 'material', #removed powertrain and next-gen for the time being\n",
    "                                   'lightweight', 'lightweight material', 'multi-material', 'multi material', \n",
    "                                   'multimaterial', 'nanotech', 'nanomaterial', 'next generation', 'glazing', 'suspension', 'spider silk', \n",
    "                                   'carbon reinforced polymer', 'corrosion', 'corrosion resistance', 'corrosion-resistant',\n",
    "                                   'corrosion resistant', 'carbon-reinforced polymer', 'carbon-reinforced', 'carbon reinforced'],\n",
    "                       'bad_words':['battery material', 'cathode design', 'anode design', 'separator design', 'battery cell', 'battery module'],\n",
    "                       'ideal_no_articles': 1000,\n",
    "                       'max_age':31,\n",
    "                       'news_download_filename':f'INL_news_updates/{search_date_str}_INL_news_download.xls',\n",
    "                       'deletions_filename':f'INL_news_updates/{search_date_str}_INL_news_deletions.xls',\n",
    "                       'make_deletions': True,\n",
    "                       'gen_docx': True,\n",
    "                       'docx_title':'INL Monthly News Update',\n",
    "                       'docx_intro':None,\n",
    "                       'docx_headers':{ 1: 'Carbon Fiber and Polymer Composites',\n",
    "                                        2: 'Lightweight Materials', \n",
    "                                        3: 'Nanomaterials', \n",
    "                                        4: 'Next Generation Materials and Vehicles',\n",
    "                                        5: 'Propulsion Materials',\n",
    "                                      },\n",
    "                       'auto_id_research':False, # Determines whether research articles are automatically identified as such or not.  \n",
    "                                                 # If true, classifies all journal articles as \"Relevant Transportation Research\".\n",
    "                                                 # docx_headers must have a header called \"Relevant Transportation Research\" if this is set to True.\n",
    "                       'restrict_sites':False,\n",
    "                          'news_df': None\n",
    "                      }\n",
    "                \n",
    "    \n",
    "}\n",
    "\n",
    "# Generates an error message if AFV scraper is run on a day other than Wednesday.\n",
    "if scraper_info['AFV']['day'] != 2:\n",
    "    sys.exit('Error: AFV scraper must ony be run on Wednesdays.')\n",
    "\n",
    "# Used for diagnostics/tracking later\n",
    "scrape_specs = {}\n",
    "\n",
    "# Needed for web scraping \"browser\"\n",
    "headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}\n",
    "\n",
    "# For database update; ensures duplicates aren't loaded\n",
    "db_update = False\n",
    "\n",
    "# Maximum number of sentences to be scraped for each article pulled.\n",
    "max_sentences = 5\n",
    "\n",
    "# Get a list of scrapers we wish to run today.\n",
    "todays_scrapers = [scraper for scraper in scraper_info if scraper_info[scraper]['day']==search_date.weekday()]\n",
    "\n",
    "# List of all scrapers in scraper_info\n",
    "all_scrapers = list(scraper_info.keys())\n",
    "\n",
    "try:\n",
    "    max_age = max([scraper_info[scraper]['max_age'] for scraper in scraper_info if scraper in todays_scrapers])\n",
    "except:\n",
    "    print('No scrapers are run on this weekday.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test a single scraper (i.e. CAV, 21CTP, etc.)\n",
    "To run a single scraper, set single_scraper = True and enter a scraper name in the input box. AFV scraper can only be run on Wednesdays due to its dependency on \"EVSE Market Analysis.ipynb.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_scraper = False#True\n",
    "if single_scraper:\n",
    "    choice = input('Which scraper would you like to test?\\nOptions:\\n'+ '\\n'.join(list(scraper_info.keys()))+'\\n')\n",
    "    choice = [scraper for scraper in scraper_info.keys() if choice.upper()==scraper.upper()][0]\n",
    "    while choice == '':\n",
    "        print('Please enter a valid input.')\n",
    "        choice = input('Which scraper would you like to test?\\nOptions:\\n', list(scraper_info.keys()))\n",
    "        choice = [scraper for scraper in scraper_info.keys() if choice.upper()==scraper.upper()][0]\n",
    "    print(choice, 'selected.')\n",
    "    if (choice == 'AFV') and (search_date.weekday() != 2):\n",
    "        sys.exit('Error: AFV scraper must ony be run on Wednesdays.')\n",
    "    else:\n",
    "        for scraper in scraper_info:\n",
    "            if scraper == choice:\n",
    "                scraper_info[scraper]['day'] = search_date.weekday()\n",
    "            else:\n",
    "                scraper_info[scraper]['day'] = None\n",
    "        # Get a list of scrapers we wish to run today.\n",
    "        todays_scrapers = [scraper for scraper in scraper_info if scraper_info[scraper]['day']==search_date.weekday()]\n",
    "\n",
    "        # List of all scrapers in scraper_info\n",
    "        all_scrapers = list(scraper_info.keys())\n",
    "        \n",
    "        max_age = max([scraper_info[scraper]['max_age'] for scraper in scraper_info if scraper in todays_scrapers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21CTP, INL scraper(s) will be run. Max age of articles is 31 days.\n"
     ]
    }
   ],
   "source": [
    "print(', '.join(todays_scrapers), 'scraper(s) will be run. Max age of articles is', max_age, 'days.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limit sites to scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful if you want to generate an entire news report restricted to just these sources.  Also useful if you just want to test the report-generating functions (e.g. gen_docx) and don't want to run the entire scraper.  Also a good way to test multiple sites at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_sites = False\n",
    "sites_to_scrape = ['Transportation Research', 'Transport Policy', 'Journal of Urban Economics'] #['Lightweighting World', 'Autoblog']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for pulling keywords, formatting scraper output and generating word doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T13:30:37.503023Z",
     "start_time": "2019-01-07T13:30:37.468125Z"
    }
   },
   "outputs": [],
   "source": [
    "# scraped_count = 0\n",
    "# skip_count = 0\n",
    "# too_old = 0\n",
    "# iteration = 0\n",
    "# skip_ind = []\n",
    "# old_ind = []\n",
    "#def click_button(driver, method, css, s = ''):\n",
    " #   button = eval(f'driver.find_element{s}_by_{method}')\n",
    "  #  button.click()\n",
    "    \n",
    "    \n",
    "def replace_em(text):\n",
    "    '''Replaces odd characters in text. Used for page titles and summaries'''\n",
    "    bad_chars = ['â€œ', 'â€™', 'â€�', '\\n', 'Â',\n",
    "                 'â€”', '(earlier post)', 'â€?', '\\t', 'â€œ', '(TNS) — ', '(Reuters) - ',\n",
    "                 'DUBAI (Reuters) - ', '(adsbygoogle = window.adsbygoogle || []).push({});']\n",
    "    for bad_char in bad_chars:\n",
    "        text = text.replace(bad_char, '')\n",
    "    return text\n",
    "\n",
    "def get_summary(sum_loc, article):\n",
    "    if type(sum_loc)==list:\n",
    "        for option in sum_loc:\n",
    "            try:\n",
    "                summary = eval(option)\n",
    "                break # Break out of the for loop once the summary is found.\n",
    "            except:\n",
    "                summary = ''\n",
    "                continue # Continue trying different options if summary has not been found.\n",
    "    elif type(sum_loc)==str:\n",
    "        summary = eval(sum_loc)\n",
    "    else:\n",
    "        sys.exit('Error: sum_loc must be a string or a list of strings.')\n",
    "    return summary\n",
    "\n",
    "# Pars must either be a list of beautifulsoup paragraph objects OR a string.\n",
    "def clean_summary(pars):\n",
    "    # Clean article paragraphs and join them into one block of text\n",
    "    try:\n",
    "        all_text = replace_em(' '.join(par.text for par in pars)) \n",
    "    except:\n",
    "        all_text = replace_em(pars)\n",
    "    # Split the article text up into sentences\n",
    "    sentence_list = tokenizer.tokenize(all_text)\n",
    "    # If the number of sentences is greater than max_sentences, return the first max_sentences sentences.\n",
    "    if len(sentence_list) > max_sentences:\n",
    "        return ' '.join([sentence.strip() for sentence in sentence_list[0:max_sentences]])\n",
    "    # Otherwise, return all sentences.\n",
    "    else:\n",
    "        return ' '.join(' '.join([sentence.strip() for sentence in sentence_list]).split())\n",
    "\n",
    "def grab_homepage(url):\n",
    "    '''Creates BeautifulSoup object using input url'''\n",
    "#     headers = {'user-agent': 'Mozilla/5.0'}\n",
    "    page_1 = requests.get(url, headers=headers)\n",
    "    return BeautifulSoup(page_1.content, 'lxml')#\"html5lib\")\n",
    "\n",
    "\n",
    "def print_results(site, scraped_count, skip_count, too_old, df, duration, scrape_specs):\n",
    "    '''Prints out a quick summary of one website's full scraping and adds summary specs to scrape_specs dictionary'''\n",
    "    print(f'{scraped_count} {site} article(s) scraped')\n",
    "    print(f'{skip_count} {site} article(s) skipped due to error')\n",
    "    print(f'{too_old} {site} article(s) skipped due to age')\n",
    "    print(f'{df.shape[0]} relevant article(s) collected')\n",
    "    scrape_specs[f\"{site}\"] = {'Pages Scraped': scraped_count, 'Relevant Articles': df.shape[0], 'Errors': skip_count,\n",
    "                               'Too old': too_old, 'Time spent': duration}\n",
    "    return scrape_specs\n",
    "\n",
    "\n",
    "def page_scan(title, summary, url, date, source):\n",
    "    '''\n",
    "    Searches a web page title and summary for keywords; returns the dictionary object that is used to create \n",
    "    the final dataframe. Searches the title first; if the keyword is there, it doesn't search the summary.\n",
    "\n",
    "    Only searches for keywords specific to that day of the week's news update.\n",
    "    '''\n",
    "    bool_dict = {scraper:0 for scraper in all_scrapers}\n",
    "    title_scrape = title+' '+title.lower()\n",
    "    summary_scrape = summary+' '+summary.lower()\n",
    "    \n",
    "    for scraper in todays_scrapers:\n",
    "        if any(keyword in title_scrape for keyword in scraper_info[scraper]['keywords']):\n",
    "            bool_dict[scraper] = 1\n",
    "            #print('title relevant')\n",
    "        elif any(keyword in summary_scrape for keyword in scraper_info[scraper]['keywords']):\n",
    "            bool_dict[scraper] = 1\n",
    "            #print('summary relevant')\n",
    "    if sum(bool_dict.values()) > 0:\n",
    "        scan_dict = {'title': title.strip(), 'summary': summary.strip(), 'link': url, 'source': source,\n",
    "                'date': date}\n",
    "        scan_dict.update({scraper:bool_dict[scraper] for scraper in all_scrapers})\n",
    "        return scan_dict\n",
    "    \n",
    "    else:\n",
    "        return 'Most definitely nope'\n",
    "\n",
    "# The following two functions are for the Word document output!\n",
    "\n",
    "\n",
    "def add_hyperlink(paragraph, url, text):\n",
    "    '''\n",
    "    :param paragraph: The paragraph we are adding the hyperlink to.\n",
    "    :param url: A string containing the required url\n",
    "    :param text: The text displayed for the url\n",
    "    :return: The hyperlink object\n",
    "    '''\n",
    "    # This gets access to the document.xml.rels file and gets a new relation id value\n",
    "    part = paragraph.part\n",
    "    r_id = part.relate_to(\n",
    "        url, docx.opc.constants.RELATIONSHIP_TYPE.HYPERLINK, is_external=True)\n",
    "\n",
    "    # Create the w:hyperlink tag and add needed values\n",
    "    hyperlink = docx.oxml.shared.OxmlElement('w:hyperlink')\n",
    "    hyperlink.set(docx.oxml.shared.qn('r:id'), r_id, )\n",
    "\n",
    "    # Create a w:r element\n",
    "    new_run = docx.oxml.shared.OxmlElement('w:r')\n",
    "\n",
    "    # Create a new w:rPr element\n",
    "    rPr = docx.oxml.shared.OxmlElement('w:rPr')\n",
    "\n",
    "    # bold the text\n",
    "    u = docx.oxml.shared.OxmlElement('w:b')\n",
    "    rPr.append(u)\n",
    "\n",
    "    # Join all the xml elements together add add the required text to the w:r element\n",
    "    new_run.append(rPr)\n",
    "    new_run.text = text\n",
    "    hyperlink.append(new_run)\n",
    "\n",
    "    paragraph._p.append(hyperlink)\n",
    "\n",
    "    return hyperlink\n",
    "\n",
    "\n",
    "def gen_docx(scraper, graph_bool = False, dwyer=True):\n",
    "    '''\n",
    "    Generates news Word doc using data file from web scrape\n",
    "    :param newstype: Either \"21CTP\", \"CAV\", or \"AFV\"\n",
    "    :param dwyer: If not running on Dwyer's computer, set this to False and put all needed files in the same directory\n",
    "    '''\n",
    "    \n",
    "    headers = scraper_info[scraper]['docx_headers']\n",
    "    \n",
    "    # select data file (xls) based on the newstype and date. Note that search_date_str is a global variable defined outside\n",
    "    # of this function. Each news update only happens once a week --> only one xls file per newstype per week --> can't just\n",
    "    # pick any old search_date_str and make a file.\n",
    "    if dwyer:\n",
    "        #Name of the excel file (standardized)\n",
    "        data_file = f\"{scraper.lower()}_news_updates/{search_date_str}_{scraper}_news_download.xls\"\n",
    "    else:\n",
    "        data_file = f\"{search_date_str}_{scraper}_news_download.xls\"\n",
    "    # Read the data in from the selected file\n",
    "    df = pd.read_excel(data_file)\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    df.category = df.category.astype('int64') # Cast category column to int\n",
    "    section_dict = {headers[header_no]: df[df.category==header_no].T.to_dict() for header_no in headers}\n",
    "\n",
    "    # Start creating the word doc\n",
    "    newsdoc = docx.Document(docx='python_docx.docx')\n",
    "\n",
    "    newsdoc.add_heading(scraper_info[scraper]['docx_title']+ ' - ' + f\"{search_date.strftime('%m/%d/%Y')}\", 0)\n",
    "    newsdoc.add_paragraph(' ')\n",
    "    if scraper_info[scraper]['docx_intro']!=None:\n",
    "        newsdoc.add_paragraph(scraper_info[scraper]['docx_intro'])\n",
    "    \n",
    "    # If it's an AFV day (i.e., Wednesday), add graphics generated by \"EVSE Market Analysis.ipynb\"\n",
    "    if scraper == 'AFV':\n",
    "        newsdoc.add_heading('EVSE Market Analysis', 1)\n",
    "        # Add EVSE bar chart if graph_bool is true\n",
    "        if graph_bool:\n",
    "            evse_bar_chart = EVSE_file_dict['EVSE_bar_chart']\n",
    "            newsdoc.add_paragraph().add_run().add_picture(evse_bar_chart, width=Inches(6))\n",
    "            CA_nums = EVSE_file_dict['CA_shares'] # Adjust this to actual string with numbers.  Read string in from CSV\n",
    "        # Otherwise create a placeholder where the graph can be added by hand\n",
    "        else:\n",
    "            evse_bar_chart = newsdoc.add_paragraph().add_run('INSERT EVSE BAR CHART HERE')\n",
    "            evse_bar_chart.font.bold = True\n",
    "            evse_bar_chart.font.size = Pt(16)\n",
    "            evse_bar_chart.font.highlight_color = WD_COLOR_INDEX.YELLOW\n",
    "            CA_nums='NEED TO INSERT' # Default string for the CA EVSE numbers (automatically populates the caption for the EVSE bar chart figure)\n",
    "        newsdoc.add_paragraph('Figure: Number of EVSE plugs (note: not stations) by state and charging level. '\n",
    "                              'CA is not included, since it would make the rest of the state numbers illegible. '\n",
    "                              f\"CA holds a disproportionately large share of the total EVSE plugs: {CA_nums} \"\n",
    "                              'of Level 1, Level 2, and DCFC plugs respectively. Data Source: U.S. DOE AFDC Station Locator.',\n",
    "                              style='Caption')\n",
    "        newsdoc.add_page_break()\n",
    "        newsdoc.add_paragraph('The table below summarizes overall changes in number of EV charging stations by state between '\n",
    "                              f\"{(search_date - dt.timedelta(7)).strftime('%m/%d/%Y')} and {search_date.strftime('%m/%d/%Y')}:\",\n",
    "                              style='Normal')\n",
    "        newsdoc.add_paragraph('Table 1: Change in number of EV charging stations by state, between '\n",
    "                              f\"{(search_date - dt.timedelta(7)).strftime('%m/%d/%Y')} and {search_date.strftime('%m/%d/%Y')}\",\n",
    "                              style='Caption')\n",
    "        evse_delta_table = newsdoc.add_paragraph().add_run('INSERT EVSE DELTA TABLE HERE')\n",
    "        evse_delta_table.font.bold = True\n",
    "        evse_delta_table.font.size = Pt(16)\n",
    "        evse_delta_table.font.highlight_color = WD_COLOR_INDEX.YELLOW\n",
    "        newsdoc.add_page_break()\n",
    "\n",
    "    for header in section_dict:\n",
    "        newsdoc.add_heading(header, 1)\n",
    "        if (header == 'Relevant Transportation Research') and ('research_intro' in scraper_info[scraper]):\n",
    "            newsdoc.add_paragraph(scraper_info[scraper]['research_intro'])\n",
    "\n",
    "        for row in section_dict[header]:\n",
    "            row = section_dict[header][row]\n",
    "            newsdoc.add_heading(row['title'], level=2)\n",
    "            p = newsdoc.add_paragraph(row['summary'] + ' ')\n",
    "            p.add_run('(')\n",
    "            # This is where the add_hyperlink function is used\n",
    "            add_hyperlink(p, '{}'.format(row['link']), '{}'.format(row['source']))\n",
    "            p.add_run(')')\n",
    "            \n",
    "    if dwyer:\n",
    "        filename = f\"{scraper.lower()}_news_updates/Energetics {scraper} News Update - {search_date_str}.docx\"\n",
    "        newsdoc.save(filename)\n",
    "    else:\n",
    "        filename = f\"Energetics {scraper} News Update - {search_date_str}.docx\"\n",
    "        newsdoc.save(filename)\n",
    "    return filename\n",
    "\n",
    "\n",
    "def which_keyword_found(row):\n",
    "    ''' Identifies and stores which keywords triggered the news item pull '''\n",
    "    words_found = []\n",
    "    todays_keywords = []\n",
    "    for scraper in todays_scrapers:\n",
    "        todays_keywords += scraper_info[scraper]['keywords']\n",
    "    for keyword in list(set(todays_keywords)):\n",
    "        try:\n",
    "            if (row['summary'].find(keyword) > 0) | (row['title'].find(keyword) > 0):\n",
    "                words_found.append(keyword)\n",
    "        except:\n",
    "            continue\n",
    "    return ', '.join(words_found)\n",
    "\n",
    "\n",
    "def keyword_pull(string):\n",
    "    ''' Pulls all relevant capitalized words out of the title, as a quick \"keyword\" list '''\n",
    "    not_keywords = ['A', 'New', 'First', 'Group', 'The', 'This', 'I', 'To', 'Who', 'Silicon', 'Valley', 'System', 'Build', 'Payment', 'Business', 'API', 'JV', 'JVs',\n",
    "                    'European', 'American', 'America', 'Europe', 'China', 'But', 'Are', 'They', 'Legal', 'Says', 'AV', 'Revolution', 'Is',\n",
    "                    'TechCrunch', 'For', 'EVs', 'Really', 'Get', 'Money', 'Adds', 'We', 'All', 'Starts', 'Return', 'Apart',\n",
    "                    'Them', 'Cities', 'After', 'Insurance', 'Back', 'Against', 'Would', 'Displace', 'Improves', 'While',\n",
    "                    'That', 'You', 'Find', 'Along', 'From', 'Their', 'Not', 'So', 'Say', 'Experts', 'Drivers', 'Its', 'Into', 'Fully',\n",
    "                    'Ranks', 'Stretch', 'SUV', 'Data', 'Sharing', 'Live', 'When', 'Agencies', 'Still', 'Trying', 'Program', 'Offer', 'Four',\n",
    "                    'Will', 'Backs', 'Just', 'Around', 'Years', 'Its', 'Future', 'Deploying', 'Objects', 'Distance', 'Highlights']\n",
    "    string = string.replace(';', '').replace(',', '').lstrip().split(' ')\n",
    "    keywords = [word for word in string if (word[0].isupper()) & (word not in not_keywords)]\n",
    "    return ', '.join(keywords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a scraper class that will be used for each website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T13:30:37.531950Z",
     "start_time": "2019-01-07T13:30:37.511002Z"
    }
   },
   "outputs": [],
   "source": [
    "class scraypah:\n",
    "    '''\n",
    "    Scraypah is a web scraper that searches through all of the recent articles on a website and extracts key information\n",
    "    from those that include relevant keywords. It requires a dictionary of parameters specific to each website that needs\n",
    "    to be scraped. See the __init__ docstring for information on the input parameter requirements.\n",
    "    '''\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"This is an object of class scraypah!\"\n",
    "\n",
    "    def __init__(self, params):\n",
    "        '''\n",
    "        Attributes:\n",
    "            params[url] (str): Homepage of the website, where each of the article page links are extracted from\n",
    "            params[source] (str): Name of the website\n",
    "            params[strain_bool] (bool): Is there a soup strainer for this website or not?\n",
    "            params[strain_tag] (str): Tag used for soup strainer\n",
    "            params[strain_attr_name] (str): Attribute name used for soup strainer\n",
    "            params[strain_attr_value] (str): Attribute value used for soup strainer\n",
    "            params[date_loc] (str): Location of the date in the HTML\n",
    "            params[date_format] (str): Allows user to set the date format, if the format on the website does not parse automaticall\n",
    "            params[sum_loc] (str): Location of the summary in the HTML (this is typically the first 3 paragraphs of the article)\n",
    "            params[title_loc] (str): Location of the title in the HTML\n",
    "            params[url_list_query] (str): BeautifulSoup code to extract the list of articles from the website homepage(s) (url)\n",
    "        '''\n",
    "        self.base_url = params['url']\n",
    "        self.source = params['source']\n",
    "        self.strainer = params['strain_bool']\n",
    "        if self.strainer:\n",
    "            self.strain_tag = params['strain_tag']\n",
    "            self.strain_attr_name = params['strain_attr_name']\n",
    "            self.strain_attr_value = params['strain_attr_value']\n",
    "        self.date_loc = params['date_loc']\n",
    "        self.date_format = params['date_format']\n",
    "        self.sum_loc = params['sum_loc']\n",
    "        self.title_loc = params['title_loc']\n",
    "        self.url_list_query = params['url_list_query']\n",
    "        self.css = params['css_bool']\n",
    "        if self.css:\n",
    "            self.load_more_css = params['load_more_css']\n",
    "            self.max_scrapes = params['max_scrapes']\n",
    "            self.max_loads = params['max_loads']\n",
    "            self.date_css = params['date_css']\n",
    "            self.title_css = params['title_css']\n",
    "            self.sum_css = params['sum_css']\n",
    "            \n",
    "\n",
    "    def get_urls(self):\n",
    "        '''Populates self.urls_to_scrape with a list of urls extracted from the website homepage(s)'''\n",
    "        self.urls_to_scrape = []\n",
    "        with requests.Session() as s:\n",
    "\n",
    "            # Checks if the base_url is a single url or a list of urls - some websites publish enough articles\n",
    "            # that we have to pull multiple pages\n",
    "            if isinstance(self.base_url, str):\n",
    "\n",
    "                # Checks if there is a \"soup strainer\" for the website being scraped. See here:\n",
    "                # https://www.crummy.com/software/BeautifulSoup/bs4/doc/#parsing-only-part-of-a-document\n",
    "                if not self.strainer:\n",
    "                    page = requests.get(self.base_url, headers=headers)\n",
    "                    time.sleep(0.5)\n",
    "                    self.base_soup = BeautifulSoup(page.content, \"lxml\")\n",
    "                else:\n",
    "                    only_parse = SoupStrainer(self.strain_tag, attrs={\n",
    "                                              self.strain_attr_name: self.strain_attr_value})\n",
    "                    self.base_soup = BeautifulSoup(requests.get(\n",
    "                        self.base_url, headers=headers).content, \"lxml\", parse_only=only_parse)\n",
    "\n",
    "                time.sleep(1)\n",
    "                self.urls_to_scrape = eval(self.url_list_query)\n",
    "\n",
    "            else:\n",
    "                for url in list(self.base_url):\n",
    "                    if not self.strainer:\n",
    "                        page = requests.get(url, headers=headers)\n",
    "                        time.sleep(0.5)\n",
    "                        self.base_soup = BeautifulSoup(page.content, \"lxml\")\n",
    "                    else:\n",
    "                        only_parse = SoupStrainer(self.strain_tag, attrs={\n",
    "                                                  self.strain_attr_name: self.strain_attr_value})\n",
    "                        self.base_soup = BeautifulSoup(requests.get(\n",
    "                            url, headers=headers).content, \"lxml\", parse_only=only_parse)\n",
    "                    time.sleep(1)\n",
    "                    self.urls_to_scrape += eval(self.url_list_query)\n",
    "        self.urls_to_scrape = list(set(self.urls_to_scrape))\n",
    "    \n",
    "    def scrape_em(self):\n",
    "        self.relevant_articles = {}\n",
    "        self.scraped_count = 0\n",
    "        self.skip_count = 0\n",
    "        self.too_old = 0\n",
    "        self.iteration = 0\n",
    "        self.skip_ind = []\n",
    "        self.old_ind = []\n",
    "        for url in self.urls_to_scrape:\n",
    "            time.sleep(0.3)\n",
    "            self.iteration += 1\n",
    "            summary = None\n",
    "            title = None\n",
    "            date = None\n",
    "            try:\n",
    "                with requests.Session() as s:\n",
    "                    page = s.get(url, headers=headers)\n",
    "                    #if self.source in ['Semiconductor Engineering', 'Reuters', 'Recode', 'CityLab', 'GreenCarCongress']:\n",
    "                     #   article = BeautifulSoup(page.content, \"html5lib\")\n",
    "                    #else:\n",
    "                    article = BeautifulSoup(page.content, \"lxml\")\n",
    "                    date = pd.to_datetime(eval(self.date_loc).strip().replace(\n",
    "                        '\\\\xa0', '').replace(' -\\nBy:', ''), format=self.date_format).date()\n",
    "                    if (date - search_date).days >= -max_age:\n",
    "                        summary = clean_summary(get_summary(self.sum_loc, article)) \n",
    "                        title = eval(self.title_loc).replace('â€™', \"'\").replace(\n",
    "                            '\\\\xa0', ' ').replace('\\\\n', '').lstrip().replace('  ', '')\n",
    "                        temp = page_scan(title, summary, url,\n",
    "                                         date, self.source)\n",
    "                        #print(title,'\\n', date, '\\n', summary, '\\n\\n')\n",
    "                        if temp != 'Most definitely nope':\n",
    "                            self.relevant_articles[self.scraped_count] = temp\n",
    "                        self.scraped_count += 1\n",
    "                        #print(title)\n",
    "                    else:\n",
    "                        self.too_old += 1\n",
    "                        self.old_ind.append(self.iteration-1)\n",
    "                        #break # Remove break if you want to scrape older articles or see how many articles will be scraped.\n",
    "            except Exception as exc:\n",
    "                print(\n",
    "                    f\"{str(exc)}: {url} \\ndate:{date}\\ntitle:{title}\\nsummary:{summary}\")\n",
    "                self.skip_count += 1\n",
    "                self.skip_ind.append(self.iteration-1)\n",
    "                continue\n",
    "        self.relevant_df = pd.DataFrame.from_dict(self.relevant_articles).T\n",
    "        if not self.relevant_df.empty:\n",
    "            self.relevant_df.drop_duplicates('link', inplace=True)\n",
    "    \n",
    "    def css_scrape_em(self):          \n",
    "\n",
    "        self.relevant_articles = {}\n",
    "        self.scraped_count = 0\n",
    "        self.skip_count = 0\n",
    "        self.too_old = 0\n",
    "        self.iteration = 0\n",
    "        self.skip_ind = []\n",
    "        self.old_ind = []\n",
    "        self.scraped_count = 0\n",
    "        papers = {}\n",
    "    \n",
    "        # Important to define these-- otherwise variables could be referenced before assignment.\n",
    "        title = ''\n",
    "        date = ''\n",
    "        summary = ''\n",
    "    \n",
    "        if type(self.base_url) != list:\n",
    "            self.base_url = [self.base_url]\n",
    "        for base_url in self.base_url:\n",
    "            load_button = ''\n",
    "            load_count = 0\n",
    "            still_more = True\n",
    "    \n",
    "            driver.get(base_url)\n",
    "            self.base_soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "    \n",
    "            # Click on CSS element to load more articles if available.\n",
    "            if self.load_more_css != None:\n",
    "                for loadNo in range(self.max_loads):\n",
    "                    try:\n",
    "                        time.sleep(.3)\n",
    "                        eval(self.load_more_css).click()\n",
    "                    except Exception as exc:\n",
    "                        pass\n",
    "                self.base_soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "            self.urls_to_scrape = eval(self.url_list_query)\n",
    "        \n",
    "            for url in self.urls_to_scrape:\n",
    "                title = ''\n",
    "                date = ''\n",
    "                summary = ''\n",
    "                time.sleep(0.3)\n",
    "                self.iteration += 1\n",
    "                bad_egg = False\n",
    "                if not still_more or self.scraped_count > self.max_scrapes:\n",
    "                    break\n",
    "                # Open article URL using selenium.\n",
    "                try:\n",
    "                    driver.get(url)\n",
    "                except TimeoutException as ex:\n",
    "                    print(ex.Message)\n",
    "                    driver.navigate().refresh()\n",
    "                article = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "                # Get article publication date, title, and summary.\n",
    "                try:\n",
    "                    # Click on CSS element to reveal the location of the publication date if needed.\n",
    "                    if self.date_css != None:\n",
    "                        eval(self.date_css).click()\n",
    "                        article = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "                    # Scrape the publication date.\n",
    "                    date = pd.to_datetime(eval(self.date_loc).strip().replace(\n",
    "                        '\\\\xa0', '').replace(' -\\nBy:', ''), format=self.date_format).date()\n",
    "                    # If the publication date shows the article is not too old (i.e., it's more recent than than max_age (in days)), \n",
    "                    # scrape it and see if it contains relevant keywords\n",
    "                    if ((date - search_date).days >= -max_age):\n",
    "                        # Click on CSS element to reveal the location of the title if needed.\n",
    "                        if self.title_css != None:\n",
    "                            eval(self.title_css).click()\n",
    "                            article = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "                        # Scrape and clean the title.\n",
    "                        title = eval(self.title_loc)\n",
    "                        title = replace_em(title)\n",
    "                        # Click on CSS element to reveal the location of the summary if needed.\n",
    "                        if self.sum_css != None:\n",
    "                            eval(self.sum_css).click()\n",
    "                            article = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "                        # Scrape and clean the summary.\n",
    "                        summary = clean_summary(get_summary(self.sum_loc, article)) \n",
    "                        temp = page_scan(title, summary, url,\n",
    "                                         date, self.source)\n",
    "                        if temp != 'Most definitely nope':\n",
    "                            self.relevant_articles[self.scraped_count] = temp\n",
    "                        self.scraped_count += 1\n",
    "                    else:\n",
    "                        self.too_old += 1\n",
    "                        self.old_ind.append(self.iteration-1)\n",
    "                        break # Remove break if you want to scrape older articles or see how many articles will be scraped.\n",
    "               \n",
    "            \n",
    "                except Exception as exc:\n",
    "                    print(f\"{str(exc)}: {url} \\ndate:{date}\\ntitle:{title}\\nsummary:{summary}\")\n",
    "                    self.skip_count += 1\n",
    "                    self.skip_ind.append(self.iteration-1)\n",
    "                    continue \n",
    "            \n",
    "            \n",
    "        self.relevant_df = pd.DataFrame.from_dict(self.relevant_articles).T\n",
    "        if not self.relevant_df.empty:\n",
    "            self.relevant_df.drop_duplicates('link', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters for each website you want to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T13:30:37.588794Z",
     "start_time": "2019-01-07T13:30:37.551893Z"
    }
   },
   "outputs": [],
   "source": [
    "# 'article' is the variable that stores the BeautifulSoup soup for a particular article page. e.g. \"energy.gov/some-article.\"\n",
    "# The values of date_loc, sum_loc, and title_loc should be the BeautifulSoup commands for accessing the date location, \n",
    "# summary location and title location, respectively.\n",
    "# News sources are rated from 1-3 (1-Best, 3-Worst).  Sources with a rating of 0 (e.g., academic journals) will be kept no matter what.\n",
    "# journal_bool is True for academic journals.  This way academic journal articles can be autocategorized later.\n",
    "\n",
    "scraper_dict = {'MIT': {'url': 'http://news.mit.edu/mit-news',\n",
    "                        'source': 'MIT',\n",
    "                        'css_bool': False,\n",
    "                        'strain_tag': 'ul',\n",
    "                        'strain_attr_name': 'class',\n",
    "                        'strain_attr_value': 'view-mit-news clearfix',\n",
    "                        'url_list_query': \"['http://news.mit.edu'+item.a['href'] for item in self.base_soup.find('ul', class_='view-mit-news clearfix').find_all('li')]\",\n",
    "                        'date_loc': \"article.find('span', attrs={'itemprop':'datePublished'}).text\",\n",
    "                        'date_format': None,\n",
    "                        'sum_loc': \"article.find('div', attrs={'class': 'field-item even'}).find_all('p')\",\n",
    "                        'title_loc': \"article.find('h1', attrs={'class':'article-heading'}).text\",\n",
    "                        'strain_bool': True,\n",
    "                        'journal_bool': False,\n",
    "                        'rating': 1},\n",
    "                'Semiconductor Engineering': {'url': 'http://semiengineering.com/category-main-page-iot-security/',\n",
    "                           'source': 'Semiconductor Engineering',\n",
    "                           'css_bool': False,\n",
    "                           'strain_tag': 'div',\n",
    "                           'strain_attr_name': 'class',\n",
    "                           'strain_attr_value': 'l_col',\n",
    "                           'url_list_query': \"[item['href'] for item in self.base_soup.find('div', class_='l_col').find_all('a', href=True,title=True)]\",\n",
    "                           'date_loc': \"article.find('div',class_='loop_post_meta').contents[0]\",\n",
    "                           'date_format': None,\n",
    "                           'sum_loc': \"article.find('div', class_='post_cnt post_cnt_first_letter').find_all('p')\",\n",
    "                           'title_loc': \"article.find('h1', class_='post_title').text\",\n",
    "                           'strain_bool': True,\n",
    "                           'journal_bool': False,                   \n",
    "                           'rating': 1},\n",
    "                \n",
    "                'Quartz': {'url': 'https://qz.com/search/self-driving',\n",
    "                           'source': 'Quartz',\n",
    "                           'css_bool': False,\n",
    "                           'url_list_query': \"['https://qz.com' + a['href'] for a in self.base_soup.find_all('a', class_='_5ff1a')]\",\n",
    "                           'date_loc': \"article.time.text\",\n",
    "                           'date_format': None,\n",
    "                           'sum_loc': \"article.find_all('p')\",\n",
    "                           'title_loc': \"article.h1.text\",\n",
    "                           'strain_bool': False,\n",
    "                           'journal_bool': False,\n",
    "                           'rating': 2},\n",
    "                            # Note: member exclusive articles for Quartz will be skipped.\n",
    "                'Recode': {'url': 'https://www.recode.net/',\n",
    "                           'source': 'Recode',\n",
    "                           'css_bool': False,\n",
    "                           'strain_tag': 'a',\n",
    "                           'strain_attr_name': 'data-analytics-link',\n",
    "                           'strain_attr_value': 'article',\n",
    "                           'url_list_query': \"[item['href'] for item in self.base_soup.find_all('a', attrs={'data-analytics-link':'article'})]\",\n",
    "                           'date_loc': \"article.time.text.replace('\\\\n', '')\",\n",
    "                           'date_format': None,\n",
    "                           'sum_loc': \"article.find_all('p')\",\n",
    "                           'title_loc': \"article.h1.text\",\n",
    "                           'strain_bool': True,\n",
    "                           'journal_bool': False,\n",
    "                           'rating': 2},\n",
    "                # Gov tech has errors due to pop up window, I think - can try solving this using selenium to click out of the window.\n",
    "                'GovTech': {'url': 'http://www.govtech.com/fs/transportation/',\n",
    "                            'source': 'GovTech',\n",
    "                            'css_bool': False,\n",
    "                            'url_list_query': \"[header.a['href'] for header in self.base_soup.find_all('h2')]\",\n",
    "                            'date_loc': \"article.find('span', class_='date').text.strip()\",\n",
    "                            'date_format': None,\n",
    "                            'sum_loc':\"article.find('div', 'Section1').find_all('p')\",\n",
    "                            'title_loc': \"article.find('h1').text.strip()\",\n",
    "                            'strain_bool': False,\n",
    "                            'journal_bool': False,\n",
    "                            'rating': 1},\n",
    "                'Reuters': {'url': [f'https://www.reuters.com/news/archive/technologynews?view=page&page={page_no}' for page_no in range(50)],\n",
    "                            'source': 'Reuters',\n",
    "                            'css_bool': False,\n",
    "                            'url_list_query': \"['https://www.reuters.com'+item.a['href'] for item in self.base_soup.find_all('div', class_='story-content')]\",\n",
    "                            'date_loc': \"article.find('div', attrs={'class':'ArticleHeader_date'}).text.split('/')[0]\",\n",
    "                            'date_format': '%B %d, %Y',\n",
    "                            'sum_loc': \"article.find('div', attrs={'class':'StandardArticleBody_body'}).find_all('p')\",\n",
    "                            'title_loc': \"article.h1.text\",\n",
    "                            'strain_bool': False,\n",
    "                            'journal_bool': False,\n",
    "                            'rating': 2},\n",
    "                'CityLab': {'url': [f'https://www.citylab.com/transportation/?page={page_no}' for page_no in range(1,3)],\n",
    "                            'source': 'CityLab',\n",
    "                            'css_bool': False,\n",
    "                            'strain_tag': ['h2', 'h1'],\n",
    "                            'strain_attr_name': 'class', 'strain_attr_value': ['c-promo__hed', 'c-river-item__hed c-river-item__hed--'],\n",
    "                            'url_list_query': \"[item.a['href'] for item in self.base_soup.find_all(['h1','h2'], class_=['c-promo__hed','c-river-item__hed c-river-item__hed--'])]\",\n",
    "                            'date_loc': \"article.time.text\",\n",
    "                            'date_format': None,\n",
    "                            'sum_loc': \"article.find('section', 's-article__section o-small-container').find_all('p')\",\n",
    "                            'title_loc': \"article.h1.text\",\n",
    "                            'strain_bool': True,\n",
    "                            'journal_bool': False,\n",
    "                            'rating': 1},\n",
    "                'Autoblog': {'url': [f'https://www.autoblog.com/archive/pg-{page_no}' for page_no in range(1,25)],\n",
    "                             'source': 'Autoblog',\n",
    "                             'css_bool': False,\n",
    "                             'strain_tag': 'h6',\n",
    "                             'strain_attr_name': 'class',\n",
    "                             'strain_attr_value': 'record-heading',\n",
    "                             'url_list_query': \"['https://www.autoblog.com' + header.a['href'] for header in self.base_soup.find_all('h6', class_ = 'record-heading')]\",\n",
    "                             'date_format': None,\n",
    "                             'date_loc': \"article.find('div', class_='post-date').text.strip().split(' at')[0]\",\n",
    "                             'sum_loc': [\"article.find('div', attrs={'class':'post-body'}).find_all('p')\",\"article.find('div', class_='post-body').text\"],\n",
    "                             'title_loc': \"article.h1.text\",\n",
    "                             'strain_bool': True,\n",
    "                             'journal_bool': False,\n",
    "                             'rating': 3},\n",
    "                'Electrek': {'url': ['https://electrek.co/'] + \n",
    "                                     ['https://electrek.co/page/' + str(i) for i in range(2,30)],\n",
    "                             'source': 'Electrek',\n",
    "                             'css_bool': False,\n",
    "                             'strain_tag': 'h1',\n",
    "                             'strain_attr_name': 'class', 'strain_attr_value': 'post-title',\n",
    "                             'url_list_query': \"[item.a['href'] for item in self.base_soup.find_all('h1', class_='post-title')]\",\n",
    "                             'date_loc': \"article.find('p', class_='time-twitter').text\",\n",
    "                             'date_format': None,\n",
    "                             'sum_loc': \"article.find('div', class_='post-body').find_all('p')[1:]\",\n",
    "                             'title_loc': \"article.find('h1', class_='post-title').text\",\n",
    "                             'strain_bool': True,\n",
    "                             'journal_bool': False,\n",
    "                             'rating': 3},\n",
    "                'The Verge': {'url': [f'https://www.theverge.com/transportation/archives/{page_no}' for page_no in range(1,3)],\n",
    "                              'source': 'The Verge',\n",
    "                              'css_bool': False,\n",
    "                              'strain_tag': 'h2',\n",
    "                              'strain_attr_name': 'class', 'strain_attr_value': 'c-entry-box--compact__title',\n",
    "                              'url_list_query': \"[item.a['href'] for item in self.base_soup.find_all('h2', class_='c-entry-box--compact__title')]\",\n",
    "                              'date_loc': \"article.time.text\",\n",
    "                              'date_format': None,\n",
    "                              'sum_loc': \"article.find_all('p')\",\n",
    "                              'title_loc': \"article.h1.text\",\n",
    "                              'strain_bool': True,\n",
    "                              'journal_bool': False,\n",
    "                              'rating': 2},\n",
    "                'Crunchbase': {'url': [f'https://news.crunchbase.com/page/{page_no}' for page_no in range(1,14)],\n",
    "                               'source': 'Crunchbase',\n",
    "                               'css_bool': False,\n",
    "                               'url_list_query': \"[header.a['href'] for header in self.base_soup.find_all('h2', 'entry-title h3') if header.a!=None]\",\n",
    "                               'date_loc': \"article.find('div', class_='meta-item herald-date').text\",\n",
    "                               'date_format': None,\n",
    "                               'sum_loc': \"article.find('div', class_='entry-content herald-entry-content').find_all('p')\",\n",
    "                               'title_loc': \"article.h1.text\",\n",
    "                               'strain_bool': False,\n",
    "                               'journal_bool': False,\n",
    "                               'rating': 3},\n",
    "                'Truck News': {'url': [f'https://www.trucknews.com/news/page/{page_no}/' for page_no in range(1,11)],\n",
    "                               'source': 'Truck News',\n",
    "                               'css_bool': False,\n",
    "                               'url_list_query': \"[item.a['href'] for item in self.base_soup.find('ul', class_='media-list').find_all('h4')]\",\n",
    "                               'date_loc': \"article.find('div', class_ = 'well').find('p').text.split('by')[0].strip()\",\n",
    "                               'date_format': None,\n",
    "                               'sum_loc': \"article.find('div', class_ = 'the-content').find_all('p')\",\n",
    "                               'title_loc': \"article.find('h2').text.strip()\",\n",
    "                               'strain_bool': False,\n",
    "                               'journal_bool': False,\n",
    "                               'rating': 3},\n",
    "                'Trucks.com': {'url': ['https://www.trucks.com/category/news/'],\n",
    "                               'source': 'Trucks.com',\n",
    "                               'css_bool': False,\n",
    "                               'url_list_query': \"[header.a['href'] for header in self.base_soup.find_all('h2', 'cb-post-title')]\",\n",
    "                               'date_loc': \"article.find('div', 'date-author').text.split(' by')[0].strip()\",\n",
    "                               'date_format': '%B %d, %Y',\n",
    "                               'sum_loc': \"article.find('section', itemprop = 'articleBody').find_all('p')\",\n",
    "                               'title_loc': \"article.h1.text\",\n",
    "                               'strain_bool': False,\n",
    "                               'journal_bool': False,\n",
    "                               'rating': 2},\n",
    "                'TechCrunch': {'url': 'https://techcrunch.com/', \n",
    "                               'source': 'TechCrunch',\n",
    "                               'css_bool': True,\n",
    "                               'date_css': None,\n",
    "                               'sum_css':None,\n",
    "                               'title_css':None,\n",
    "                               'load_more_css':'driver.find_element_by_css_selector(\"button.load-more\")',\n",
    "                               'max_scrapes':200,\n",
    "                               'max_loads':40,\n",
    "                               'strain_tag': 'a',\n",
    "                               'strain_attr_name': 'class',\n",
    "                               'strain_attr_value': 'post-block__title__link',\n",
    "                               'url_list_query': \"['https://techcrunch.com' + item['href'] for item in self.base_soup.find_all('a', class_='post-block__title__link')]\",\n",
    "                               'date_loc': \"driver.find_element_by_css_selector(\\'time.time-since\\').get_attribute(\\'datetime\\').split('T')[0]\",\n",
    "                               'date_format': '%Y-%m-%d',\n",
    "                               'sum_loc': \"article.find('div', attrs={'class':'article-content'}).find_all('p')\",\n",
    "                               'title_loc': \"article.find('h1', attrs={'class':'article__title'}).text\",\n",
    "                               'strain_bool': True,\n",
    "                               'journal_bool': False,\n",
    "                               'rating': 3},\n",
    "                'Charged EVs': {'url': ['https://chargedevs.com/category/newswire/', 'https://chargedevs.com/category/newswire/page/2/'],\n",
    "                                'source': 'Charged EVs',\n",
    "                                'css_bool': False,\n",
    "                                'strain_tag': 'h3',\n",
    "                                'strain_attr_name': 'class',\n",
    "                                'strain_attr_value': 'h2',\n",
    "                                'url_list_query': '[item.a[\"href\"] for item in self.base_soup.find_all(\"h3\", class_=\"h2\")]',\n",
    "                                'date_loc': \"article.find('time').text\",\n",
    "                                'date_format': None,\n",
    "                                'sum_loc': \"article.find('section',class_='entry-content clearfix').find_all('p')\",\n",
    "                                'title_loc': \"article.find('h2', class_='page-title').text\",\n",
    "                                'strain_bool': True,\n",
    "                                'journal_bool': False,\n",
    "                                'rating': 3},\n",
    "                'ARS Technica': {'url': 'https://arstechnica.com/cars/',\n",
    "                                 'source': 'ARS Technica',\n",
    "                                 'css_bool': False,\n",
    "                                 'strain_tag': 'a',\n",
    "                                 'strain_attr_name': 'class',\n",
    "                                 'strain_attr_value': 'overlay',\n",
    "                                 'url_list_query': \"[item['href'] for item in self.base_soup.find_all('a', attrs={'class': 'overlay'})]\",\n",
    "                                 'date_loc': \"article.find('time', attrs={'class':'date'}).text\",\n",
    "                                 'date_format': None,\n",
    "                                 'sum_loc': \"article.find('div', attrs={'itemprop':'articleBody'}).find_all('p', attrs={'class':None})\",\n",
    "                                 'title_loc': \"article.h1.text\",\n",
    "                                 'strain_bool': True,\n",
    "                                 'journal_bool': False,\n",
    "                                 'rating': 3},\n",
    "                'Venture Beat': {'url': 'https://venturebeat.com/category/transportation/',\n",
    "                                 'source': 'Venture Beat',\n",
    "                                 'css_bool': False,\n",
    "                                 'url_list_query': \"[item.a['href'] for item in self.base_soup.select('h2.article-title')]+[item.a['href'] for item in self.base_soup.select('article')]\",\n",
    "                                 'date_loc': \"article.find('meta', attrs={'property':'article:published_time'})['content']\",\n",
    "                                 'date_format': None,\n",
    "                                 'sum_loc': \"article.find('div', class_ = 'article-content').find_all('p')\",\n",
    "                                 'title_loc': \"article.find('h1').text\",\n",
    "                                 'strain_bool': False,\n",
    "                                 'journal_bool': False,\n",
    "                                 'rating': 3},\n",
    "                'IEEE Spectrum': {'url': 'https://spectrum.ieee.org/transportation', \n",
    "                                  'source': 'IEEE Spectrum',\n",
    "                                  'css_bool': False,\n",
    "                                  'strain_tag': 'article',\n",
    "                                  'strain_attr_name': 'class',\n",
    "                                  'url_list_query': \"['https://spectrum.ieee.org'+item.a['href'] for item in self.base_soup.find_all('article')]\",\n",
    "                                  'strain_attr_value': 'item sml_article transportation',\n",
    "                                  'date_loc': \"article.label.text\",\n",
    "                                  'date_format': '%d %b %Y | %H:%M GMT',\n",
    "                                  'sum_loc': \"article.find_all('p')\",\n",
    "                                  'title_loc': \"article.h1.text\",\n",
    "                                  'strain_bool': True,\n",
    "                                  'journal_bool': False,\n",
    "                                  'rating': 1},\n",
    "                'Transport Topics': {'url': ['https://www.ttnews.com/government',\n",
    "                                             'https://www.ttnews.com/government?page=1',\n",
    "                                             'https://www.ttnews.com/government?page=2',\n",
    "                                             'https://www.ttnews.com/government?page=3',\n",
    "                                             'https://www.ttnews.com/business',\n",
    "                                             'https://www.ttnews.com/business?page=1',\n",
    "                                             'https://www.ttnews.com/business?page=2',\n",
    "                                             'https://www.ttnews.com/technology',\n",
    "                                             'https://www.ttnews.com/technology?page=1',\n",
    "                                             'https://www.ttnews.com/technology?page=2',\n",
    "                                             'https://www.ttnews.com/equipment',\n",
    "                                             'https://www.ttnews.com/equipment?page=1',\n",
    "                                             'https://www.ttnews.com/equipment?page=2'],\n",
    "                                     'source': 'Transport Topics',\n",
    "                                     'css_bool': False,\n",
    "                                     'vehicle_only_source' : True,\n",
    "                                     'url_list_query': \"['https://www.ttnews.com'+item.a['href'] for item in self.base_soup.find_all('h2', class_='content-access-1067')]\",\n",
    "                                     'date_loc': \"article.find('span',class_='date-display-single')['content']\",\n",
    "                                     'date_format': None,\n",
    "                                     'sum_loc': \"[p for p in article.find_all('p') if p.text and len(p.text)>10]\",\n",
    "                                     'title_loc': \"article.find('h1').text\",\n",
    "                                     'strain_bool': False,\n",
    "                                     'journal_bool': False,\n",
    "                                     'rating': 1},\n",
    "                'GreenCarCongress': {'url': [f'http://www.greencarcongress.com/page/{page_no}/' for page_no in range(1,5)],\n",
    "                                     'source': 'GreenCarCongress',\n",
    "                                     'css_bool': False,\n",
    "                                     'strain_tag': 'div',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'highlight-image-wrapper',\n",
    "                                     'url_list_query': \"[div.a['href'] for div in self.base_soup.find_all('div', 'highlight-image-wrapper')]\",\n",
    "                                     'date_loc': \"article.find('span', 'entry-date').a.text\",\n",
    "                                     'date_format': '%d %B %Y',\n",
    "                                     'sum_loc': \"article.find('div', 'entry-body font-entrybody').find_all('p')\",\n",
    "                                     'title_loc': \"article.h2.a.text\",\n",
    "                                     'strain_bool': True,\n",
    "                                     'journal_bool': False,\n",
    "                                     'rating': 1},\n",
    "            #    'Green Car Reports': {'url': [f'https://www2.greencarreports.com/news/page-{page_no}' for page_no in range(1,7)],\n",
    "             #                        'source': 'Green Car Reports',\n",
    "              #                       'css_bool': True,\n",
    "               #                      'max_loads':None,\n",
    "                #                     'max_scrapes':500,\n",
    "                 #                    'load_more_css':None,\n",
    "                  #                   'date_css':None,\n",
    "                   #                  'title_css':None,\n",
    "                    #                 'sum_css':None,\n",
    "                     #                'strain_tag': 'a',\n",
    "                      #               'strain_attr_name': 'class',\n",
    "                       #              'strain_attr_value': 'title',\n",
    "                        #             'url_list_query': \"['https://www.greencarreports.com' + a['href'] for a in self.base_soup.find_all('a', 'title')]\",\n",
    "                         #            'date_loc': \"article.time['datetime'].split('T')[0]\",\n",
    "                          #           'date_format': None,\n",
    "                           #          'sum_loc': \"article.find('section', class_='article-body').find_all('p')\",\n",
    "                            #         'title_loc': \"article.h1.text\",\n",
    "                             #        'strain_bool': True,\n",
    "                              #       'journal_bool': False,\n",
    "                               #      'rating': 3},\n",
    "                'The Fuse': {'url': 'http://energyfuse.org/category/autonomous-vehicles/',\n",
    "                                     'source': 'The Fuse',\n",
    "                                     'css_bool': False,\n",
    "                                     'strain_tag': 'div',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'category-content-block active',\n",
    "                                     'url_list_query': \"[a['href'] for a in self.base_soup.find_all('a', class_='full-block-link')]\",\n",
    "                                     'date_loc': \"article.h2.text.split('| ')[-1]\",\n",
    "                                     'date_format': None,\n",
    "                                     'sum_loc': \"article.find('div', class_='content-wrapper').find('p')\",\n",
    "                                     'title_loc': \"article.h1.text\",\n",
    "                                     'strain_bool': True,\n",
    "                                     'journal_bool': False,\n",
    "                                     'rating': 3},\n",
    "                'Business Wire': {'url': 'https://www.businesswire.com/portal/site/home/news/',\n",
    "                                     'source': 'Business Wire',\n",
    "                                     'css_bool': False,\n",
    "                                     'strain_tag': 'a',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'bwTitleLink',\n",
    "                                     'url_list_query': \"['https://www.businesswire.com' + a['href'] for a in self.base_soup.find_all('a', class_='bwTitleLink') if '/en/' in a['href']]\",\n",
    "                                     'date_loc': \"' '.join(article.find('time').text.split()[:3])\",\n",
    "                                     'date_format': None,\n",
    "                                     'sum_loc': \"article.find('div', class_='bw-release-story').find_all('p')\",\n",
    "                                     'title_loc': \"' '.join(article.h1.text.strip().split())\",\n",
    "                                     'strain_bool': True,\n",
    "                                     'journal_bool': False,\n",
    "                                     'rating': 3},\n",
    "                'U.S. Department of Energy': {'url': 'https://www.energy.gov/listings/energy-news',\n",
    "                                     'source': 'U.S. Department of Energy',\n",
    "                                     'css_bool': False,\n",
    "                                     'strain_tag': 'a',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'title-link',\n",
    "                                     'url_list_query': \"['https://www.energy.gov' + a['href'] for a in self.base_soup.find_all('a', class_='title-link')]\",\n",
    "                                     'date_loc': \"article.find('div', class_='node-hero-date').text\",\n",
    "                                     'date_format': None,\n",
    "                                     'sum_loc': \"article.find('div', class_='field-items').find_all('p')\",\n",
    "                                     'title_loc': \"article.h1.text\",\n",
    "                                     'strain_bool': True,\n",
    "                                     'journal_bool': False,\n",
    "                                     'rating': 2},\n",
    "               \n",
    "                'Journal of Modern Transportation': {'url': 'https://link.springer.com/journal/40534/onlineFirst/page/1',\n",
    "                                     'source': 'Journal of Modern Transportation',\n",
    "                                     'css_bool': False,\n",
    "                                     'strain_tag': 'div',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'toc-item',\n",
    "                                     'url_list_query': \"['https://link.springer.com' + title.a['href'] for title in self.base_soup.find_all('div', class_='toc-item') if title.p.text == 'OriginalPaper']\",\n",
    "                                     'date_loc': \"article.time.text\",\n",
    "                                     'date_format': None,\n",
    "                                     'sum_loc': \"article.find('section', class_='Abstract').p\",\n",
    "                                     'title_loc': \"article.h1.text\",\n",
    "                                     'strain_bool': True,\n",
    "                                     'journal_bool': True,\n",
    "                                     'rating': 0},\n",
    "                \n",
    "                'Jalopnik': {'url':  'https://jalopnik.com/c/news',\n",
    "                                     'source': 'Jalopnik',\n",
    "                                     'css_bool': False,\n",
    "                                     'strain_tag': 'article',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'postlist__item--compact',\n",
    "                                     'url_list_query': \"[article.a['href'] for article in self.base_soup.find_all('article', class_='postlist__item--compact')]\",\n",
    "                                     'date_loc': \"article.time['datetime'].split('T')[0]\",\n",
    "                                     'date_format': None,\n",
    "                                     # The below method of searching for the summary filters out some odd divs in the middle of articles.\n",
    "                                     'sum_loc': \"[par for par in list(article.find('div', class_='post-content entry-content js_entry-content ').children) if str(par)[:3] =='<p>']\",\n",
    "                                     'title_loc': \"article.h1.text\",\n",
    "                                     'strain_bool': True,\n",
    "                                     'journal_bool': False,\n",
    "                                     'rating': 3},\n",
    "                \n",
    "                'Bloomberg': {'url':  ['https://www.bloomberg.com/search?query=self-driving&sort=time:desc',\n",
    "                                       'https://www.bloomberg.com/search?query=self-driving&sort=time:desc&page=2'],\n",
    "                                     'source': 'Bloomberg',\n",
    "                                     'css_bool': False,\n",
    "                                     'strain_tag': 'h1',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'search-result-story__headline',\n",
    "                                     'url_list_query': \"[article.a['href'] for article in self.base_soup.find_all('h1', class_ = 'search-result-story__headline')]\",\n",
    "                                     'date_loc': \"article.time['datetime'].split('T')[0]\",\n",
    "                                     'date_format': None,\n",
    "                                     'sum_loc': \"article.find('div', class_='middle-column').find_all('p')\",\n",
    "                                     'title_loc': \"article.h1.text\",\n",
    "                                     'strain_bool': True,\n",
    "                                     'journal_bool': False,\n",
    "                                     'rating': 2},\n",
    "                \n",
    "                'Business Insider': {'url':  [f'https://www.businessinsider.com/sai?page={page_no}' for page_no in range(1,9)],\n",
    "                                     'source': 'Business Insider',\n",
    "                                     'css_bool': False,\n",
    "                                     'url_list_query': \"[a['href'] for a in self.base_soup.find_all('a', class_='title')[1:]]\",\n",
    "                                     'date_loc': \"article.find('div', class_ = 'byline-timestamp')['data-timestamp'].split('T')[0]\",\n",
    "                                     'date_format': None,\n",
    "                                     'sum_loc': \"article.find('div', id='piano-inline').findNext('div').find_all('p')\",\n",
    "                                     'title_loc': \"article.h1.text\",\n",
    "                                     'strain_bool': False,\n",
    "                                     'journal_bool': False,\n",
    "                                     'rating': 3},\n",
    "                \n",
    "                'CNET': {'url':  'https://www.cnet.com/roadshow/search/?query=self-driving',\n",
    "                                     'source': 'CNET',\n",
    "                                     'css_bool': False,\n",
    "                                     'strain_tag': 'section',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'searchItem product',\n",
    "                                     'url_list_query': \"['https://www.cnet.com' + article.a['href'] for article in self.base_soup.find_all('section', class_='searchItem product')]\",\n",
    "                                     'date_loc': \"article.find('span', class_='formattedDate').text\",\n",
    "                                     'date_format': None,\n",
    "                                     'sum_loc': \"article.find('div', class_='col-7 article-main-body row').find_all('p')\",\n",
    "                                     'title_loc': \"article.h1.text\",\n",
    "                                     'strain_bool': True,\n",
    "                                     'journal_bool': False,\n",
    "                                     'rating': 3},\n",
    "                \n",
    "                'Electric VTOL News': {'url':  'http://evtol.news/news/',\n",
    "                                     'source': 'Electric VTOL News',\n",
    "                                     'css_bool': False,\n",
    "                                     'strain_tag': 'h2',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'entry-title',\n",
    "                                     'url_list_query': \"[title.a['href'] for title in self.base_soup.find_all('h2', class_='entry-title')]\",\n",
    "                                     'date_loc': \"article.time.text\",\n",
    "                                     'date_format': None,\n",
    "                                     'sum_loc': \"article.find('div', class_='entry-content').find_all('p')\",\n",
    "                                     'title_loc': \"article.h1.text\",\n",
    "                                     'strain_bool': True,\n",
    "                                     'journal_bool': False,\n",
    "                                     'rating': 1},\n",
    "                \n",
    "                # 'Detroit News': {'url': 'https://www.detroitnews.com/autos/',\n",
    "                    #                 'source': 'Detroit News',\n",
    "                     #                'url_list_query': \"['https://www.detroitnews.com' + a['href'] for a in self.base_soup.find('div', class_='headline-page active').find_all('a')]\",\n",
    "                      #               'css_bool': True,\n",
    "                      #               #'load_more_css': \"a.button-add-content\",\n",
    "                       #              'load_more_css': None,\n",
    "                        #             'max_scrapes': 30,\n",
    "                         #            #'max_loads': 2, # Make sure that loads is not too many -- otherwise load button may become inactive.  Test this.\n",
    "                          #           'max_loads': None,\n",
    "                            #         'date_css': None,\n",
    "                             #        'date_loc': \"article.find('span', class_='asset-metabar-time asset-metabar-item nobyline').text.split('ET ')[1].split(' |')[0]\",\n",
    "                              #       'date_format': '%b. %d, %Y',\n",
    "                               #      'sum_css': None,\n",
    "                                #     'sum_loc': \"[article.find('p', class_='speakable-p-1 p-text'), soup.find('p', class_='speakable-p-2 p-text')]\",\n",
    "                                 #    'title_loc': \"article.find('h1', class_='asset-headline speakable-headline').text\",\n",
    "                                  #   'title_css': None,\n",
    "                                   #  'strain_bool': False,\n",
    "                                    # 'journal_bool': False,\n",
    "                                    # 'rating': 3},\n",
    "                                 \n",
    "                    'Transportation Research': {'url': ['https://www.journals.elsevier.com/transportation-research-part-a-policy-and-practice/recent-articles',\n",
    "                                                            'https://www.journals.elsevier.com/transportation-research-part-b-methodological/recent-articles',\n",
    "                                                            'https://www.journals.elsevier.com/transportation-research-part-c-emerging-technologies/recent-articles',\n",
    "                                                            'https://www.journals.elsevier.com/transportation-research-part-d-transport-and-environment/recent-articles',\n",
    "                                                            'https://www.journals.elsevier.com/transportation-research-part-e-logistics-and-transportation-review/recent-articles',\n",
    "                                                            'https://www.journals.elsevier.com/transportation-research-part-f-traffic-psychology-and-behaviour/recent-articles' \n",
    "                                                           ],\n",
    "                                     'source': 'Transportation Research',\n",
    "                                     'url_list_query': \"[div.a['href'] for div in self.base_soup.find_all('div', class_='pod-listing-header')]\",\n",
    "                                     'css_bool': True,\n",
    "                                     'load_more_css': None,\n",
    "                                     'max_scrapes': 100,\n",
    "                                     'max_loads': None,\n",
    "                                     'date_css': \"driver.find_element_by_css_selector(\\'button.show-hide-details\\')\",\n",
    "                                     'date_loc': \"article.find('div', 'wrapper').p.text.split('online ')[1][:-1]\",\n",
    "                                     'date_format': '%d %B %Y',\n",
    "                                     'sum_css': None,\n",
    "                                     'sum_loc': \"article.find('div', class_='abstract author').find_all('p')\",\n",
    "                                     'title_loc': \"article.h1.text\",\n",
    "                                     'title_css': None,\n",
    "                                     'strain_bool': False,\n",
    "                                     'journal_bool': True,\n",
    "                                     'rating': 0},\n",
    "    \n",
    "                    'Journal of Urban Economics': {'url': 'https://www.journals.elsevier.com/journal-of-urban-economics/recent-articles',\n",
    "                                     'source': 'Journal of Urban Economics',\n",
    "                                     'url_list_query': \"[div.a['href'] for div in self.base_soup.find_all('div', class_='pod-listing-header')]\",\n",
    "                                     'css_bool': True,\n",
    "                                     'load_more_css': None,\n",
    "                                     'max_scrapes': 100,\n",
    "                                     'max_loads': None,\n",
    "                                     'date_css': \"driver.find_element_by_css_selector(\\'button.show-hide-details\\')\",\n",
    "                                     'date_loc': \"article.find('div', 'wrapper').p.text.split('online ')[1][:-1]\",\n",
    "                                     'date_format': '%d %B %Y',\n",
    "                                     'sum_css': None,\n",
    "                                     'sum_loc': \"article.find('div', class_='Abstracts').find_all('p')\",\n",
    "                                     'title_loc': \"article.h1.text\",\n",
    "                                     'title_css': None,\n",
    "                                     'strain_bool': False,\n",
    "                                     'journal_bool': True,\n",
    "                                     'rating': 0},\n",
    "    \n",
    "                    'Transport Policy': {'url': 'https://www.journals.elsevier.com/transport-policy/recent-articles',\n",
    "                                     'source': 'Transport Policy',\n",
    "                                     'url_list_query': \"[div.a['href'] for div in self.base_soup.find_all('div', class_='pod-listing-header')]\",\n",
    "                                     'css_bool': True,\n",
    "                                     'load_more_css': None,\n",
    "                                     'max_scrapes': 100,\n",
    "                                     'max_loads': None,\n",
    "                                     'date_css': \"driver.find_element_by_css_selector(\\'button.show-hide-details\\')\",\n",
    "                                     'date_loc': \"article.find('div', 'wrapper').p.text.split('online ')[1][:-1]\",\n",
    "                                     'date_format': '%d %B %Y',\n",
    "                                     'sum_css': None,\n",
    "                                     'sum_loc': \"article.find('div', class_='Abstracts').find_all('p')\",\n",
    "                                     'title_loc': \"article.h1.text\",\n",
    "                                     'title_css': None,\n",
    "                                     'strain_bool': False,\n",
    "                                     'journal_bool': True,\n",
    "                                     'rating': 0},\n",
    "                \n",
    "                     'Science': {'url':  [f'https://www.sciencemag.org/news/latest-news?r3f_986=https%3A//www.google.ru/&page={page_no}' for page_no in range(7)],\n",
    "                                     'source': 'Science',\n",
    "                                     'css_bool': False,\n",
    "                                     'strain_tag': 'div',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'primary page primary--listpage',\n",
    "                                     'url_list_query': \"['https://www.sciencemag.org' + header.a['href'] for header in self.base_soup.find('div', 'primary page primary--listpage').find_all('h2')]\",\n",
    "                                     'date_loc': \"article.time.text.split(' ,')[0]\",\n",
    "                                     'date_format': None,\n",
    "                                     'sum_loc': \"article.find('div', 'article__body').find_all('p')[1:]\",\n",
    "                                     'title_loc': \"article.h1.text\",\n",
    "                                     'strain_bool': True,\n",
    "                                     'journal_bool': False,\n",
    "                                     'rating': 3},\n",
    "                     'Biomass Magazine': {'url':  [f'http://biomassmagazine.com/browse/30/{no*30}' for no in range(4)],\n",
    "                                     'source': 'Biomass Magazine',\n",
    "                                     'css_bool': False,\n",
    "                                     'url_list_query': \"['http://biomassmagazine.com' + header.a['href'] for header in self.base_soup.find('div', id = 'browse').find_all('h2')]\",\n",
    "                                     'date_loc': \"article.find('div', 'author').text.split('| ')[1].split('\\n')[0]\",\n",
    "                                     'date_format': None,\n",
    "                                     'sum_loc': \"article.find('div', 'body').find_all('p')\",\n",
    "                                     'title_loc': \"article.h1.text\",\n",
    "                                     'strain_bool': False,\n",
    "                                     'journal_bool': False,\n",
    "                                     'rating': 3},\n",
    "                 'Alternative Energy News': {'url': [f\"http://www.alternative-energy-news.info/technology/{topic}/page/{page_no}\" for topic in ['transportation', 'inventions'] for page_no in range(1,3)],\n",
    "                                     'source': 'Alternative Energy News',\n",
    "                                     'strain_tag': 'div',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'index_post',\n",
    "                                     'url_list_query': \"[div.a['href'] for div in self.base_soup.find_all('div', class_ = 'index_post')]\",\n",
    "                                     'css_bool': False,\n",
    "                                     'date_loc': \"article.find('div', 'post_content').div.text\",\n",
    "                                     'date_format':'%b %d',\n",
    "                                     'sum_loc': \"article.find('div', 'post_content_font').find_all('p')\",\n",
    "                                     'title_loc': \"article.h2.text\",\n",
    "                                     'strain_bool': False,\n",
    "                                     'journal_bool': False,\n",
    "                                     'rating': 3},\n",
    "                 'New Atlas': {'url': [f\"https://newatlas.com/transport/{page_no}/\" for page_no in range(1, 14)], # New Atlas is the new GizMag\n",
    "                                     'source': 'New Atlas',\n",
    "                                     'url_list_query': \"[header.a['href'] for header in self.base_soup.find_all('h2')]\",\n",
    "                                     'css_bool': False,\n",
    "                                     'date_loc': \"article.find('div','article-detail__byline').find_all('span')[-1].text\",\n",
    "                                     'date_format':None,\n",
    "                                     'sum_loc': \"article.find('div', 'ArticleBody article-detail__body').find_all('p')\",\n",
    "                                     'title_loc': \"article.h1.text\",\n",
    "                                     'strain_bool': False,\n",
    "                                     'journal_bool': False,\n",
    "                                     'rating': 3},\n",
    "                # 'Automotive News': {'url': 'https://www.autonews.com/news',\n",
    "                 #                    'source': 'Automotive News',\n",
    "                  #                   'url_list_query': \"['https://www.autonews.com' + div.a['href'] for div in self.base_soup.find_all('div', 'feature-article-headline')]\",\n",
    "                   #                  'css_bool': True,\n",
    "                    #                 'load_more_css': \"a.button.omnitrack\",\n",
    "                     #                'max_scrapes': 100,\n",
    "                      #               'max_loads': 70, # Make sure that loads is not too many -- otherwise load button may become inactive.  Test this.\n",
    "                       #              'date_css': None,\n",
    "                        #             'date_loc': \"' '.join(article.find('span', 'text-gray article-created-date').text.split()[:3])\",\n",
    "                         #            'date_format': None,\n",
    "                          #           'sum_css': None,\n",
    "                           #          'sum_loc': \"article.find('div', itemprop = 'articleBody').find_all('p')\",\n",
    "                            #         'title_loc': \"article.h1.text\",\n",
    "                             #        'title_css': None,\n",
    "                              #       'strain_bool': False,\n",
    "                               #      'journal_bool': False,\n",
    "                                #     'rating': 1},\n",
    "                 'AZoM': {'url': [f\"https://www.azom.com/materials-news-index.aspx?page={page_no}\" for page_no in range(25)], \n",
    "                                     'source': 'AZoM',\n",
    "                                     'strain_tag': 'div',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'col-xs-9',\n",
    "                                     'url_list_query': \"['https://azom.com' + div.a['href'] for div in self.base_soup.find_all('div', 'col-xs-9')]\",\n",
    "                                     'css_bool': False,\n",
    "                                     'date_loc': \"article.find('span', 'article-meta-date').text\",\n",
    "                                     'date_format':None,\n",
    "                                     'sum_loc': \"[p for p in article.find('div', 'item-body content-item-body clearfix').find_all('p')[1:] if p.text!='']\",\n",
    "                                     'title_loc': \"article.h1.text\",\n",
    "                                     'strain_bool': True,\n",
    "                                     'journal_bool': False,\n",
    "                                     'rating': 3},\n",
    "                 'CompositesWorld': {'url': [f\"https://www.compositesworld.com/news/list/{page_no}/\" for page_no in range(1, 10)], \n",
    "                                     'source': 'CompositesWorld',\n",
    "                                     'url_list_query': \"['https://www.compositesworld.com' + p.a['href'] for p in self.base_soup.find_all('p', 'headline')]\",\n",
    "                                     'css_bool': False,\n",
    "                                     'strain_tag': 'p',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'headline',\n",
    "                                     'date_loc': \"article.find('span', property = 'dc:created').text.split(': ')[1]\",\n",
    "                                     'date_format':None,\n",
    "                                     'sum_loc': \"article.find('div', id = 'short').find_all('p')\",\n",
    "                                     'title_loc': \"article.h1.text\",\n",
    "                                     'strain_bool': True,\n",
    "                                     'journal_bool': False,\n",
    "                                     'rating': 3},\n",
    "                'Lightweighting World': {'url':['http://lightweightingworld.com/category/in-the-news/',\n",
    "                                        'http://lightweightingworld.com/category/material-matter/',\n",
    "                                        'http://lightweightingworld.com/category/people-and-processes/',\n",
    "                                        'http://lightweightingworld.com/category/design-tooling/',\n",
    "                                        'http://lightweightingworld.com/category/supply-chains-logistics/'],\n",
    "                                     'source': 'Lightweighting World',\n",
    "                                     'css_bool': False,\n",
    "                                     'strain_tag': 'h2',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'post-title',\n",
    "                                     'url_list_query': \"[header.a['href'] for header in self.base_soup.find_all('h2', 'post-title')]\",\n",
    "                                     'date_format': None,\n",
    "                                     'date_loc': \"article.find('span', 'meta-date').find('span', 'meta-inner').text.strip()\",\n",
    "                                     'sum_loc': [\"article.find('div', attrs={'class':'post-body'}).find_all('p')\",\"soup.find('div', class_='post-body').text\"],\n",
    "                                     'title_loc': \"article.h1.text\",\n",
    "                                     'strain_bool': True,\n",
    "                                     'journal_bool': False,\n",
    "                                     'rating': 3},\n",
    "                'SAE International': {'url': 'https://www.sae.org/news/',\n",
    "                                     'source': 'SAE International',\n",
    "                                     'url_list_query': \"[a['href'] for a in self.base_soup.find_all('a', ' nx-card-view-link') if 'sae' in a['href']]\",\n",
    "                                     'css_bool': True,\n",
    "                                     'load_more_css': \"driver.find_element_by_css_selector(\\'button.nx-button.nx-button-more\\')\",\n",
    "                                     'max_scrapes': 100,\n",
    "                                     'max_loads': 25, # Make sure that loads is not too many -- otherwise load button may become inactive.  Test this.\n",
    "                                     'date_css': None,\n",
    "                                     'date_loc': \"article.find('span', 'nx-article-date').text.strip()\",\n",
    "                                     'date_format': '%Y-%m-%d',\n",
    "                                     'sum_css': None,\n",
    "                                     'sum_loc': \"article.find('article').find_all('p')\",\n",
    "                                     'title_loc': \"article.h2.text\",\n",
    "                                     'title_css': None,\n",
    "                                     'strain_bool': False,\n",
    "                                     'journal_bool': False,\n",
    "                                     'rating': 2},\n",
    "                 'Chemical & Engineering News': {'url': 'https://cen.acs.org/topics/materials.html',\n",
    "                                     'source': 'Chemical & Engineering News',\n",
    "                                     'url_list_query': \"['https://cen.acs.org' + header.a['href'] for header in self.base_soup.find_all('h2', 'topic-content-title') if header.a['href']!='' and 'postrelease' not in header.a['href']]\",\n",
    "                                     'css_bool': True,\n",
    "                                     'load_more_css': \"driver.find_element_by_css_selector(\\'button.load-more\\')\",\n",
    "                                     'max_scrapes': 30,\n",
    "                                     'max_loads': 10, # Make sure that loads is not too many -- otherwise load button may become inactive.  Test this.\n",
    "                                     'date_css': None,\n",
    "                                     'date_loc': \"article.find('div', 'article-intro-volume-number').text.split('|')[0].strip()\",\n",
    "                                     'date_format': '%B %d, %Y',\n",
    "                                     'sum_css': None,\n",
    "                                     'sum_loc': \"[p for div in article.find_all('div', 'text-left article-content') for p in div.find_all('p')][1:]\",\n",
    "                                     'title_loc': \"article.h1.text\",\n",
    "                                     'title_css': None,\n",
    "                                     'strain_bool': False,\n",
    "                                     'journal_bool': False,\n",
    "                                     'rating': 2},\n",
    "                'Popular Science': {'url':  'https://www.popsci.com/cars',\n",
    "                                     'source': 'Popular Science',\n",
    "                                     'css_bool': True, #Using selenium because page has hidden elements that can't be accessed otherwise\n",
    "                                     'max_loads':None,\n",
    "                                     'max_scrapes':200,\n",
    "                                     'load_more_css':None,\n",
    "                                     'date_css':None,\n",
    "                                     'title_css':None,\n",
    "                                     'sum_css':None,\n",
    "                                     'url_list_query': \"['https://www.popsci.com' + header.a['href'] for header in self.base_soup.find_all('h3')]\",\n",
    "                                     'date_loc': \"article.find('span', 'date timestamp-processed').text\",\n",
    "                                     'date_format': '%B %d, %Y',\n",
    "                                     'sum_loc': \"article.find('div', 'content-main basic-sidebar').find_all('p')\",\n",
    "                                     'title_loc': \"article.h1.text\",\n",
    "                                     'strain_bool': False,\n",
    "                                     'journal_bool': False,\n",
    "                                     'rating': 3},\n",
    "                 'National Center for Manufacturing Sciences': {'url': 'https://www.ncms.org/news/',\n",
    "                                                'source': 'National Center for Manufacturing Sciences',\n",
    "                                                'url_list_query': \"[header.a['href'] for header in self.base_soup.find_all('h4')]\",\n",
    "                                                'css_bool': False,\n",
    "                                                'date_loc': \"article.find('a', 'date updated').text\",\n",
    "                                                'date_format':'%d %B %Y',\n",
    "                                                'sum_loc': \"article.find('div', 'entry-content blog_postcontent').find_all('p')\",\n",
    "                                                'title_loc': \"article.h3.text.strip()\",\n",
    "                                                'strain_bool': False,\n",
    "                                                'journal_bool': False,\n",
    "                                                'rating': 1},\n",
    "                'Composites Manufacturing': {'url': ['http://compositesmanufacturingmagazine.com/category/columns/tech-talk/'] + [f'http://compositesmanufacturingmagazine.com/category/automotive/page/{page_no}' for page_no in range(1,9)],\n",
    "                                                'source': 'Composites Manufacturing',\n",
    "                                                'url_list_query': \"[header.a['href'] for header in self.base_soup.find_all('h2', 'h4')]\",\n",
    "                                                'css_bool': False,\n",
    "                                                'date_loc': \"article.find('time', 'updated').text\",\n",
    "                                                'date_format':None,\n",
    "                                                'sum_loc': \"article.find('div', itemprop='articleBody').find_all('p')\",\n",
    "                                                'title_loc':\"article.h1.text\",\n",
    "                                                'strain_bool': False,\n",
    "                                                'journal_bool': False,\n",
    "                                                'rating': 2},\n",
    "         #       'Nanowerk': {'url':  [f'https://www.nanowerk.com/category-nanoresearch.php?page={page_no}' for page_no in range(1,12)],\n",
    "          #                           'source': 'Nanowerk',\n",
    "           #                          'css_bool': True,\n",
    "            #                         'max_loads':None,\n",
    "             #                        'max_scrapes':200,\n",
    "              #                       'load_more_css':None,\n",
    "               #                      'date_css':None,\n",
    "                #                     'title_css':None,\n",
    "                 #                    'sum_css':None,\n",
    "                  #                   'url_list_query': \"[header.a['href'] for header in self.base_soup.find_all('h2')]\",\n",
    "                   #                  'date_loc': \"article.find('td', 'date').text\",\n",
    "                    #                 'date_format': '%b %d, %Y',\n",
    "                     #                'sum_loc': \"[' '.join([tr.text for tr in article.find_all('tr')[1:]])]\",\n",
    "                      #               'title_loc': \"article.h1.text\",\n",
    "                       #              'strain_bool': False,\n",
    "                        #             'journal_bool': False,\n",
    "                         #            'rating': 1},\n",
    "                'Engineering': {'url': \"https://www.engineering.com/Industries/IndustryArticle.aspx?industry=1\",\n",
    "                                     'source': 'Engineering',\n",
    "                                     'url_list_query': \"[a['href'] for a in self.base_soup.find_all('a', 'articleLinkTitle')]\",\n",
    "                                     'css_bool': True,\n",
    "                                     'load_more_css': \"\"\"driver.find_element_by_xpath(\\\"//a[contains(text(), \\'Next\\')]\\\")\"\"\",\n",
    "                                     'max_scrapes': 100,\n",
    "                                     'max_loads': 3, # Make sure that loads is not too many -- otherwise load button may become inactive.  Test this.\n",
    "                                     'date_css': None,\n",
    "                                     'date_loc': \"article.find('div', 'article').text.split('posted on ')[1].split(' |')[0]\",\n",
    "                                     'date_format': '%B %d, %Y',\n",
    "                                     'sum_css': None,\n",
    "                                     'sum_loc': \"article.find('div', id = 'articleBodyArea').text\",\n",
    "                                     'title_loc': \"article.find('span', 'article_title').text\",\n",
    "                                     'title_css': None,\n",
    "                                     'strain_bool': False,\n",
    "                                     'journal_bool': False,\n",
    "                                     'rating': 2},\n",
    "     #           'Materials Science & Engineering': {'url': 'https://www.scientific.net/Search/Search?age=0&SortBy=1&IncludePapers=true&searchString=vehicle',\n",
    "      #                      'source': 'Materials Science & Engineering',\n",
    "       #                     'css_bool': False,\n",
    "        #                    'strain_tag': 'div',\n",
    "         #                   'strain_attr_name': 'class', \n",
    "          #                  'strain_attr_value': 'main-volume-item-block',\n",
    "           #                 'url_list_query': \"['https://www.scientific.net' + div.a['href'] for div in self.base_soup.find_all('div', 'main-volume-item-block')]\",\n",
    "            #                'date_loc': \"April 1, 2019\",#[' 1, '.join(div.text.strip().split('\\n')[-1].strip().split()) for div in article.find_all('div', 'papers-block-info col-lg-12') if 'Online since' in div.text][0]\",\n",
    "             #               'date_format': '%B %d, %Y',\n",
    "              #              'sum_loc': \"article.find('p', 'normal-text')\",\n",
    "               #             'title_loc': \"article.h1.text\",\n",
    "                #            'strain_bool': True,\n",
    "                 #           'journal_bool': True,\n",
    "                  #          'rating': 0},\n",
    "                \n",
    "               }\n",
    "\n",
    "# Resets scraper dict to just include keys from sites_to_scrape if you set limit_sites = True\n",
    "if limit_sites:\n",
    "    scraper_dict = {site: scraper_dict[site] for site in sites_to_scrape}\n",
    "   #scraper_dict = {site: scraper_dict[site] for site in scraper_dict if scraper_dict[site]['css_bool']} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the scrapers\n",
    "Note: there will be a couple errors, especially with Autoblog. The scraper for that site still picks up a couple irrelevant items that it can't handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T13:36:05.340083Z",
     "start_time": "2019-01-07T13:30:37.595775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MIT\n",
      "32 MIT article(s) scraped\n",
      "0 MIT article(s) skipped due to error\n",
      "0 MIT article(s) skipped due to age\n",
      "15 relevant article(s) collected\n",
      "\n",
      "Semiconductor Engineering\n",
      "9 Semiconductor Engineering article(s) scraped\n",
      "0 Semiconductor Engineering article(s) skipped due to error\n",
      "11 Semiconductor Engineering article(s) skipped due to age\n",
      "3 relevant article(s) collected\n",
      "\n",
      "Quartz\n",
      "5 Quartz article(s) scraped\n",
      "0 Quartz article(s) skipped due to error\n",
      "5 Quartz article(s) skipped due to age\n",
      "2 relevant article(s) collected\n",
      "\n",
      "Recode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebarnard\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py:1204: UnknownTimezoneWarning: tzname EDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  category=UnknownTimezoneWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 Recode article(s) scraped\n",
      "0 Recode article(s) skipped due to error\n",
      "4 Recode article(s) skipped due to age\n",
      "12 relevant article(s) collected\n",
      "\n",
      "GovTech\n",
      "'NoneType' object has no attribute 'find_all': https://www.govtech.com/fs/New-Haven-Conn-Approves-Electric-Vehicle-Charging-Study.html \n",
      "date:2019-03-05\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.govtech.com/fs/transportation/New-York-Gets-Serious-About-Traffic-with-the-First-Citywide-US-Congestion-Pricing-Plan.html \n",
      "date:2019-04-02\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.govtech.com/biz/Startup-Ownums-First-Product-is-Blockchain-Vehicle-Titles.html \n",
      "date:2019-03-22\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.govtech.com/biz/Coord-Turns-Street-Photos-Into-Curb-Data-for-Six-Cities.html \n",
      "date:2019-04-03\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.govtech.com/fs/infrastructure/Rail-Travel-Is-Cleaner-than-Driving-or-Flying-but-Will-Americans-Buy-In.html \n",
      "date:2019-04-01\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.govtech.com/fs/transportation/St-Louis-Creates-Partnership-for-Multimodal-Trip-Planning-.html \n",
      "date:2019-03-06\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.govtech.com/public-safety/Will-California-Answer-the-Call-to-Ban-Hands-Free-Tech-in-Cars.html \n",
      "date:2019-04-04\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'find_all': https://www.govtech.com/fs/automation/Drivers-Embrace-Tech-But-Unsure-About-Self-Driving-Cars.html \n",
      "date:2019-03-15\n",
      "title:None\n",
      "summary:None\n",
      "21 GovTech article(s) scraped\n",
      "8 GovTech article(s) skipped due to error\n",
      "17 GovTech article(s) skipped due to age\n",
      "10 relevant article(s) collected\n",
      "\n",
      "Reuters\n",
      "562 Reuters article(s) scraped\n",
      "0 Reuters article(s) skipped due to error\n",
      "74 Reuters article(s) skipped due to age\n",
      "250 relevant article(s) collected\n",
      "\n",
      "CityLab\n",
      "24 CityLab article(s) scraped\n",
      "0 CityLab article(s) skipped due to error\n",
      "16 CityLab article(s) skipped due to age\n",
      "8 relevant article(s) collected\n",
      "\n",
      "Autoblog\n",
      "HTTPSConnectionPool(host='www.autoblog.comhttps', port=443): Max retries exceeded with url: //www.autoblog.com/cars-for-sale (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000000000D225AC8>: Failed to establish a new connection: [Errno 11004] getaddrinfo failed')): https://www.autoblog.comhttps://www.autoblog.com/cars-for-sale \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "HTTPSConnectionPool(host='www.autoblog.comhttps', port=443): Max retries exceeded with url: //amzn.to/2TE1DIf (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000000000D6A70B8>: Failed to establish a new connection: [Errno 11004] getaddrinfo failed')): https://www.autoblog.comhttps://amzn.to/2TE1DIf \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "HTTPSConnectionPool(host='www.autoblog.comhttps', port=443): Max retries exceeded with url: //www.autoblog.com/2019/02/26/kia-telluride-review/ (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000000000DC08D68>: Failed to establish a new connection: [Errno 11004] getaddrinfo failed')): https://www.autoblog.comhttps://www.autoblog.com/2019/02/26/kia-telluride-review/ \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "HTTPSConnectionPool(host='www.autoblog.comhttps', port=443): Max retries exceeded with url: //www.autoblog.com/photos/the-five-least-reliable-cars/ (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000000000D24F080>: Failed to establish a new connection: [Errno 11004] getaddrinfo failed')): https://www.autoblog.comhttps://www.autoblog.com/photos/the-five-least-reliable-cars/ \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "HTTPSConnectionPool(host='www.autoblog.comhttps', port=443): Max retries exceeded with url: //amzn.to/2FQHQMb (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000000000D631F28>: Failed to establish a new connection: [Errno 11004] getaddrinfo failed')): https://www.autoblog.comhttps://amzn.to/2FQHQMb \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "HTTPSConnectionPool(host='www.autoblog.comhttps', port=443): Max retries exceeded with url: //www.autoblog.com/photos/ebay-auction-finds-for-15000/ (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000000000D939588>: Failed to establish a new connection: [Errno 11004] getaddrinfo failed')): https://www.autoblog.comhttps://www.autoblog.com/photos/ebay-auction-finds-for-15000/ \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "('Unknown string format:', '6 days ago'): https://www.autoblog.com/photos/ebay-finds-fun-for-5-000/ \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "HTTPSConnectionPool(host='www.autoblog.comhttps', port=443): Max retries exceeded with url: //amzn.to/2XYs5uT (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000000000CBBDF98>: Failed to establish a new connection: [Errno 11004] getaddrinfo failed')): https://www.autoblog.comhttps://amzn.to/2XYs5uT \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "HTTPSConnectionPool(host='www.autoblog.comhttps', port=443): Max retries exceeded with url: //eds.us.matchbox.maruhub.com/survey/enter/s/ESV-vlt5-230079464/Sample_Flag/2 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000000000504BF28>: Failed to establish a new connection: [Errno 11004] getaddrinfo failed')): https://www.autoblog.comhttps://eds.us.matchbox.maruhub.com/survey/enter/s/ESV-vlt5-230079464/Sample_Flag/2 \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "HTTPSConnectionPool(host='www.autoblog.comhttps', port=443): Max retries exceeded with url: //www.autoblog.com/photos/jeep-gladiator-configurator-autoblog-editors/ (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000000000CEB9E10>: Failed to establish a new connection: [Errno 11004] getaddrinfo failed')): https://www.autoblog.comhttps://www.autoblog.com/photos/jeep-gladiator-configurator-autoblog-editors/ \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "HTTPSConnectionPool(host='www.autoblog.comhttps', port=443): Max retries exceeded with url: //www.autoblog.com/2019/03/28/2019-mazda6-manual-transmission/ (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000000000F224C50>: Failed to establish a new connection: [Errno 11004] getaddrinfo failed')): https://www.autoblog.comhttps://www.autoblog.com/2019/03/28/2019-mazda6-manual-transmission/ \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "HTTPSConnectionPool(host='www.autoblog.comhttps', port=443): Max retries exceeded with url: //www.autoblog.com/photos/2019-geneva-motor-show-editors-picks/ (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000000000D13CA90>: Failed to establish a new connection: [Errno 11004] getaddrinfo failed')): https://www.autoblog.comhttps://www.autoblog.com/photos/2019-geneva-motor-show-editors-picks/ \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "HTTPSConnectionPool(host='www.autoblog.comhttps', port=443): Max retries exceeded with url: //www.autoblog.com/photos/10-most-expensive-cars-to-insure/ (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000000000CA0C0F0>: Failed to establish a new connection: [Errno 11004] getaddrinfo failed')): https://www.autoblog.comhttps://www.autoblog.com/photos/10-most-expensive-cars-to-insure/ \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "HTTPSConnectionPool(host='www.autoblog.comhttps', port=443): Max retries exceeded with url: //www.autoblog.com/article/2020-ford-bronco-suv-details/ (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000000000DA5ED68>: Failed to establish a new connection: [Errno 11004] getaddrinfo failed')): https://www.autoblog.comhttps://www.autoblog.com/article/2020-ford-bronco-suv-details/ \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "HTTPSConnectionPool(host='www.autoblog.comhttps', port=443): Max retries exceeded with url: //www.autoblog.com/vehicle-subscription-services/ (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x000000000CA934E0>: Failed to establish a new connection: [Errno 11004] getaddrinfo failed')): https://www.autoblog.comhttps://www.autoblog.com/vehicle-subscription-services/ \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "487 Autoblog article(s) scraped\n",
      "15 Autoblog article(s) skipped due to error\n",
      "0 Autoblog article(s) skipped due to age\n",
      "112 relevant article(s) collected\n",
      "\n",
      "Electrek\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebarnard\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py:1204: UnknownTimezoneWarning: tzname ET identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  category=UnknownTimezoneWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261 Electrek article(s) scraped\n",
      "0 Electrek article(s) skipped due to error\n",
      "0 Electrek article(s) skipped due to age\n",
      "196 relevant article(s) collected\n",
      "\n",
      "The Verge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebarnard\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py:1204: UnknownTimezoneWarning: tzname EST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  category=UnknownTimezoneWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'text': https://www.theverge.com/2019/3/5/18251543/geneva-motor-show-2019-best-cars-honda-audi-vw \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "'NoneType' object has no attribute 'text': https://www.theverge.com/2019/3/14/18261968/tesla-model-y-announcement-news-compact-suv-elon-musk-updates-highlights \n",
      "date:None\n",
      "title:None\n",
      "summary:None\n",
      "61 The Verge article(s) scraped\n",
      "2 The Verge article(s) skipped due to error\n",
      "15 The Verge article(s) skipped due to age\n",
      "46 relevant article(s) collected\n",
      "\n",
      "Crunchbase\n",
      "109 Crunchbase article(s) scraped\n",
      "0 Crunchbase article(s) skipped due to error\n",
      "24 Crunchbase article(s) skipped due to age\n",
      "47 relevant article(s) collected\n",
      "\n",
      "Truck News\n",
      "117 Truck News article(s) scraped\n",
      "0 Truck News article(s) skipped due to error\n",
      "33 Truck News article(s) skipped due to age\n",
      "56 relevant article(s) collected\n",
      "\n",
      "Trucks.com\n",
      "17 Trucks.com article(s) scraped\n",
      "0 Trucks.com article(s) skipped due to error\n",
      "0 Trucks.com article(s) skipped due to age\n",
      "11 relevant article(s) collected\n",
      "\n",
      "TechCrunch\n",
      "Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"time.time-since\"}\n",
      "  (Session info: chrome=73.0.3683.86)\n",
      "  (Driver info: chromedriver=2.33.506120 (e3e53437346286c0bc2d2dc9aa4915ba81d9023f),platform=Windows NT 6.1.7601 SP1 x86_64)\n",
      ": https://techcrunch.com/video-article/behind-the-scenes-at-laikas-wildly-imaginative-new-stop-motion-movie-missing-link/ \n",
      "date:\n",
      "title:\n",
      "summary:\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "scrape_specs = {}\n",
    "scraypahs = {}\n",
    "start_time = time.time()\n",
    "\n",
    "for site in list(scraper_dict.keys()):\n",
    "    temp_start_time = time.time()\n",
    "    print('\\n'+site)\n",
    "    scraypahs[site] = scraypah(scraper_dict[site])\n",
    "    if scraper_dict[site]['css_bool']:\n",
    "        scraypahs[site].css_scrape_em()\n",
    "    else:\n",
    "        scraypahs[site].get_urls()\n",
    "        scraypahs[site].scrape_em()\n",
    "    scrape_specs = print_results(scraypahs[site].source, scraypahs[site].scraped_count, scraypahs[site].skip_count,\n",
    "                                 scraypahs[site].too_old, scraypahs[site].relevant_df, round(\n",
    "                                     time.time()-temp_start_time, 2),\n",
    "                                 scrape_specs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for auto-categorizing articles from academic journals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_academic(news):\n",
    "    uncat_news = news.copy()\n",
    "    # Create a column called is_journal for journal_bool.  If True, associated article is a journal\n",
    "    uncat_news['is_journal']=uncat_news.source.apply(lambda x: scraper_dict[x]['journal_bool'])\n",
    "    # Filter news based on whether or not it's a journal\n",
    "    journal_news = uncat_news[uncat_news.is_journal].copy()\n",
    "    nonjournal_news = uncat_news[~uncat_news.is_journal].copy()\n",
    "    # If the source of the article corresponding with the row is a journal, set the category = 4\n",
    "    journal_news.category = 4\n",
    "    nonjournal_news.category = ''\n",
    "    categorized_news = pd.concat([journal_news, nonjournal_news])\n",
    "    # Drop the is_journal column\n",
    "    categorized_news.drop(columns = 'is_journal', inplace = True)\n",
    "    return categorized_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get summary of articles scraped and filter to today's newstype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T14:34:17.790747Z",
     "start_time": "2018-11-09T14:34:17.279486Z"
    }
   },
   "outputs": [],
   "source": [
    "# Meta-data from the scrape session\n",
    "scrape_specs_df = pd.DataFrame.from_dict(scrape_specs).T.reset_index()\n",
    "scrape_specs_df['Time per relevant article'] = scrape_specs_df['Time spent'] / \\\n",
    "    scrape_specs_df['Relevant Articles']\n",
    "display(scrape_specs_df)\n",
    "\n",
    "# List all of the relevant news from each of the scrapers (each scraypah item has an attribute \"relevant_df\", which is a pandas\n",
    "# dataframe with all of the selected news items from that website)\n",
    "all_news_dfs = []\n",
    "for key, value in scraypahs.items():\n",
    "    all_news_dfs.append(value.relevant_df)\n",
    "\n",
    "# Stack all of the articles into a single dataframe and do some cleaning (drop duplicate articles)\n",
    "all_df = pd.concat(all_news_dfs)\n",
    "all_df = all_df[['title', 'date'] + all_scrapers + ['summary', 'source', 'link']].sort_values('date', ascending=False)\n",
    "all_df.drop_duplicates(subset='title', inplace=True)\n",
    "all_df = all_df.replace('\\$', '$', regex=True)\n",
    "\n",
    "for scraper in all_scrapers:\n",
    "    print(scraper + ' articles found: {}'.format(\n",
    "    all_df[scraper].sum().astype(int)))\n",
    "\n",
    "# Populate meta-data columns (helpful for searching all news items in the future if we want)\n",
    "all_df['reason_for_tag'] = all_df.apply(which_keyword_found, axis=1)\n",
    "all_df['keywords'] = all_df['title'].str.strip().apply(keyword_pull)\n",
    "\n",
    "# Drop rows if summary is 'NA'\n",
    "all_df = all_df[~all_df.summary.isna()].copy()\n",
    "\n",
    "# Add a column for category\n",
    "all_df['category'] = ''\n",
    "\n",
    "# Auto-categorize articles from academic journals\n",
    "all_df = categorize_academic(all_df)\n",
    "\n",
    "# Drop all with summary == None.  This removes some Autoblog articles.\n",
    "all_df = all_df[all_df.summary==''].copy()\n",
    "\n",
    "# Format for excel writing.  Split all_news into CAV, truck, AFV, etc. news dataframes\n",
    "for scraper in all_scrapers:\n",
    "    scraper_info[scraper]['news_df']= all_df[all_df[scraper] == 1].sort_values('date', ascending=False).drop(all_scrapers, axis=1).copy()\n",
    "    # Automatically labels academic articles category '4' for ease of categorization.\n",
    "    \n",
    "    \n",
    "    if not scraper_info[scraper]['auto_id_research']:\n",
    "        scraper_info[scraper]['news_df'].category = ''\n",
    "        \n",
    "        \n",
    "    # Drop vehicles older than a given date -- this is useful if two scrapers with different max ages are run on the same day.\n",
    "    # If 21CTP max age is 7 and INL max age is 31 and both are run on the same day, articles 31 days old or less will be scraped for\n",
    "    # 21CTP.  So we need a way of dropping the articles that are older than the max age (7 days) for 21CTP.\n",
    "    scraper_info[scraper]['news_df'] = scraper_info[scraper]['news_df'][(scraper_info[scraper]['news_df'].date - search_date).apply(lambda x: x.days) >= -scraper_info[scraper]['max_age']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete duplicate articles, delete irrelevant articles, and sample the remaining articles to get the desired number of total articles\n",
    "Filter out duplicate articles -- If two sets of keywords are similar, keep article from best source\n",
    "\n",
    "Delete irrelevant articles -- Based on bad_words in title\n",
    "\n",
    "Sample remaining articles -- Keeps all articles from best sources (with ratings of 1 or 2), randomly samples the remaining articles (from the worst sources), and combines the two to get the total desired number of articles (see ideal_no_articles dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions needed for article removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a news dataframe (i.e. AFV_news) and a list of keywords corresponding to that news (i.e. AFV_keywords), delete_duplicates()\n",
    "# uses Jaccard similarity to identify duplicate articles and drops the duplicate articles.\n",
    "# The function returns a dataframe with no duplicates (news) and a dataframe with the dropped duplicates for reference.\n",
    "def delete_duplicates(all_news, keywords):\n",
    "    def get_jaccard_sim(list1, list2): # https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50\n",
    "        # For our purposes, we'll first subtract the keywords from each set so that they don't skew our similarity rating\n",
    "        # too high\n",
    "        a = set(list1)\n",
    "        b = set(list2)\n",
    "        c = a.intersection(b)\n",
    "        return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "    def get_duplicates(new_news, MAX_SIM = .5): # Increase MAX_SIM (must be between 0 and 1) if \"duplicates\" are not close enough.  \n",
    "                                                # Reduce if not picking up any duplicates.\n",
    "        jaccard_sim_df = pd.DataFrame(columns = [*range(len(news))])\n",
    "        news['kw_set'] = news.keywords.apply(lambda x: str(x).split(','))\n",
    "        news['kw_set'] = news.kw_set.apply(lambda x: [y.strip() for y in x])\n",
    "        for col in jaccard_sim_df:\n",
    "            jaccard_sim_df[col] = news.kw_set.apply(lambda x: get_jaccard_sim(x, news.kw_set.loc[col]))\n",
    "        duplicate_pairs = list(jaccard_sim_df[jaccard_sim_df > MAX_SIM].stack().index) # Create list of article pairs that exceed a \n",
    "                                                                                   # Jaccard similarity of MAX_SIM\n",
    "        duplicate_pairs = set([frozenset(pair) for pair in duplicate_pairs if pair[0] != pair[1]]) # Remove articles paired with \n",
    "                                                                                               # themselves as well as \n",
    "                                                                                               # entries like (2,4) if (4,2) \n",
    "                                                                                               # is already in the list\n",
    "        duplicate_pairs = [tuple(pair) for pair in duplicate_pairs] # List of duplicate pairs\n",
    "        return duplicate_pairs\n",
    "\n",
    "    # Finds all the duplicates removed as well as the titles of the articles that were kept instead.\n",
    "    def get_removed_duplicates(all_news, dropped_vs_kept_tuples):\n",
    "        duplicates_removed = all_news.copy()\n",
    "        column_mapping = dict(zip(all_news.columns, all_news.columns + ' of dropped article'))\n",
    "        duplicates_removed.rename(columns = column_mapping, inplace = True)\n",
    "        duplicates_removed.insert(1, 'title of kept article', duplicates_removed.index)\n",
    "        duplicates_removed['title of kept article'] = duplicates_removed['title of kept article'].map(dict(dropped_vs_kept_tuples))\n",
    "        title_mapping = all_news.title.to_dict()\n",
    "        duplicates_removed['title of kept article'] = duplicates_removed['title of kept article'].map(title_mapping)\n",
    "        return duplicates_removed[~duplicates_removed['title of kept article'].isna()]\n",
    "    \n",
    "    all_news.reset_index(inplace = True, drop = True)\n",
    "    news = all_news.copy()\n",
    "    \n",
    "    # Get a list of tuples with pairs of indices corresponding to duplicate articles.\n",
    "    duplicate_pairs = get_duplicates(news)\n",
    "    \n",
    "    # Initialize a list to store tuples of (dropped, kept) index pairs.  This is for tracking which articles\n",
    "    # were dropped vs. kept.  To be used with get_removed_duplicates()\n",
    "    dropped_vs_kept_tuples = []\n",
    "    \n",
    "    # Drop duplicates with higher source ratings. Article sources are rated from 1-Best to 3-Worst.\n",
    "    for pair in duplicate_pairs:\n",
    "        # Make sure that both indices are in the news dataframe.  If not, continue.\n",
    "        if (pair[0] not in news.index) or (pair[1] not in news.index):\n",
    "            continue\n",
    "        # Get the ratings of the article sources associated with each index in the pair.  \n",
    "        rating0 = scraper_dict[news.loc[pair[0]].source]['rating']\n",
    "        rating1 = scraper_dict[news.loc[pair[1]].source]['rating']\n",
    "        # Drop the article that has the source with the lowest rating.  If sources have the same rating, choose randomly.\n",
    "        if rating0 > rating1:\n",
    "            index_choice = 0\n",
    "        elif rating0 < rating1:\n",
    "            index_choice = 1\n",
    "        else:\n",
    "            index_choice = random.choice([0, 1])\n",
    "        \n",
    "        # Add a tuple to track which index was dropped vs. kept (dropped, kept).  \n",
    "        # This list is to be used with get_duplicates_removed().\n",
    "        dropped_vs_kept_tuples.append(tuple([pair[index_choice], pair[int(not index_choice)]]))\n",
    "        # Drop the article from the news dataframe.\n",
    "        news.drop(pair[index_choice], inplace = True)\n",
    "\n",
    "    duplicates_removed = get_removed_duplicates(all_news, dropped_vs_kept_tuples)\n",
    "    news.drop(columns = 'kw_set', inplace = True)\n",
    "    duplicates_removed['classification (d, x, s)'] = 'd' # Classified as 'd' for duplicate\n",
    "    return news, duplicates_removed\n",
    "\n",
    "# Deletes articles from news based on bad_words in title.  For example if we don't want 'picture' in the title of\n",
    "# AFV_news articles, we would add 'picture' to AFV_bad_words.\n",
    "def delete_irrelevant(news, bad_words):\n",
    "    bad_words = [bad_word.lower() for bad_word in bad_words] # Make bad words lowercase and later make titles lowercase to remove case sensitivity.\n",
    "    deleted_articles = news.copy()\n",
    "    column_mapping = dict(zip(news.columns, news.columns + ' of dropped article'))\n",
    "    deleted_articles.rename(columns = column_mapping, inplace = True)\n",
    "    deleted_articles['reason_for_deletion'] = deleted_articles['title of dropped article'].str.lower().apply(lambda x: list((set(x.split())&set(bad_words))))\n",
    "    deleted_articles.reason_for_deletion = deleted_articles.reason_for_deletion.apply(lambda x: ', '.join(x))\n",
    "    kept_articles = deleted_articles.copy()\n",
    "    kept_articles = kept_articles[kept_articles.reason_for_deletion==''].copy()\n",
    "    #print(kept_articles)\n",
    "    kept_articles.drop(columns = 'reason_for_deletion', inplace = True)\n",
    "    deleted_articles['classification (d, x, s)'] = 'x' # Classified as 'i' for irrelevant\n",
    "    deleted_articles = deleted_articles[deleted_articles.reason_for_deletion!=''].copy()\n",
    "    kept_articles.rename(columns = {v:k for k,v in column_mapping.items()}, inplace = True) # Map column names back to what they originally were in news.\n",
    "    return kept_articles, deleted_articles\n",
    "\n",
    "def sample_news(news, ideal_no):\n",
    "    def get_deleted_articles(news, kept_articles):\n",
    "        deleted_articles = news.copy()\n",
    "        deleted_articles['deleted'] = ~deleted_articles.title.isin(kept_articles.title)\n",
    "        deleted_articles = deleted_articles[deleted_articles.deleted].copy()\n",
    "        deleted_articles.drop(columns = 'deleted', inplace = True)\n",
    "        column_mapping = dict(zip(news.columns, news.columns + ' of dropped article'))\n",
    "        deleted_articles.rename(columns = column_mapping, inplace = True)\n",
    "        deleted_articles['classification (d, x, s)'] = 's'\n",
    "        return deleted_articles\n",
    "\n",
    "    news_with_source_ratings = news.copy()\n",
    "    # Create a column with the ratings of each news source.\n",
    "    news_with_source_ratings['rating']=news_with_source_ratings.source.apply(lambda x: scraper_dict[x]['rating'])\n",
    "    # Create dataframes with best and worst sources.\n",
    "    best_source_news = news_with_source_ratings[news_with_source_ratings.rating < 3].copy()\n",
    "    worst_source_news = news_with_source_ratings[news_with_source_ratings.rating == 3].copy()\n",
    "    \n",
    "    # If we have less than the idel_no of articles, just keep all news.\n",
    "    if len(news_with_source_ratings) <= ideal_no:\n",
    "        kept_articles = news_with_source_ratings\n",
    "    \n",
    "    # If the number of \"good\" articles is greater than the desired total number of articles sample only best_source_news\n",
    "    elif len(best_source_news) > ideal_no: \n",
    "        no_to_sample = ideal_no\n",
    "        kept_articles = best_source_news.sample(no_to_sample)\n",
    "    \n",
    "    # Otherwise, Sample worst_source_news and concatenate with best_source_news\n",
    "    else:\n",
    "        no_to_sample = ideal_no - len(best_source_news)\n",
    "        kept_articles = pd.concat([best_source_news, worst_source_news.sample(no_to_sample)])\n",
    "    kept_articles.drop(columns = 'rating', inplace = True)\n",
    "    return kept_articles, get_deleted_articles(news,kept_articles)\n",
    "\n",
    "# Sets aside articles to keep no matter what; i.e., keeps all articles with a certain rating.\n",
    "def keep_no_matter_what(news, rating_no):\n",
    "    all_articles = news.copy()\n",
    "    all_articles['rating']=all_articles.source.apply(lambda x: scraper_dict[x]['rating'])\n",
    "    kept_no_matter_what_articles = all_articles[all_articles.rating == rating_no].copy()\n",
    "    all_other_articles = all_articles[all_articles.rating != rating_no].copy()\n",
    "    kept_no_matter_what_articles.drop(columns = 'rating', inplace = True)\n",
    "    all_other_articles.drop(columns = 'rating', inplace = True)\n",
    "    return kept_no_matter_what_articles, all_other_articles\n",
    "\n",
    "# Executes delete_duplicates, delete_irrelevant, and sample_news functions to remove all unwanted articles from news df.\n",
    "def make_all_deletions(news, keywords, bad_words, ideal_no_articles):\n",
    "    all_articles = news.copy()\n",
    "    kept_no_matter_what_articles, all_other_articles = keep_no_matter_what(all_articles, 0) # Keep all articles with a rating of '0' no matter what.\n",
    "    kept_articles, duplicate_articles = delete_duplicates(all_other_articles, keywords) # Delete duplicates\n",
    "    kept_articles, irrelevant_articles = delete_irrelevant(kept_articles, bad_words) # Delete irrelevant\n",
    "    kept_articles, articles_not_sampled = sample_news(kept_articles, ideal_no_articles)\n",
    "    deleted_articles = pd.concat([duplicate_articles, irrelevant_articles, articles_not_sampled], sort = True) # Concatenate duplicate and irrelevant dataframes to get all deleted articles\n",
    "    # Reorder columns and fill NA\n",
    "    deleted_articles = deleted_articles[['title of dropped article', 'title of kept article', 'reason_for_deletion', 'date of dropped article', 'summary of dropped article', \n",
    "                                         'source of dropped article', 'link of dropped article', 'reason_for_tag of dropped article', 'keywords of dropped article', 'classification (d, x, s)']]\n",
    "    deleted_articles.fillna('NA', inplace = True)\n",
    "    # Add column for correct classification -- to be filled in by hand in Excel\n",
    "    deleted_articles['correct classification (d = duplicate, x = irrelevant, s = sample, n = neither)'] = ''\n",
    "    kept_articles = kept_articles[~kept_articles.summary.isna()].copy()\n",
    "    kept_articles = pd.concat([kept_articles, kept_no_matter_what_articles])\n",
    "    return kept_articles, deleted_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute duplicate and irrelevant article removal and write duplicates and irrelevant articles to spreadsheet (skip this cell if you don't want to remove articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "for scraper in todays_scrapers:\n",
    "    # If we have at least 1 news item and we want to make deletions...\n",
    "    if (scraper_info[scraper]['news_df'].shape[0]>0) and (scraper_info[scraper]['make_deletions']):\n",
    "        # Execute duplicate and irrelevant article removal                                                             \n",
    "        scraper_info[scraper]['news_df'], scraper_info[scraper]['deletions_df'] = make_all_deletions(scraper_info[scraper]['news_df'], \n",
    "                                                                                                                                scraper_info[scraper]['keywords'], \n",
    "                                                                                                                                scraper_info[scraper]['bad_words'], \n",
    "                                                                                                                                scraper_info[scraper]['ideal_no_articles'])\n",
    "        # Write duplicates and irrelevant articles to spreadsheet\n",
    "        if scraper_info[scraper]['deletions_df'].shape[0] > 0:\n",
    "            filename = scraper_info[scraper]['deletions_filename']\n",
    "            scraper_info[scraper]['deletions_df'].to_excel(filename)\n",
    "            print('Some ' + scraper + ' stuff!')\n",
    "            os.startfile(cwd + '/' + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write final dataframe to a spreadsheet\n",
    "CAVs on Monday, AFVs on Wednesday, 21CTP on Friday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T16:55:54.374736Z",
     "start_time": "2018-10-17T16:55:54.098473Z"
    }
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "for scraper in todays_scrapers:\n",
    "    if scraper_info[scraper]['news_df'].shape[0] > 0:\n",
    "        filename = scraper_info[scraper]['news_download_filename']\n",
    "        scraper_info[scraper]['news_df'].to_excel(filename)\n",
    "        print('Some ' + scraper + ' stuff!')\n",
    "        os.startfile(cwd + '/' + filename)\n",
    "        \n",
    "_=input('Press enter to continue: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get AFV graph, table, and CA shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_bool = False\n",
    "if search_date.weekday() == scraper_info['AFV']['day']:\n",
    "    try:\n",
    "        # Go into EVSE directory.  This directory should be in the same directory as News_scraper.ipynb. This directory must contain EVSE Market Analysis.ipynb\n",
    "        %cd EVSE \n",
    "        cwd = os.getcwd()\n",
    "        print(cwd)\n",
    "        #!jupyter \"EVSE Market Analysis.ipynb\" --to script\n",
    "        %run \"EVSE Market Analysis.ipynb\"\n",
    "        # Get CA_shares from all_CA_shares.csv\n",
    "        CA_shares_data = pd.read_csv(cwd + '\\\\' + 'all_CA_shares.csv', ';')[['date', 'text']]\n",
    "        CA_shares_data.set_index('date', inplace = True)\n",
    "        \n",
    "        EVSE_file_dict = {\n",
    "            'EVSE_bar_chart' : cwd + '\\\\'+ f'EVSE_bar_chart_{search_date_str}.png', # Get bar chart\n",
    "            'CA_shares' : CA_shares_data.loc[search_date_str]['text'], # Get CA shares from today\n",
    "            \n",
    "        }\n",
    "        # Open deltstations.xlsx file for copying and pasting delt stations table into AFV news\n",
    "        os.startfile(cwd + '\\\\' + f'{search_date_str}_deltstation.xlsx')\n",
    "        graph_bool = True # Means all the necessary files are in the directory. If false, graphs will not be added to the docx\n",
    "    except:\n",
    "        print('There is a problem with one of the files.  The possibilities are:')\n",
    "        print('\\t1. ' + \"alt_fuel_stations ({}).csv\".format((dt.datetime.today() - dt.timedelta(days=14)).strftime(\"%B %d %Y\")) + 'is not in the EVSE folder.')\n",
    "        print('\\t2. ' + \"alt_fuel_stations ({}).csv\".format((dt.datetime.today() - dt.timedelta(days=7)).strftime(\"%B %d %Y\")) + 'is not in the EVSE folder.')\n",
    "        print('\\t3. all_CA_shares.csv is not in the EVSE folder.')\n",
    "        print('\\t4. ' + search_date_str + ' is not in the all_CA_shares.csv file.')\n",
    "    %cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Word file from the news update spreadsheets\n",
    "Automatically does CAV on Mondays, AFV on Wednesdays, and 21CTP on Fridays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:25:20.873985Z",
     "start_time": "2018-10-17T17:25:20.757295Z"
    }
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "for scraper in todays_scrapers:\n",
    "    if scraper_info[scraper]['gen_docx']:\n",
    "        print(scraper)\n",
    "        docx_filename = cwd + '/'+ gen_docx(scraper, graph_bool)\n",
    "        os.startfile(docx_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update news item tracking and news scraper meta-data databases\n",
    "Only run when **final** news item spreadsheet is saved in your working directory (i.e., after you have manually added other articles to the already-saved spreadsheet from the cell above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T11:30:00.039138Z",
     "start_time": "2018-09-12T11:30:00.031855Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is in case you go to upload this week's news items to the database, and realize you forgot to do last week's. Just replace\n",
    "# all instances of \"search_date_str\" in the next cell with \"last_week\"and run it. Make sure you switch them all back to \"search_date_str\"..\n",
    "last_week = str((pd.to_datetime(search_date_str) - dt.timedelta(days=7)).date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T21:47:24.974039Z",
     "start_time": "2018-11-19T21:47:24.165717Z"
    }
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('news_updates.db')\n",
    "if (search_date.weekday() == scraper_info['CAV']['day']) & (~db_update):\n",
    "    print('CAV')\n",
    "    pd.read_excel('cav_news_updates/{}_cav_news_download.xls'.format(search_date_str)\n",
    "                  ).to_sql('CAV', conn, if_exists='append', index=False)\n",
    "    db_update = True\n",
    "elif (search_date.weekday() == scraper_info['AFV']['day']) & (~db_update):\n",
    "    print('AFV')\n",
    "    pd.read_excel('afv_news_updates/{}_afv_news_download.xls'.format(search_date_str)\n",
    "                  ).to_sql('AFV', conn, if_exists='append', index=False)\n",
    "    db_update = True\n",
    "conn.close()\n",
    "\n",
    "# This saves the meta-data from all of the scraper runs every Wednesday (print out \"scrape_specs_df\" to see what the meta-data includes)\n",
    "if search_date.weekday() == 2:\n",
    "    conn = sqlite3.connect('news_updates_meta.db')\n",
    "    scrape_specs_df.drop(['Time spent', 'Time per relevant article'], axis=1).to_sql(\n",
    "        'news_updates_meta', conn, if_exists='append', index=False)\n",
    "    conn.close()\n",
    "    print('Uploaded metadata! So many datas!')\n",
    "    \n",
    "# Need to add 21CTP, evtol, and hyperloop metadata?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Halt run all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False #Will generate an error and stop notebook execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For scraper testing (no need to run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing a single website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing a single scraper - only needed when adding new sites (don't want to run all of them over and over...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T13:37:17.149891Z",
     "start_time": "2019-01-07T13:37:04.771546Z"
    }
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "scrape_specs = {}\n",
    "scraypahs = {}\n",
    "temp_start_time = time.time()\n",
    "\n",
    "site = 'Autoblog'\n",
    "scraypahs[site] = scraypah(scraper_dict[site])\n",
    "if scraper_dict[site]['css_bool'] == True:\n",
    "    scraypahs[site].css_scrape_em()\n",
    "else:\n",
    "    scraypahs[site].get_urls()\n",
    "    scraypahs[site].scrape_em()\n",
    "scrape_specs = print_results(scraypahs[site].source, scraypahs[site].scraped_count, scraypahs[site].skip_count,\n",
    "                             scraypahs[site].too_old, scraypahs[site].relevant_df, round(\n",
    "                                 time.time()-temp_start_time, 2),\n",
    "                             scrape_specs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For scraper development (no need to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " 'GreenCarCongress': {'url': ['http://www.greencarcongress.com/', 'http://www.greencarcongress.com/page/2/'],\n",
    "                                     'source': 'GreenCarCongress',\n",
    "                                     'css_bool': False,\n",
    "                                     'strain_tag': 'article',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'post entry',\n",
    "                                     'url_list_query': \"[item.a['href'] for item in self.base_soup.find_all('article', attrs={'class': 'post entry'})]\",\n",
    "                                     'date_loc': \"article.find('span', attrs={'class':'entry-date'}).a.text\",\n",
    "                                     'date_format': None,\n",
    "                                     'sum_loc': \"article.find_all('p')\",\n",
    "                                     'title_loc': \"article.h2.a.text\",\n",
    "                                     'strain_bool': True,\n",
    "                                     'journal_bool': False,\n",
    "                                     'rating': 1},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'GreenCarCongress': {'url': [f'http://www.greencarcongress.com/page/{page_no}/' for page_no in range(1,8)],\n",
    "                                     'source': 'GreenCarCongress',\n",
    "                                     'css_bool': False,\n",
    "                                     'strain_tag': 'article',\n",
    "                                     'strain_attr_name': 'class',\n",
    "                                     'strain_attr_value': 'post entry',\n",
    "                                     'url_list_query': \"[item.a['href'] for item in self.base_soup.find_all('article', attrs={'class': 'post entry'})]\",\n",
    "                                     'date_loc': \"article.find('span', attrs={'class':'entry-date'}).a.text\",\n",
    "                                     'date_format': None,\n",
    "                                     'sum_loc': \"article.find_all('p')\",\n",
    "                                     'title_loc': \"article.h2.a.text\",\n",
    "                                     'strain_bool': True,\n",
    "                                     'journal_bool': False,\n",
    "                                     'rating': 1},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Quartz': {'url': 'https://qz.com/search/self-driving',\n",
    "                           'source': 'Quartz',\n",
    "                           'css_bool': False,\n",
    "                           'url_list_query': \"['https://qz.com' + a['href'] for a in self.base_soup.find_all('a', class_='_5ff1a')]\",\n",
    "                           'date_loc': \"article.time.text\",\n",
    "                           'date_format': None,\n",
    "                           'sum_loc': \"article.find_all('p')\",\n",
    "                           'title_loc': \"article.h1.text\",\n",
    "                           'strain_bool': False,\n",
    "                           'journal_bool': False,\n",
    "                           'rating': 2},"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set url, date, summary, and title locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_urls = ['https://qz.com/search/self-driving']\n",
    "url_loc = \"[header.a['href'] for header in soup.find_all('h2') if header.a!=None]\"\n",
    "date_loc = \"article.time.text\"\n",
    "sum_loc = \"article.find_all('p')\"\n",
    "title_loc = \"article.h1.text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test a site using selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "for base_url in base_urls:\n",
    "    driver.get(base_url)\n",
    "    urls = eval(url_loc)\n",
    "    for url in urls:\n",
    "        driver.get(url)\n",
    "        article = BeautifulSoup(driver.page_source)\n",
    "        title = eval(title_loc)\n",
    "        date = eval(date_loc)\n",
    "        summary = eval(sum_loc)\n",
    "        print(title, '\\n', date, '\\n', summary, '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test a site using only Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for base_url in base_urls:\n",
    "    soup = grab_homepage(base_url)\n",
    "    urls = eval(url_loc)\n",
    "    for url in urls: \n",
    "        article = grab_homepage(url)\n",
    "        title = eval(title_loc)\n",
    "        date = eval(date_loc)\n",
    "        summary = eval(sum_loc)\n",
    "        print(title, '\\n', date, '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change Log\n",
    "* 8/29/2018: Added Citylab, Electrek, cleaned code\n",
    "* 8/7/2018: Added Transport Reviews to academic paper scraper\n",
    "* 7/30/2018: Fixed GovTech scraper\n",
    "* 6/29/2018: Changed the whole scraper over to utilize a new class called *scraypah*. \n",
    "* 5/12/2018: Added Semiconductor Engineering scraper and academic articles scraper (~3 hours)\n",
    "* 4/13/2018: Integrated word document production through python\n",
    "* 3/19/2018: Added OEM/Gov section that quickly checks 17 sites for updates - only prints a notification that it needs to be checked if there are new updates from the past week\n",
    "* 2/27/2018: Wrote a function *page_scan* to more efficiently create the relevant web page dictionary \"profiles\"\n",
    "* 2/27/2018: Added 21CTP trucking news keywords to search for. Integrated functionality into existing web scraper.\n",
    "* 2/14/2018: Added NGV Global scraper for AFV stuff\n",
    "* 2/14/2018: Added fuel cells, hybrid, hybrid-electric, 'electric buses', 'electric truck', 'electric trucks', 'electric drive' to the search terms for AFVs...\n",
    "* 1/31/2018: Added *print_results* function to streamline printed results for each scraper. Added counter to track #articles that were too old. Added meta-data tracking capability (dumps into SQL database every week)\n",
    "* 1/31/2018: Split EV market analysis and web scraper into two different Notebooks\n",
    "* 1/26/2018: Added Lexology scraper\n",
    "* 1/19/2018: Fixed GreenCarCongress scraper (site redesign)\n",
    "* 1/4/2018: Added Engadget scraper\n",
    "* 1/4/2018: Added \"replace_em\" function to streamline removal of meaningless substrings from body text summaries\n",
    "* 12/29/2017: Added Reuters, MITNews, and ARSTechnica scrapers. Did some streamlining in the EV Sales analysis\n",
    "* 12/20/2017: Wrote up quick-guide to all the post-Python processing needed for the final News Update doc.\n",
    "* 12/20/2017: Changed to .xls format. Had to import a different package to do so, but makes mail merge work better\n",
    "* 12/13/2017: Fixed Trucks.com scraper - was pulling out the wrong date for each article (pulled a date from the sidebar...)\n",
    "* 12/8/2017: Edited Trucks.com search so that it doesn't pick up paragraph tags that are actually image captions (added condition that \"class = None\")\n",
    "* 12/8/2017: Added a bunch of comments, specifically in the first code segment (\"IEEE Spectrum\") for explanatory purposes\n",
    "* 1/8/2019: Added Green Car Reports, DOE, Business Wire, and The Fuse.\n",
    "* 1/10/2019: Uploaded ipynb to Energetics' GitHub (EICode).  Log entries can now be found via GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "498px",
    "left": "1480.3px",
    "right": "20px",
    "top": "119.976px",
    "width": "658px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
